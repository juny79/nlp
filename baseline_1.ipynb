{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6n-Ps2nkVsBb"
   },
   "source": [
    "# **ğŸ’ğŸ»ğŸ—¨ï¸ğŸ’ğŸ»â€â™‚ï¸ëŒ€í™” ìš”ì•½ Baseline code**\n",
    "> **Dialogue Summarization** ê²½ì§„ëŒ€íšŒì— ì˜¤ì‹  ì—¬ëŸ¬ë¶„ í™˜ì˜í•©ë‹ˆë‹¤! ğŸ‰    \n",
    "> ë³¸ ëŒ€íšŒì—ì„œëŠ” ìµœì†Œ 2ëª…ì—ì„œ ìµœëŒ€ 7ëª…ì´ ë“±ì¥í•˜ì—¬ ë‚˜ëˆ„ëŠ” ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” BART ê¸°ë°˜ ëª¨ë¸ì˜ baseline codeë¥¼ ì œê³µí•©ë‹ˆë‹¤.     \n",
    "> ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì¼ìƒ ëŒ€í™”ì— ëŒ€í•œ ìš”ì•½ì„ íš¨ê³¼ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNq_LylZa1ug"
   },
   "source": [
    "## âš™ï¸ ë°ì´í„° ë° í™˜ê²½ì„¤ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjCiuI_V4glr"
   },
   "source": [
    "### 1) í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYqDF_-r2ToB"
   },
   "source": [
    "- í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•œ í›„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zbZ7SU9P2TYN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import yaml\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from rouge import Rouge # ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration, BartConfig\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "import wandb # ëª¨ë¸ í•™ìŠµ ê³¼ì •ì„ ì†ì‰½ê²Œ Trackingí•˜ê³ , ì‹œê°í™”í•  ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Qq46k6_CNQn"
   },
   "source": [
    "## ğŸ”§ ì„¤ì • ì •ì˜í•˜ê¸°\n",
    "\n",
    "### 3-1. Special Token ë™ì  ìˆ˜ì§‘ ë° ë“±ë¡\n",
    "- í•™ìŠµ ë°ì´í„°ì—ì„œ ì‹¤ì œ ë“±ì¥í•˜ëŠ” ëª¨ë“  ë§ˆìŠ¤í‚¹/ë°œí™”ì í† í°ì„ ìë™ ìˆ˜ì§‘\n",
    "- `additional_special_tokens`ë¡œ ë“±ë¡í•˜ì—¬ í† í°ì´ ë¶„í•´ë˜ì§€ ì•Šë„ë¡ í•¨\n",
    "- `model.resize_token_embeddings(len(tokenizer))` í˜¸ì¶œë¡œ ì„ë² ë”© ì‚¬ì´ì¦ˆ ì¡°ì •\n",
    "\n",
    "### 3-2. í”„ë¡¬í”„íŠ¸ ë¹Œë” (ë°œí™”ì/turn êµ¬ì¡° í™œìš©)\n",
    "- `build_summary_prompt()`: ìš”ì•½ íƒœìŠ¤í¬ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì•Œë ¤ì£¼ëŠ” í”„ë¡¬í”„íŠ¸ ì¶”ê°€\n",
    "- `prompt_style` ì˜µì…˜: \"default\", \"speaker_aware\", \"minimal\", \"none\"\n",
    "- ë°œí™”ì ìˆ˜ì— ë”°ë¥¸ ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„± ê°€ëŠ¥\n",
    "\n",
    "### 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬ (ìŠ¬ë­/ê°íƒ„ì‚¬/ì˜ì„±ì–´ ì •ê·œí™”)\n",
    "- `normalize_slang()`: ã…‹ã…‹â†’ì›ƒê¸°ë‹¤, ã… ã… â†’ìŠ¬í”„ë‹¤ ë“± êµ¬ì–´ì²´â†’ë¬¸ì–´ì²´ ë³€í™˜\n",
    "- ìš”ì•½ë¬¸ì´ ë¬¸ì–´ì²´ì´ë¯€ë¡œ, ëª¨ë¸ì´ ì§ì ‘ í•´ì„í•˜ëŠ” ê²ƒë³´ë‹¤ ë¯¸ë¦¬ ë³€í™˜í•˜ëŠ” ê²Œ íš¨ê³¼ì \n",
    "- `SLANG_MAP`: ì•½ 50ê°œ ì´ìƒì˜ ìŠ¬ë­/ê°íƒ„ì‚¬/ì¸í„°ë„· ìš©ì–´ ë§¤í•‘\n",
    "- `normalize_repeated_chars()`: ê³¼ë„í•œ ë°˜ë³µ ë¬¸ì ì •ê·œí™” (ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ â†’ ã…‹ã…‹ã…‹ã…‹)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197,
     "referenced_widgets": [
      "e920dbc173c045d1a32143349f1dff8e",
      "58c794fb7ce543a39fdf66d757f6eeab",
      "8a6464a355f7464c989033965d418a8a",
      "3645438ace1f4596a8dbc157b48c1521",
      "58001a60eacc44d5b38a68648adccde4",
      "6f5fde5b0ac840a18bd5cc380e564ff6",
      "45187decb58b4ad39ad532259c6277e5",
      "2307c6dcbe0141acb5e61baae19cade7",
      "4747b668e2fa4ab58a449446f80030f5",
      "14f6c91d6c634379b498586c51e606e0",
      "08d05bc20a96432badd459e1ffaf868e",
      "5dfcf310ca9e4e2794076098a5d69cea",
      "3c284a826f6843f6aa47eacad478ac30",
      "6caedd60c6b747469c82930be1f95d6d",
      "64f2218f899d446393cfea44f206f0a6",
      "d068f541df3f438dbd5138863e64b2f2",
      "affff1d8a89e4c14955d1b2aa39ff1ab",
      "13651c09564a4337b8274c1cb436faa5",
      "3bcd6b6b956347b29e1efa20a1d00542",
      "2fd3d7bbcd6948d8904d33001f95ea03",
      "d22fbc2c5dbf422399e496c9b500025a",
      "775d8bbeceac4e2da4f21ab6235c89ed",
      "de1a3f7701c243839fe03b930a9b9e30",
      "ebc22683058a4f229c5588e52fc93536",
      "52095cc7087243ac916055e569fd22f3",
      "a15af9e8158f4903b9189f3d322a5ef3",
      "21d2e54b5a0a4f79973a512105da43eb",
      "083ea69907bb48d4a8fff919bac51aad",
      "2a190bda0b72407e9a953cd2104dd3b2",
      "c18f0e3bc35e44d9915c3f84cd282a26",
      "3a04e871b74b45d7bf02fd33bb103577",
      "ac00d6c2cf974b33a628acb3f1471316",
      "285007b45236478ca147c6df752c8da4"
     ]
    },
    "id": "gZOE9TInCQHJ",
    "outputId": "8ce58487-6199-408c-cb37-49af1e218bc2"
   },
   "outputs": [],
   "source": [
    "# config ì„¤ì •ì— tokenizer ëª¨ë“ˆì´ ì‚¬ìš©ë˜ë¯€ë¡œ ë¯¸ë¦¬ tokenizerë¥¼ ì •ì˜í•´ì¤ë‹ˆë‹¤.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vsACJI7CVb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”– ë™ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ Special Tokens:\n",
      "============================================================\n",
      "ì´ 22ê°œ: ['#Address#', '#Alex#', '#Bob#', '#CarNumber#', '#CardNumber#', '#DateOfBirth#', '#Email#', '#Kristin#', '#Liliana#', '#Name#', '#PassportNumber#', '#Person1#', '#Person2#', '#Person3#', '#Person4#', '#Person5#', '#Person6#', '#Person7#', '#PersonName#', '#PhoneNumber#', '#Price#', '#SSN#']\n",
      "\n",
      "============================================================\n",
      "ğŸ” 6. ì •ê·œí‘œí˜„ì‹ ê¸°ë°˜ íŒ¨í„´ í™œìš© ì „ëµ:\n",
      "============================================================\n",
      "  6-1. í™”ì ìˆ˜ ëª…ì‹œ: True\n",
      "  6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: True\n",
      "\n",
      "ğŸ“Š ëŒ€í™” êµ¬ì¡° ë¶„ì„ ì˜ˆì‹œ:\n",
      "  - í™”ì ìˆ˜: 2ëª…\n",
      "  - í™”ì í† í°: {'#Person1#', '#Person2#'}\n",
      "  - PII ë§ˆìŠ¤í‚¹ ì—¬ë¶€: False\n",
      "  - PII ë§ˆìŠ¤í‚¹ ì˜ˆì‹œ: []\n",
      "\n",
      "============================================================\n",
      "ğŸ—£ï¸ êµ¬ì–´ì²´ ì „ì²˜ë¦¬ (ìŠ¬ë­/ê°íƒ„ì‚¬ ì •ê·œí™”):\n",
      "============================================================\n",
      "ì´ 59ê°œ ìŠ¬ë­ ë§¤í•‘ ì •ì˜\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™”:\n",
      "============================================================\n",
      "âœ… TF-IDF í•™ìŠµ ì™„ë£Œ: 2000ê°œ feature\n",
      "\n",
      "ğŸ“ TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì˜ˆì‹œ:\n",
      "  í‚¤ì›Œë“œ: ë§¤ë…„ / í•œ / ë²ˆ / í•´ìš” / ì˜ì‚¬ ì„ ìƒë‹˜\n",
      "\n",
      "============================================================\n",
      "ğŸ“ ê¸¸ì´/ì£¼ì œ ê¸°ë°˜ ë™ì  ì¶”ë¡  ì„¤ì •:\n",
      "============================================================\n",
      "ê¸¸ì´ ì¹´í…Œê³ ë¦¬:\n",
      "  - short: ~500ì, max_tokens=60, beams=4\n",
      "  - medium: ~1500ì, max_tokens=80, beams=5\n",
      "  - long: ~infì, max_tokens=100, beams=6\n",
      "\n",
      "ì£¼ì œ ì¹´í…Œê³ ë¦¬:\n",
      "  - medical: max_tokens=100, length_penalty=1.3\n",
      "  - insurance: max_tokens=100, length_penalty=1.3\n",
      "  - finance: max_tokens=95, length_penalty=1.2\n",
      "  - travel: max_tokens=85, length_penalty=1.1\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ ì „ì²´ ì„¤ì • ìš”ì•½:\n",
      "============================================================\n",
      "\n",
      "ğŸ”– 3-1. Special Tokens:\n",
      "  - ë™ì  ìˆ˜ì§‘: 22ê°œ\n",
      "\n",
      "ğŸ“ 3-2. í”„ë¡¬í”„íŠ¸ ì„¤ì •:\n",
      "  - í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: True\n",
      "  - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: balanced\n",
      "  - TF-IDF í‚¤ì›Œë“œ ìˆ˜: 5ê°œ\n",
      "\n",
      "ğŸ” 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš© ì „ëµ:\n",
      "  - 6-1. í™”ì ìˆ˜ ëª…ì‹œ: True\n",
      "  - 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: True\n",
      "\n",
      "ğŸ—£ï¸ 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬:\n",
      "  - ìŠ¬ë­ ì •ê·œí™”: True\n",
      "\n",
      "ğŸ“ 5-2. ë™ì  ì¶”ë¡ :\n",
      "  - ê¸¸ì´ ê¸°ë°˜: True\n",
      "  - ì£¼ì œ ê¸°ë°˜: True\n",
      "\n",
      "------------------------------------------------------------\n",
      "ğŸ“ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ (6-1, 6-2 ì ìš©):\n",
      "------------------------------------------------------------\n",
      "[1. ì›ë³¸ ëŒ€í™” (ì• 150ì)]:\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? \n",
      "#Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. \n",
      "#Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. \n",
      "#Person...\n",
      "\n",
      "[2. ëŒ€í™” êµ¬ì¡° ë¶„ì„]:\n",
      "  - í™”ì ìˆ˜: 2ëª…\n",
      "  - PII ë§ˆìŠ¤í‚¹ í† í°: []\n",
      "\n",
      "[3. ìŠ¬ë­ ì •ê·œí™” í›„]:\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? #Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. #Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. #Person2#:...\n",
      "\n",
      "[4. TF-IDF í‚¤ì›Œë“œ]: ë§¤ë…„ / í•œ / ë²ˆ / í•´ìš” / ì˜ì‚¬ ì„ ìƒë‹˜\n",
      "\n",
      "[5. 'balanced' í”„ë¡¬í”„íŠ¸ ì ìš© (6-1, 6-2)]:\n",
      "ë‹¤ìŒì€ 2ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ë‘ ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \n",
      "\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? #Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. #Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. #Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”. #Person1#: ìŒ, ì‹¬ê°í•œ ì§ˆë³‘ì„ í”¼í•˜ë ¤ë©´ ë¯¸ë¦¬ ë°œê²¬í•˜ëŠ” ê²Œ ì œì¼ ì¢‹ê±°ë“ ìš”. ë³¸ì¸ì„ ìœ„í•´ì„œë¼ë„ ë§¤ë…„ í•œ ë²ˆì€ ì˜¤ì„¸ìš”. #Person2#: ì•Œê² ìŠµë‹ˆë‹¤. #Person1#: ì—¬ê¸° ì¢€ ë³¼ê¹Œìš”. ëˆˆê³¼ ê·€ëŠ” ê´œì°®ìœ¼ì‹œë„¤ìš”. ê¹Šê²Œ ìˆ¨ í•œ ë²ˆ ì‰¬ì–´ë³´ì„¸ìš”. Mr. Smith, ë‹´ë°° í”¼ìš°ì„¸ìš”? #Pe...\n",
      "\n",
      "[6. 'keyword_aware' í”„ë¡¬í”„íŠ¸ ì ìš© (TF-IDF + 6-1, 6-2)]:\n",
      "ë‹¤ìŒì€ 2ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ë‘ ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. ì£¼ìš” í‚¤ì›Œë“œ: ë§¤ë…„ / í•œ / ë²ˆ / í•´ìš” / ì˜ì‚¬ ì„ ìƒë‹˜. \n",
      "\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? #Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. #Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. #Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”. #Person1#: ìŒ, ì‹¬ê°í•œ ì§ˆë³‘ì„ í”¼í•˜ë ¤ë©´ ë¯¸ë¦¬ ë°œê²¬í•˜ëŠ” ê²Œ ì œì¼ ì¢‹ê±°ë“ ìš”. ë³¸ì¸ì„ ìœ„í•´ì„œë¼ë„ ë§¤ë…„ í•œ ë²ˆì€ ì˜¤ì„¸ìš”. #Person2#: ì•Œê² ìŠµë‹ˆë‹¤. #Person1#: ì—¬ê¸° ì¢€ ë³¼ê¹Œìš”. ëˆˆê³¼ ê·€ëŠ” ê´œì°®ìœ¼ì‹œë„¤ìš”. ê¹Šê²Œ ìˆ¨ ...\n",
      "\n",
      "[7. ë™ì  ì¶”ë¡  ì„¤ì •]:\n",
      "  - ê¸¸ì´ ì¹´í…Œê³ ë¦¬: medium\n",
      "  - ê°ì§€ëœ ì£¼ì œ: medical\n",
      "  - max_new_tokens: 100\n",
      "\n",
      "âœ… decode_with_speaker_tags, postprocess_summary í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n",
      "âœ… TF-IDF í•™ìŠµ ì™„ë£Œ: 2000ê°œ feature\n",
      "\n",
      "ğŸ“ TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì˜ˆì‹œ:\n",
      "  í‚¤ì›Œë“œ: ë§¤ë…„ / í•œ / ë²ˆ / í•´ìš” / ì˜ì‚¬ ì„ ìƒë‹˜\n",
      "\n",
      "============================================================\n",
      "ğŸ“ ê¸¸ì´/ì£¼ì œ ê¸°ë°˜ ë™ì  ì¶”ë¡  ì„¤ì •:\n",
      "============================================================\n",
      "ê¸¸ì´ ì¹´í…Œê³ ë¦¬:\n",
      "  - short: ~500ì, max_tokens=60, beams=4\n",
      "  - medium: ~1500ì, max_tokens=80, beams=5\n",
      "  - long: ~infì, max_tokens=100, beams=6\n",
      "\n",
      "ì£¼ì œ ì¹´í…Œê³ ë¦¬:\n",
      "  - medical: max_tokens=100, length_penalty=1.3\n",
      "  - insurance: max_tokens=100, length_penalty=1.3\n",
      "  - finance: max_tokens=95, length_penalty=1.2\n",
      "  - travel: max_tokens=85, length_penalty=1.1\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ ì „ì²´ ì„¤ì • ìš”ì•½:\n",
      "============================================================\n",
      "\n",
      "ğŸ”– 3-1. Special Tokens:\n",
      "  - ë™ì  ìˆ˜ì§‘: 22ê°œ\n",
      "\n",
      "ğŸ“ 3-2. í”„ë¡¬í”„íŠ¸ ì„¤ì •:\n",
      "  - í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: True\n",
      "  - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: balanced\n",
      "  - TF-IDF í‚¤ì›Œë“œ ìˆ˜: 5ê°œ\n",
      "\n",
      "ğŸ” 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš© ì „ëµ:\n",
      "  - 6-1. í™”ì ìˆ˜ ëª…ì‹œ: True\n",
      "  - 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: True\n",
      "\n",
      "ğŸ—£ï¸ 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬:\n",
      "  - ìŠ¬ë­ ì •ê·œí™”: True\n",
      "\n",
      "ğŸ“ 5-2. ë™ì  ì¶”ë¡ :\n",
      "  - ê¸¸ì´ ê¸°ë°˜: True\n",
      "  - ì£¼ì œ ê¸°ë°˜: True\n",
      "\n",
      "------------------------------------------------------------\n",
      "ğŸ“ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ (6-1, 6-2 ì ìš©):\n",
      "------------------------------------------------------------\n",
      "[1. ì›ë³¸ ëŒ€í™” (ì• 150ì)]:\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? \n",
      "#Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. \n",
      "#Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. \n",
      "#Person...\n",
      "\n",
      "[2. ëŒ€í™” êµ¬ì¡° ë¶„ì„]:\n",
      "  - í™”ì ìˆ˜: 2ëª…\n",
      "  - PII ë§ˆìŠ¤í‚¹ í† í°: []\n",
      "\n",
      "[3. ìŠ¬ë­ ì •ê·œí™” í›„]:\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? #Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. #Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. #Person2#:...\n",
      "\n",
      "[4. TF-IDF í‚¤ì›Œë“œ]: ë§¤ë…„ / í•œ / ë²ˆ / í•´ìš” / ì˜ì‚¬ ì„ ìƒë‹˜\n",
      "\n",
      "[5. 'balanced' í”„ë¡¬í”„íŠ¸ ì ìš© (6-1, 6-2)]:\n",
      "ë‹¤ìŒì€ 2ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ë‘ ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \n",
      "\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? #Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. #Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. #Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”. #Person1#: ìŒ, ì‹¬ê°í•œ ì§ˆë³‘ì„ í”¼í•˜ë ¤ë©´ ë¯¸ë¦¬ ë°œê²¬í•˜ëŠ” ê²Œ ì œì¼ ì¢‹ê±°ë“ ìš”. ë³¸ì¸ì„ ìœ„í•´ì„œë¼ë„ ë§¤ë…„ í•œ ë²ˆì€ ì˜¤ì„¸ìš”. #Person2#: ì•Œê² ìŠµë‹ˆë‹¤. #Person1#: ì—¬ê¸° ì¢€ ë³¼ê¹Œìš”. ëˆˆê³¼ ê·€ëŠ” ê´œì°®ìœ¼ì‹œë„¤ìš”. ê¹Šê²Œ ìˆ¨ í•œ ë²ˆ ì‰¬ì–´ë³´ì„¸ìš”. Mr. Smith, ë‹´ë°° í”¼ìš°ì„¸ìš”? #Pe...\n",
      "\n",
      "[6. 'keyword_aware' í”„ë¡¬í”„íŠ¸ ì ìš© (TF-IDF + 6-1, 6-2)]:\n",
      "ë‹¤ìŒì€ 2ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ë‘ ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. ì£¼ìš” í‚¤ì›Œë“œ: ë§¤ë…„ / í•œ / ë²ˆ / í•´ìš” / ì˜ì‚¬ ì„ ìƒë‹˜. \n",
      "\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? #Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. #Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. #Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”. #Person1#: ìŒ, ì‹¬ê°í•œ ì§ˆë³‘ì„ í”¼í•˜ë ¤ë©´ ë¯¸ë¦¬ ë°œê²¬í•˜ëŠ” ê²Œ ì œì¼ ì¢‹ê±°ë“ ìš”. ë³¸ì¸ì„ ìœ„í•´ì„œë¼ë„ ë§¤ë…„ í•œ ë²ˆì€ ì˜¤ì„¸ìš”. #Person2#: ì•Œê² ìŠµë‹ˆë‹¤. #Person1#: ì—¬ê¸° ì¢€ ë³¼ê¹Œìš”. ëˆˆê³¼ ê·€ëŠ” ê´œì°®ìœ¼ì‹œë„¤ìš”. ê¹Šê²Œ ìˆ¨ ...\n",
      "\n",
      "[7. ë™ì  ì¶”ë¡  ì„¤ì •]:\n",
      "  - ê¸¸ì´ ì¹´í…Œê³ ë¦¬: medium\n",
      "  - ê°ì§€ëœ ì£¼ì œ: medical\n",
      "  - max_new_tokens: 100\n",
      "\n",
      "âœ… decode_with_speaker_tags, postprocess_summary í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3-1. Special Token ë™ì  ìˆ˜ì§‘ ë° ë“±ë¡\n",
    "# ============================================================================\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# ë°ì´í„° ê²½ë¡œ ì •ì˜ (ì»¤ë„ ì¬ì‹œì‘ í›„ì—ë„ ë™ì‘í•˜ë„ë¡)\n",
    "data_path = \"./data/\"\n",
    "\n",
    "# ë§ˆìŠ¤í‚¹ í† í° & ë°œí™”ì í† í° íŒ¨í„´ ì •ì˜\n",
    "MASK_PAT = r\"#\\w+#\"      # ëª¨ë“  #...# í˜•íƒœì˜ í† í°\n",
    "PERSON_PAT = r\"#Person\\d+#\"  # ë°œí™”ì í† í°ë§Œ\n",
    "# 6-1, 6-2ì—ì„œ í™œìš©í•  ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹ íŒ¨í„´ (ë°œí™”ì ì œì™¸)\n",
    "PII_MASK_PAT = r\"#(?!Person\\d+#)\\w+#\"  # #Person1# ë“± ì œì™¸í•œ ë§ˆìŠ¤í‚¹ í† í°\n",
    "\n",
    "def get_mask_tokens(text):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ì—ì„œ ëª¨ë“  ë§ˆìŠ¤í‚¹ í† í° ì¶”ì¶œ (#Word# í˜•íƒœ)\"\"\"\n",
    "    return re.findall(MASK_PAT, text)\n",
    "\n",
    "def get_person_tokens(text):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ì—ì„œ ë°œí™”ì í† í°ë§Œ ì¶”ì¶œ (#PersonN# í˜•íƒœ)\"\"\"\n",
    "    return re.findall(PERSON_PAT, text)\n",
    "\n",
    "def get_pii_mask_tokens(text):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ì—ì„œ ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹ í† í°ë§Œ ì¶”ì¶œ (ë°œí™”ì ì œì™¸)\"\"\"\n",
    "    return re.findall(PII_MASK_PAT, text)\n",
    "\n",
    "def collect_special_tokens_from_data(dialogues):\n",
    "    \"\"\"\n",
    "    í•™ìŠµ ë°ì´í„°ì—ì„œ ì‹¤ì œ ë“±ì¥í•˜ëŠ” ëª¨ë“  special token ìˆ˜ì§‘\n",
    "    - #Person1#, #Person2# ë“± ë°œí™”ì í† í°\n",
    "    - #PhoneNumber#, #Address# ë“± ë§ˆìŠ¤í‚¹ í† í°\n",
    "    \"\"\"\n",
    "    all_tokens = set()\n",
    "    for d in dialogues:\n",
    "        all_tokens.update(get_mask_tokens(d))\n",
    "    return sorted(list(all_tokens))\n",
    "\n",
    "# Train ë°ì´í„°ì—ì„œ special tokens ìˆ˜ì§‘\n",
    "train_file = os.path.join(data_path, 'train.csv')\n",
    "train_df_for_tokens = pd.read_csv(train_file)\n",
    "DYNAMIC_SPECIAL_TOKENS = collect_special_tokens_from_data(train_df_for_tokens['dialogue'])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ”– ë™ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ Special Tokens:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ì´ {len(DYNAMIC_SPECIAL_TOKENS)}ê°œ: {DYNAMIC_SPECIAL_TOKENS}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. ì •ê·œí‘œí˜„ì‹ ê¸°ë°˜ íŒ¨í„´ í™œìš© ì „ëµ\n",
    "# ============================================================================\n",
    "# 6-1. í™”ì ìˆ˜ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ì— í¬í•¨\n",
    "# 6-2. ë§ˆìŠ¤í‚¹ëœ ì •ë³´(#PhoneNumber# ë“±)ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë¼ê³  ì§€ì‹œ\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_dialogue_structure(dialogue_text):\n",
    "    \"\"\"\n",
    "    ëŒ€í™” êµ¬ì¡° ë¶„ì„: í™”ì ìˆ˜, ë§ˆìŠ¤í‚¹ í† í° ì •ë³´ ì¶”ì¶œ\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            'speakers': set of speaker tokens,\n",
    "            'num_speakers': number of speakers,\n",
    "            'pii_masks': set of PII masking tokens,\n",
    "            'has_pii_masks': whether PII masks exist\n",
    "        }\n",
    "    \"\"\"\n",
    "    speakers = set(get_person_tokens(dialogue_text))\n",
    "    pii_masks = set(get_pii_mask_tokens(dialogue_text))\n",
    "    \n",
    "    return {\n",
    "        'speakers': speakers,\n",
    "        'num_speakers': len(speakers) if speakers else 2,  # ê¸°ë³¸ê°’ 2ëª…\n",
    "        'pii_masks': pii_masks,\n",
    "        'has_pii_masks': len(pii_masks) > 0,\n",
    "        'pii_mask_examples': list(pii_masks)[:3]  # ì˜ˆì‹œ ìµœëŒ€ 3ê°œ\n",
    "    }\n",
    "\n",
    "def build_speaker_aware_prompt(num_speakers):\n",
    "    \"\"\"\n",
    "    6-1. í™”ì ìˆ˜ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì•Œë ¤ì£¼ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "    - \"í•œ ì‚¬ëŒ ì´ì•¼ê¸°ë§Œ ìš”ì•½í•˜ëŠ”\" ë²„ê·¸ ë°©ì§€\n",
    "    \"\"\"\n",
    "    if num_speakers == 1:\n",
    "        return \"ë‹¤ìŒì€ 1ëª…ì˜ ë…ë°±ì…ë‹ˆë‹¤. \"\n",
    "    elif num_speakers == 2:\n",
    "        return f\"ë‹¤ìŒì€ 2ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ë‘ ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \"\n",
    "    else:\n",
    "        return f\"ë‹¤ìŒì€ {num_speakers}ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ê° ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \"\n",
    "\n",
    "def build_pii_mask_instruction(pii_masks, show_examples=True):\n",
    "    \"\"\"\n",
    "    6-2. ë§ˆìŠ¤í‚¹ëœ ì •ë³´ ìœ ì§€ ì§€ì‹œ í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "    - ìš”ì•½ë¬¸ì—ì„œ ì—‰ëš±í•œ ì „í™”ë²ˆí˜¸/ì£¼ì†Œ ìƒì„± ë°©ì§€\n",
    "    - ROUGE ì ìˆ˜ í–¥ìƒ (ë ˆí¼ëŸ°ìŠ¤ì™€ ë™ì¼í•œ ë§ˆìŠ¤í‚¹ í† í° ìœ ì§€)\n",
    "    \"\"\"\n",
    "    if not pii_masks:\n",
    "        return \"\"\n",
    "    \n",
    "    if show_examples:\n",
    "        examples = list(pii_masks)[:3]  # ìµœëŒ€ 3ê°œ ì˜ˆì‹œ\n",
    "        examples_str = \", \".join(examples)\n",
    "        return (\n",
    "            f\"ëŒ€í™” ì¤‘ ê°œì¸ì •ë³´ëŠ” {examples_str} ì™€ ê°™ì´ ë§ˆìŠ¤í‚¹ë˜ì–´ ìˆìŠµë‹ˆë‹¤. \"\n",
    "            \"ìš”ì•½ë¬¸ì—ì„œë„ ì´ ë§ˆìŠ¤í‚¹ í‘œí˜„ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”. \"\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            \"ëŒ€í™” ì¤‘ ê°œì¸ì •ë³´ëŠ” ì´ë¯¸ ë§ˆìŠ¤í‚¹ë˜ì–´ ìˆìŠµë‹ˆë‹¤. \"\n",
    "            \"ìš”ì•½ë¬¸ì—ì„œë„ ë§ˆìŠ¤í‚¹ í‘œí˜„ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•´ ì£¼ì„¸ìš”. \"\n",
    "        )\n",
    "\n",
    "# 6. ì„¤ì •\n",
    "REGEX_PATTERN_CONFIG = {\n",
    "    \"use_speaker_count\": True,      # 6-1. í™”ì ìˆ˜ ëª…ì‹œ\n",
    "    \"use_pii_mask_instruction\": True,  # 6-2. ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ\n",
    "    \"show_pii_examples\": True,       # ë§ˆìŠ¤í‚¹ ì˜ˆì‹œ í‘œì‹œ ì—¬ë¶€\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ” 6. ì •ê·œí‘œí˜„ì‹ ê¸°ë°˜ íŒ¨í„´ í™œìš© ì „ëµ:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  6-1. í™”ì ìˆ˜ ëª…ì‹œ: {REGEX_PATTERN_CONFIG['use_speaker_count']}\")\n",
    "print(f\"  6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: {REGEX_PATTERN_CONFIG['use_pii_mask_instruction']}\")\n",
    "\n",
    "# ì˜ˆì‹œ í…ŒìŠ¤íŠ¸\n",
    "sample_analysis = analyze_dialogue_structure(train_df_for_tokens['dialogue'].iloc[0])\n",
    "print(f\"\\nğŸ“Š ëŒ€í™” êµ¬ì¡° ë¶„ì„ ì˜ˆì‹œ:\")\n",
    "print(f\"  - í™”ì ìˆ˜: {sample_analysis['num_speakers']}ëª…\")\n",
    "print(f\"  - í™”ì í† í°: {sample_analysis['speakers']}\")\n",
    "print(f\"  - PII ë§ˆìŠ¤í‚¹ ì—¬ë¶€: {sample_analysis['has_pii_masks']}\")\n",
    "print(f\"  - PII ë§ˆìŠ¤í‚¹ ì˜ˆì‹œ: {sample_analysis['pii_mask_examples']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬: ìŠ¬ë­/ê°íƒ„ì‚¬/ì˜ì„±ì–´ ì •ê·œí™”\n",
    "# ============================================================================\n",
    "\n",
    "# ìŠ¬ë­/ê°íƒ„ì‚¬/ì˜ì„±ì–´ â†’ ë¬¸ì–´ì²´ ë§¤í•‘ ì‚¬ì „\n",
    "SLANG_MAP = {\n",
    "    # ì›ƒìŒ í‘œí˜„\n",
    "    \"ã…‹ã…‹ã…‹ã…‹ã…‹\": \"ë§¤ìš° ì›ƒê¸°ë‹¤\",\n",
    "    \"ã…‹ã…‹ã…‹ã…‹\": \"ë§¤ìš° ì›ƒê¸°ë‹¤\",\n",
    "    \"ã…‹ã…‹ã…‹\": \"ì›ƒê¸°ë‹¤\",\n",
    "    \"ã…‹ã…‹\": \"ì›ƒê¸°ë‹¤\",\n",
    "    \"ã…ã…ã…ã…\": \"ì›ƒê¸°ë‹¤\",\n",
    "    \"ã…ã…ã…\": \"ì›ƒê¸°ë‹¤\",\n",
    "    \"ã…ã…\": \"ì›ƒê¸°ë‹¤\",\n",
    "    \"ã…‹\": \"\",  # ë‹¨ì¼ ã…‹ëŠ” ì œê±°\n",
    "    \"ã…\": \"\",  # ë‹¨ì¼ ã…ëŠ” ì œê±°\n",
    "    \n",
    "    # ìŠ¬í””/ìš¸ìŒ í‘œí˜„\n",
    "    \"ã… ã… ã… ã… \": \"ë§¤ìš° ìŠ¬í”„ë‹¤\",\n",
    "    \"ã… ã… ã… \": \"ìŠ¬í”„ë‹¤\",\n",
    "    \"ã… ã… \": \"ìŠ¬í”„ë‹¤\",\n",
    "    \"ã…œã…œã…œã…œ\": \"ë§¤ìš° ìŠ¬í”„ë‹¤\",\n",
    "    \"ã…œã…œã…œ\": \"ìŠ¬í”„ë‹¤\",\n",
    "    \"ã…œã…œ\": \"ìŠ¬í”„ë‹¤\",\n",
    "    \"ã… \": \"\",\n",
    "    \"ã…œ\": \"\",\n",
    "    \n",
    "    # ë†€ëŒ/ê°íƒ„ í‘œí˜„\n",
    "    \"í—\": \"ë†€ëë‹¤\",\n",
    "    \"í—ã…‹\": \"ë†€ëë‹¤\",\n",
    "    \"ëŒ€ë°•\": \"ë†€ëë‹¤\",\n",
    "    \"ëŒ€ë°•ã…‹ã…‹\": \"ë†€ëë‹¤\",\n",
    "    \"ì™€\": \"ë†€ëë‹¤\",\n",
    "    \"ì™€ìš°\": \"ë†€ëë‹¤\",\n",
    "    \"ìš°ì™€\": \"ë†€ëë‹¤\",\n",
    "    \"ì˜¤ì˜¤\": \"ë†€ëë‹¤\",\n",
    "    \"ì˜¤ì˜¤ì˜¤\": \"ë§¤ìš° ë†€ëë‹¤\",\n",
    "    \"ã…‡ã…‡\": \"ê·¸ë ‡ë‹¤\",\n",
    "    \"ã„´ã„´\": \"ì•„ë‹ˆë‹¤\",\n",
    "    \n",
    "    # ê¸ì •/ë™ì˜ í‘œí˜„\n",
    "    \"ã„±ã„±\": \"ê°€ì\",\n",
    "    \"ã„±ã„±ã„±\": \"ë¹¨ë¦¬ ê°€ì\",\n",
    "    \"ã…‡ã…‹\": \"ì•Œê² ë‹¤\",\n",
    "    \"ã…‡ã…‹ã…‡ã…‹\": \"ì•Œê² ë‹¤\",\n",
    "    \"ë„¹\": \"ë„¤\",\n",
    "    \"ë„µ\": \"ë„¤\",\n",
    "    \"ìš¤\": \"ìš”\",\n",
    "    \"ìš©\": \"ìš”\",\n",
    "    \"ì¥¬\": \"ì¤˜\",\n",
    "    \n",
    "    # ë¶€ì •/ì§œì¦ í‘œí˜„\n",
    "    \"ì•„ã…‹ã…‹\": \"ì•„\",\n",
    "    \"ì—íœ´\": \"í•œìˆ¨\",\n",
    "    \"ã…¡ã…¡\": \"ì§œì¦ë‚œë‹¤\",\n",
    "    \";;\": \"ë‹¹í™©ìŠ¤ëŸ½ë‹¤\",\n",
    "    \";;;\": \"ë§¤ìš° ë‹¹í™©ìŠ¤ëŸ½ë‹¤\",\n",
    "    \n",
    "    # ê°•ì¡° í‘œí˜„\n",
    "    \"ì§„ì§œã…‹ã…‹\": \"ì •ë§\",\n",
    "    \"ë ˆì•Œ\": \"ì •ë§\",\n",
    "    \"ë¦¬ì–¼\": \"ì •ë§\",\n",
    "    \"ì°\": \"ì§„ì§œ\",\n",
    "    \"ì©ë‹¤\": \"ëŒ€ë‹¨í•˜ë‹¤\",\n",
    "    \"ì©”ì–´\": \"ëŒ€ë‹¨í•˜ë‹¤\",\n",
    "    \"ì§±\": \"ìµœê³ ë‹¤\",\n",
    "    \n",
    "    # ê¸°íƒ€ ì¸í„°ë„· ìš©ì–´\n",
    "    \"ã„·ã„·\": \"ë†€ëë‹¤\",\n",
    "    \"ã„·ã„·ã„·\": \"ë§¤ìš° ë†€ëë‹¤\",\n",
    "    \"ã…‚ã…‚\": \"ì•ˆë…•\",\n",
    "    \"ã…ƒã…ƒ\": \"ì•ˆë…•\",\n",
    "    \"ã„³\": \"ê°ì‚¬\",\n",
    "    \"ã„±ã……\": \"ê°ì‚¬\",\n",
    "    \"ã…ˆã……\": \"ì£„ì†¡\",\n",
    "    \"ã…ã„¹\": \"ëª¨ë¥´ê² ë‹¤\",\n",
    "    \"ã„¹ã…‡\": \"ì •ë§\",\n",
    "    \"ã…‡ã„·\": \"ì–´ë””\",\n",
    "}\n",
    "\n",
    "# ë°˜ë³µ ë¬¸ì ì •ê·œí™” íŒ¨í„´\n",
    "REPEAT_PATTERNS = [\n",
    "    (r'ã…‹{5,}', 'ã…‹ã…‹ã…‹ã…‹'),\n",
    "    (r'ã…{5,}', 'ã…ã…ã…ã…'),\n",
    "    (r'ã… {5,}', 'ã… ã… ã… ã… '),\n",
    "    (r'ã…œ{5,}', 'ã…œã…œã…œã…œ'),\n",
    "    (r'\\.{4,}', '...'),\n",
    "    (r'!{3,}', '!!'),\n",
    "    (r'\\?{3,}', '??'),\n",
    "    (r'~{3,}', '~~'),\n",
    "]\n",
    "\n",
    "def normalize_repeated_chars(text):\n",
    "    \"\"\"ë°˜ë³µ ë¬¸ìë¥¼ ì •ê·œí™”\"\"\"\n",
    "    for pattern, replacement in REPEAT_PATTERNS:\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "    return text\n",
    "\n",
    "def normalize_slang(text):\n",
    "    \"\"\"ìŠ¬ë­/ê°íƒ„ì‚¬/ì˜ì„±ì–´ë¥¼ ë¬¸ì–´ì²´ë¡œ ë³€í™˜\"\"\"\n",
    "    text = normalize_repeated_chars(text)\n",
    "    sorted_slang = sorted(SLANG_MAP.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "    for slang, replacement in sorted_slang:\n",
    "        if slang in text:\n",
    "            if replacement:\n",
    "                text = text.replace(slang, f\" {replacement} \")\n",
    "            else:\n",
    "                text = text.replace(slang, \" \")\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def preprocess_dialogue(text, normalize_slang_flag=True):\n",
    "    \"\"\"ëŒ€í™” ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return text\n",
    "    if normalize_slang_flag:\n",
    "        text = normalize_slang(text)\n",
    "    return text\n",
    "\n",
    "PREPROCESS_CONFIG = {\n",
    "    \"normalize_slang\": True,\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ—£ï¸ êµ¬ì–´ì²´ ì „ì²˜ë¦¬ (ìŠ¬ë­/ê°íƒ„ì‚¬ ì •ê·œí™”):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ì´ {len(SLANG_MAP)}ê°œ ìŠ¬ë­ ë§¤í•‘ ì •ì˜\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ë° í”„ë¡¬í”„íŠ¸ í™œìš©\n",
    "# ============================================================================\n",
    "# (sklearnì€ ì…€ ìƒë‹¨ì—ì„œ ì´ë¯¸ importë¨)\n",
    "\n",
    "# TF-IDF ì„¤ì •\n",
    "TFIDF_CONFIG = {\n",
    "    \"min_df\": 2,           # ìµœì†Œ ë¬¸ì„œ ë¹ˆë„\n",
    "    \"max_df\": 0.9,         # ìµœëŒ€ ë¬¸ì„œ ë¹ˆë„ (90% ì´ìƒ ë“±ì¥ ë‹¨ì–´ ì œì™¸)\n",
    "    \"max_features\": 2000,  # ìµœëŒ€ feature ìˆ˜\n",
    "    \"ngram_range\": (1, 2), # unigram + bigram\n",
    "    \"top_k\": 5,            # ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š TF-IDF ì„¤ì •:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  min_df: {TFIDF_CONFIG['min_df']}\")\n",
    "print(f\"  max_df: {TFIDF_CONFIG['max_df']}\")\n",
    "print(f\"  max_features: {TFIDF_CONFIG['max_features']}\")\n",
    "print(f\"  ngram_range: {TFIDF_CONFIG['ngram_range']}\")\n",
    "print(f\"  top_k: {TFIDF_CONFIG['top_k']}\")\n",
    "\n",
    "# TF-IDF Vectorizer ì´ˆê¸°í™” (í•œêµ­ì–´ ë¶ˆìš©ì–´ í¬í•¨)\n",
    "KOREAN_STOPWORDS = [\n",
    "    'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼',\n",
    "    'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 'ì´ë‹¤', 'ìˆë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆ˜', 'ê²ƒ', 'ë“±',\n",
    "    'ê·¸', 'ì €', 'ì´ëŸ°', 'ì €ëŸ°', 'ê·¸ëŸ°', 'ì–´ë–¤', 'ë¬´ìŠ¨', 'ë­', 'ë­”ê°€', 'ë­˜',\n",
    "    'ë‚˜', 'ë„ˆ', 'ìš°ë¦¬', 'ì €í¬', 'ê·¸ë…€', 'ê·¸', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ê·¸ê²ƒ',\n",
    "    'ì•„', 'ì–´', 'ì˜¤', 'ì˜ˆ', 'ë„¤', 'ì‘', 'ìŒ', 'ì—„', 'ì•„ë‹ˆ', 'ì•ˆ',\n",
    "    'ì¢€', 'ì˜', 'ë”', 'ëœ', 'ë§ì´', 'ì¡°ê¸ˆ', 'ì•„ì£¼', 'ë§¤ìš°', 'ì •ë§', 'ì§„ì§œ',\n",
    "    'ê·¸ë˜ì„œ', 'ê·¸ëŸ¬ë©´', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ë¦¬ê³ ', 'ë˜', 'ë˜í•œ',\n",
    "    '#Person1#', '#Person2#', '#Person3#', '#Person4#', '#Person5#',\n",
    "]\n",
    "\n",
    "class TFIDFKeywordExtractor:\n",
    "    \"\"\"TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œê¸°\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus=None, max_features=1000, ngram_range=(1, 2)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            corpus: TF-IDF í•™ìŠµìš© ì½”í¼ìŠ¤ (ëŒ€í™” ë¦¬ìŠ¤íŠ¸)\n",
    "            max_features: ìµœëŒ€ feature ìˆ˜\n",
    "            ngram_range: n-gram ë²”ìœ„\n",
    "        \"\"\"\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            ngram_range=ngram_range,\n",
    "            token_pattern=r'[ê°€-í£]+',  # í•œê¸€ë§Œ ì¶”ì¶œ\n",
    "            stop_words=KOREAN_STOPWORDS,\n",
    "        )\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        if corpus is not None:\n",
    "            self.fit(corpus)\n",
    "    \n",
    "    def fit(self, corpus):\n",
    "        \"\"\"ì½”í¼ìŠ¤ë¡œ TF-IDF í•™ìŠµ\"\"\"\n",
    "        # ë§ˆìŠ¤í‚¹ í† í° ì œê±° í›„ í•™ìŠµ\n",
    "        cleaned_corpus = [re.sub(MASK_PAT, ' ', doc) for doc in corpus]\n",
    "        self.vectorizer.fit(cleaned_corpus)\n",
    "        self.feature_names = self.vectorizer.get_feature_names_out()\n",
    "        self.is_fitted = True\n",
    "        print(f\"âœ… TF-IDF í•™ìŠµ ì™„ë£Œ: {len(self.feature_names)}ê°œ feature\")\n",
    "    \n",
    "    def extract_keywords(self, text, top_k=5):\n",
    "        \"\"\"\n",
    "        ë‹¨ì¼ ë¬¸ì„œì—ì„œ TF-IDF ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "        \n",
    "        Args:\n",
    "            text: ëŒ€í™” í…ìŠ¤íŠ¸\n",
    "            top_k: ì¶”ì¶œí•  í‚¤ì›Œë“œ ìˆ˜\n",
    "        \n",
    "        Returns:\n",
    "            List of (keyword, score) tuples\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            return []\n",
    "        \n",
    "        # ë§ˆìŠ¤í‚¹ í† í° ì œê±°\n",
    "        cleaned_text = re.sub(MASK_PAT, ' ', text)\n",
    "        \n",
    "        try:\n",
    "            tfidf_vector = self.vectorizer.transform([cleaned_text])\n",
    "            scores = tfidf_vector.toarray()[0]\n",
    "            \n",
    "            # ìƒìœ„ kê°œ ì¸ë±ìŠ¤\n",
    "            top_indices = scores.argsort()[-top_k:][::-1]\n",
    "            \n",
    "            keywords = []\n",
    "            for idx in top_indices:\n",
    "                if scores[idx] > 0:\n",
    "                    keywords.append((self.feature_names[idx], scores[idx]))\n",
    "            \n",
    "            return keywords\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def get_keyword_string(self, text, top_k=5, separator=\" / \"):\n",
    "        \"\"\"í‚¤ì›Œë“œë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜\"\"\"\n",
    "        keywords = self.extract_keywords(text, top_k)\n",
    "        if not keywords:\n",
    "            return \"\"\n",
    "        return separator.join([kw for kw, score in keywords])\n",
    "\n",
    "# TF-IDF ì¶”ì¶œê¸° ì´ˆê¸°í™” (í•™ìŠµ ë°ì´í„° ê¸°ë°˜)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™”:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tfidf_extractor = TFIDFKeywordExtractor(\n",
    "    corpus=train_df_for_tokens['dialogue'].tolist(),\n",
    "    max_features=2000,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "# ì˜ˆì‹œ í…ŒìŠ¤íŠ¸\n",
    "sample_text = train_df_for_tokens['dialogue'].iloc[0]\n",
    "sample_keywords = tfidf_extractor.get_keyword_string(sample_text, top_k=5)\n",
    "print(f\"\\nğŸ“ TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì˜ˆì‹œ:\")\n",
    "print(f\"  í‚¤ì›Œë“œ: {sample_keywords}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5-1. TF-IDF í‚¤ì›Œë“œ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ë¹Œë” (6-1, 6-2 í†µí•©)\n",
    "# ============================================================================\n",
    "\n",
    "def build_tfidf_keyword_prompt(dialogue_text, tfidf_extractor, top_k=5, prompt_style=\"keyword\",\n",
    "                                use_speaker_count=True, use_pii_instruction=True):\n",
    "    \"\"\"\n",
    "    TF-IDF í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± (6-1, 6-2 í†µí•©)\n",
    "    \n",
    "    Args:\n",
    "        dialogue_text: ì›ë³¸ ëŒ€í™” í…ìŠ¤íŠ¸\n",
    "        tfidf_extractor: TFIDFKeywordExtractor ì¸ìŠ¤í„´ìŠ¤\n",
    "        top_k: ì¶”ì¶œí•  í‚¤ì›Œë“œ ìˆ˜\n",
    "        prompt_style: \"keyword\" (í‚¤ì›Œë“œë§Œ), \"keyword_aware\" (í‚¤ì›Œë“œ+ë°œí™”ììˆ˜)\n",
    "        use_speaker_count: 6-1. í™”ì ìˆ˜ ëª…ì‹œ ì—¬ë¶€\n",
    "        use_pii_instruction: 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ ì—¬ë¶€\n",
    "    \n",
    "    Returns:\n",
    "        í”„ë¡¬í”„íŠ¸ê°€ ì¶”ê°€ëœ ëŒ€í™” í…ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    # ëŒ€í™” êµ¬ì¡° ë¶„ì„\n",
    "    analysis = analyze_dialogue_structure(dialogue_text)\n",
    "    num_speakers = analysis['num_speakers']\n",
    "    pii_masks = analysis['pii_masks']\n",
    "    \n",
    "    # í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "    keywords = tfidf_extractor.get_keyword_string(dialogue_text, top_k=top_k)\n",
    "    \n",
    "    prompt_parts = []\n",
    "    \n",
    "    # 6-1. í™”ì ìˆ˜ ëª…ì‹œ (keyword_aware ìŠ¤íƒ€ì¼ì—ì„œë§Œ)\n",
    "    if prompt_style == \"keyword_aware\" and use_speaker_count:\n",
    "        prompt_parts.append(build_speaker_aware_prompt(num_speakers))\n",
    "    elif prompt_style == \"keyword\":\n",
    "        prompt_parts.append(\"ë‹¤ìŒ ëŒ€í™”ë¥¼ í•œ ë¬¸ë‹¨ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì–´ì²´ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \")\n",
    "    \n",
    "    # TF-IDF í‚¤ì›Œë“œ ì¶”ê°€\n",
    "    if keywords:\n",
    "        prompt_parts.append(f\"ì£¼ìš” í‚¤ì›Œë“œ: {keywords}. \")\n",
    "    \n",
    "    # 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ\n",
    "    if use_pii_instruction and pii_masks:\n",
    "        prompt_parts.append(build_pii_mask_instruction(pii_masks, show_examples=REGEX_PATTERN_CONFIG['show_pii_examples']))\n",
    "    \n",
    "    prompt_parts.append(\"\\n\\n\")\n",
    "    \n",
    "    prompt = \"\".join(prompt_parts)\n",
    "    return prompt + dialogue_text\n",
    "\n",
    "# ============================================================================\n",
    "# 5-2. ëŒ€í™” ê¸¸ì´/ì£¼ì œë³„ ë™ì  ì¶”ë¡  ì„¤ì •\n",
    "# ============================================================================\n",
    "\n",
    "# ê¸¸ì´ ê¸°ë°˜ ì¶”ë¡  ì„¤ì •\n",
    "LENGTH_BASED_INFERENCE_CONFIG = {\n",
    "    # ì§§ì€ ëŒ€í™” (500ì ì´í•˜)\n",
    "    \"short\": {\n",
    "        \"char_threshold\": 500,\n",
    "        \"max_new_tokens\": 60,\n",
    "        \"num_beams\": 4,\n",
    "        \"length_penalty\": 1.0,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "    },\n",
    "    # ì¤‘ê°„ ëŒ€í™” (500~1500ì)\n",
    "    \"medium\": {\n",
    "        \"char_threshold\": 1500,\n",
    "        \"max_new_tokens\": 80,\n",
    "        \"num_beams\": 5,\n",
    "        \"length_penalty\": 1.0,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "    },\n",
    "    # ê¸´ ëŒ€í™” (1500ì ì´ˆê³¼)\n",
    "    \"long\": {\n",
    "        \"char_threshold\": float('inf'),\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"num_beams\": 6,\n",
    "        \"length_penalty\": 1.2,  # ê¸´ ëŒ€í™”ëŠ” ìš”ì•½ë„ ê¸¸ê²Œ\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "    },\n",
    "}\n",
    "\n",
    "# ì£¼ì œ í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ë¡  ì„¤ì • (íŠ¹ì • ì£¼ì œëŠ” ë” ê¸´ ìš”ì•½ í•„ìš”)\n",
    "TOPIC_KEYWORDS = {\n",
    "    \"medical\": [\"ë³‘ì›\", \"ì˜ì‚¬\", \"ì¹˜ë£Œ\", \"ì§„ë£Œ\", \"ìˆ˜ìˆ \", \"ì•½\", \"ì¦ìƒ\", \"ì§„ë‹¨\", \"ê²€ì‚¬\"],\n",
    "    \"insurance\": [\"ë³´í—˜\", \"ê°€ì…\", \"ë³´ì¥\", \"ì²­êµ¬\", \"ê³„ì•½\", \"ë‚©ì…\", \"ë³´ìƒ\", \"ì•½ê´€\"],\n",
    "    \"finance\": [\"ëŒ€ì¶œ\", \"ì´ì\", \"ê¸ˆë¦¬\", \"ê³„ì¢Œ\", \"ì†¡ê¸ˆ\", \"ì…ê¸ˆ\", \"ì¶œê¸ˆ\", \"íˆ¬ì\", \"ì£¼ì‹\"],\n",
    "    \"travel\": [\"ì—¬í–‰\", \"ë¹„í–‰ê¸°\", \"í˜¸í…”\", \"ì˜ˆì•½\", \"ê´€ê´‘\", \"ìˆ™ì†Œ\", \"í•­ê³µ\"],\n",
    "}\n",
    "\n",
    "TOPIC_INFERENCE_CONFIG = {\n",
    "    \"medical\": {\"max_new_tokens\": 100, \"length_penalty\": 1.3},\n",
    "    \"insurance\": {\"max_new_tokens\": 100, \"length_penalty\": 1.3},\n",
    "    \"finance\": {\"max_new_tokens\": 95, \"length_penalty\": 1.2},\n",
    "    \"travel\": {\"max_new_tokens\": 85, \"length_penalty\": 1.1},\n",
    "    \"default\": {\"max_new_tokens\": 80, \"length_penalty\": 1.0},\n",
    "}\n",
    "\n",
    "def detect_topic(dialogue_text):\n",
    "    \"\"\"ëŒ€í™” ì£¼ì œ ê°ì§€\"\"\"\n",
    "    text_lower = dialogue_text.lower()\n",
    "    \n",
    "    topic_scores = {}\n",
    "    for topic, keywords in TOPIC_KEYWORDS.items():\n",
    "        score = sum(1 for kw in keywords if kw in text_lower)\n",
    "        if score > 0:\n",
    "            topic_scores[topic] = score\n",
    "    \n",
    "    if topic_scores:\n",
    "        return max(topic_scores, key=topic_scores.get)\n",
    "    return \"default\"\n",
    "\n",
    "def get_length_category(dialogue_text):\n",
    "    \"\"\"ëŒ€í™” ê¸¸ì´ ì¹´í…Œê³ ë¦¬ ë°˜í™˜\"\"\"\n",
    "    length = len(dialogue_text)\n",
    "    \n",
    "    if length <= LENGTH_BASED_INFERENCE_CONFIG[\"short\"][\"char_threshold\"]:\n",
    "        return \"short\"\n",
    "    elif length <= LENGTH_BASED_INFERENCE_CONFIG[\"medium\"][\"char_threshold\"]:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"long\"\n",
    "\n",
    "def get_dynamic_inference_config(dialogue_text, use_topic=True, use_length=True):\n",
    "    \"\"\"\n",
    "    ëŒ€í™” íŠ¹ì„±ì— ë”°ë¥¸ ë™ì  ì¶”ë¡  ì„¤ì • ë°˜í™˜\n",
    "    \n",
    "    Args:\n",
    "        dialogue_text: ëŒ€í™” í…ìŠ¤íŠ¸\n",
    "        use_topic: ì£¼ì œ ê¸°ë°˜ ì„¤ì • ì‚¬ìš© ì—¬ë¶€\n",
    "        use_length: ê¸¸ì´ ê¸°ë°˜ ì„¤ì • ì‚¬ìš© ì—¬ë¶€\n",
    "    \n",
    "    Returns:\n",
    "        dict: ì¶”ë¡  ì„¤ì • (max_new_tokens, num_beams, length_penalty ë“±)\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"max_new_tokens\": 80,\n",
    "        \"num_beams\": 5,\n",
    "        \"length_penalty\": 1.0,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "    }\n",
    "    \n",
    "    # 1. ê¸¸ì´ ê¸°ë°˜ ì„¤ì •\n",
    "    if use_length:\n",
    "        length_cat = get_length_category(dialogue_text)\n",
    "        length_config = LENGTH_BASED_INFERENCE_CONFIG[length_cat]\n",
    "        config.update({\n",
    "            \"max_new_tokens\": length_config[\"max_new_tokens\"],\n",
    "            \"num_beams\": length_config[\"num_beams\"],\n",
    "            \"length_penalty\": length_config[\"length_penalty\"],\n",
    "            \"no_repeat_ngram_size\": length_config[\"no_repeat_ngram_size\"],\n",
    "        })\n",
    "    \n",
    "    # 2. ì£¼ì œ ê¸°ë°˜ ì„¤ì • (ê¸¸ì´ë³´ë‹¤ ìš°ì„ )\n",
    "    if use_topic:\n",
    "        topic = detect_topic(dialogue_text)\n",
    "        if topic != \"default\":\n",
    "            topic_config = TOPIC_INFERENCE_CONFIG[topic]\n",
    "            config[\"max_new_tokens\"] = max(config[\"max_new_tokens\"], topic_config[\"max_new_tokens\"])\n",
    "            config[\"length_penalty\"] = max(config[\"length_penalty\"], topic_config[\"length_penalty\"])\n",
    "    \n",
    "    return config\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“ ê¸¸ì´/ì£¼ì œ ê¸°ë°˜ ë™ì  ì¶”ë¡  ì„¤ì •:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ê¸¸ì´ ì¹´í…Œê³ ë¦¬:\")\n",
    "for cat, cfg in LENGTH_BASED_INFERENCE_CONFIG.items():\n",
    "    print(f\"  - {cat}: ~{cfg['char_threshold']}ì, max_tokens={cfg['max_new_tokens']}, beams={cfg['num_beams']}\")\n",
    "\n",
    "print(\"\\nì£¼ì œ ì¹´í…Œê³ ë¦¬:\")\n",
    "for topic in TOPIC_KEYWORDS.keys():\n",
    "    cfg = TOPIC_INFERENCE_CONFIG[topic]\n",
    "    print(f\"  - {topic}: max_tokens={cfg['max_new_tokens']}, length_penalty={cfg['length_penalty']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3-2. í”„ë¡¬í”„íŠ¸ ë¹Œë” í•¨ìˆ˜ - ë°œí™”ì/turn êµ¬ì¡° í™œìš© (6-1, 6-2 í†µí•©)\n",
    "# ============================================================================\n",
    "\n",
    "def build_summary_prompt(dialogue_text, prompt_style=\"default\", tfidf_extractor=None, top_k=5,\n",
    "                         use_speaker_count=True, use_pii_instruction=True):\n",
    "    \"\"\"\n",
    "    ëŒ€í™”ì— ìš”ì•½ í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì—ê²Œ íƒœìŠ¤í¬ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì•Œë¦¼\n",
    "    6-1. í™”ì ìˆ˜ ëª…ì‹œ: \"í•œ ì‚¬ëŒ ì´ì•¼ê¸°ë§Œ ìš”ì•½í•˜ëŠ”\" ë²„ê·¸ ë°©ì§€\n",
    "    6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: ROUGE ì ìˆ˜ í–¥ìƒ\n",
    "    \n",
    "    Args:\n",
    "        dialogue_text: ì›ë³¸ ëŒ€í™” í…ìŠ¤íŠ¸\n",
    "        prompt_style: í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼\n",
    "            - \"default\": ê¸°ë³¸ í”„ë¡¬í”„íŠ¸\n",
    "            - \"speaker_aware\": ë°œí™”ì ìˆ˜ í¬í•¨\n",
    "            - \"minimal\": ìµœì†Œ í”„ë¡¬í”„íŠ¸\n",
    "            - \"keyword\": TF-IDF í‚¤ì›Œë“œ í¬í•¨ (5-1)\n",
    "            - \"keyword_aware\": í‚¤ì›Œë“œ + ë°œí™”ì ìˆ˜ (5-1)\n",
    "            - \"balanced\": 6-1, 6-2 ì „ëµ ì ìš© (NEW)\n",
    "            - \"none\": í”„ë¡¬í”„íŠ¸ ì—†ìŒ\n",
    "        tfidf_extractor: TFIDFKeywordExtractor ì¸ìŠ¤í„´ìŠ¤ (keyword ìŠ¤íƒ€ì¼ìš©)\n",
    "        top_k: í‚¤ì›Œë“œ ì¶”ì¶œ ê°œìˆ˜\n",
    "        use_speaker_count: 6-1. í™”ì ìˆ˜ ëª…ì‹œ ì—¬ë¶€\n",
    "        use_pii_instruction: 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ ì—¬ë¶€\n",
    "    \n",
    "    Returns:\n",
    "        í”„ë¡¬í”„íŠ¸ê°€ ì¶”ê°€ëœ ëŒ€í™” í…ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    if prompt_style == \"none\":\n",
    "        return dialogue_text\n",
    "    \n",
    "    # ëŒ€í™” êµ¬ì¡° ë¶„ì„ (6-1, 6-2ìš©)\n",
    "    analysis = analyze_dialogue_structure(dialogue_text)\n",
    "    num_speakers = analysis['num_speakers']\n",
    "    pii_masks = analysis['pii_masks']\n",
    "    \n",
    "    # 5-1. TF-IDF í‚¤ì›Œë“œ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ (6-1, 6-2 í†µí•©)\n",
    "    if prompt_style in [\"keyword\", \"keyword_aware\"] and tfidf_extractor is not None:\n",
    "        return build_tfidf_keyword_prompt(\n",
    "            dialogue_text, tfidf_extractor, top_k, prompt_style,\n",
    "            use_speaker_count=use_speaker_count,\n",
    "            use_pii_instruction=use_pii_instruction\n",
    "        )\n",
    "    \n",
    "    prompt_parts = []\n",
    "    \n",
    "    # 6-1. í™”ì ìˆ˜ ëª…ì‹œ í”„ë¡¬í”„íŠ¸\n",
    "    if prompt_style == \"balanced\":\n",
    "        # ìƒˆë¡œìš´ \"balanced\" ìŠ¤íƒ€ì¼: 6-1, 6-2 ì „ëµ ëª¨ë‘ ì ìš©\n",
    "        prompt_parts.append(build_speaker_aware_prompt(num_speakers))\n",
    "    elif prompt_style == \"speaker_aware\":\n",
    "        prompt_parts.append(build_speaker_aware_prompt(num_speakers))\n",
    "    elif prompt_style == \"minimal\":\n",
    "        prompt_parts.append(\"ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ì„¸ìš”. \")\n",
    "    else:  # default\n",
    "        if use_speaker_count:\n",
    "            prompt_parts.append(build_speaker_aware_prompt(num_speakers))\n",
    "        else:\n",
    "            prompt_parts.append(\"ë‹¤ìŒì€ ì—¬ëŸ¬ ì‚¬ëŒì´ ë‚˜ëˆˆ ì¼ìƒ ëŒ€í™”ì…ë‹ˆë‹¤. ì´ ëŒ€í™”ë¥¼ í•œ ë¬¸ë‹¨ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì–´ì²´ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \")\n",
    "    \n",
    "    # 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ (balanced, default, speaker_aware ìŠ¤íƒ€ì¼ì—ì„œ)\n",
    "    if use_pii_instruction and pii_masks and prompt_style in [\"balanced\", \"default\", \"speaker_aware\"]:\n",
    "        prompt_parts.append(build_pii_mask_instruction(pii_masks, show_examples=REGEX_PATTERN_CONFIG['show_pii_examples']))\n",
    "    \n",
    "    prompt_parts.append(\"\\n\\n\")\n",
    "    \n",
    "    prompt = \"\".join(prompt_parts)\n",
    "    return prompt + dialogue_text\n",
    "\n",
    "def add_turn_separators(dialogue_text, separator=\"[SEP]\"):\n",
    "    \"\"\"ë°œí™” turn ì‚¬ì´ì— êµ¬ë¶„ì ì¶”ê°€\"\"\"\n",
    "    lines = dialogue_text.strip().split('\\n')\n",
    "    result_lines = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if i > 0 and re.match(PERSON_PAT, line.strip()):\n",
    "            result_lines.append(f\"{separator} {line}\")\n",
    "        else:\n",
    "            result_lines.append(line)\n",
    "    return '\\n'.join(result_lines)\n",
    "\n",
    "# ============================================================================\n",
    "# ì„¤ì • ì—…ë°ì´íŠ¸ (6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš© ì „ëµ ë°˜ì˜)\n",
    "# ============================================================================\n",
    "\n",
    "config_data = {\n",
    "    \"general\": {\n",
    "        \"data_path\": \"./data/\",\n",
    "        \"model_name\": \"digit82/kobart-summarization\",\n",
    "        \"output_dir\": \"./prediction_kobart_v1\"\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "        \"encoder_max_len\": 1024,\n",
    "        \"decoder_max_len\": 100,\n",
    "        \"bos_token\": f\"{tokenizer.bos_token}\",\n",
    "        \"eos_token\": f\"{tokenizer.eos_token}\",\n",
    "        \"special_tokens\": DYNAMIC_SPECIAL_TOKENS\n",
    "    },\n",
    "    # 3-2. í”„ë¡¬í”„íŠ¸ ì„¤ì • (6-1, 6-2 í†µí•©)\n",
    "    \"prompt\": {\n",
    "        \"use_prompt\": True,\n",
    "        \"prompt_style\": \"balanced\",  # NEW: 6-1, 6-2 ì „ëµ ì ìš©\n",
    "        \"use_turn_separator\": False,\n",
    "        \"turn_separator\": \"[SEP]\",\n",
    "        \"tfidf_top_k\": 5,  # 5-1. TF-IDF í‚¤ì›Œë“œ ê°œìˆ˜\n",
    "    },\n",
    "    # 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš© ì„¤ì • (NEW)\n",
    "    \"regex_pattern\": {\n",
    "        \"use_speaker_count\": True,       # 6-1. í™”ì ìˆ˜ ëª…ì‹œ\n",
    "        \"use_pii_instruction\": True,     # 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ\n",
    "        \"show_pii_examples\": True,       # ë§ˆìŠ¤í‚¹ ì˜ˆì‹œ í‘œì‹œ\n",
    "    },\n",
    "    # 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •\n",
    "    \"preprocess\": {\n",
    "        \"normalize_slang\": True,\n",
    "    },\n",
    "    # 5-2. ë™ì  ì¶”ë¡  ì„¤ì •\n",
    "    \"dynamic_inference\": {\n",
    "        \"use_length_based\": True,   # ê¸¸ì´ ê¸°ë°˜ ë™ì  ì„¤ì • ì‚¬ìš©\n",
    "        \"use_topic_based\": True,    # ì£¼ì œ ê¸°ë°˜ ë™ì  ì„¤ì • ì‚¬ìš©\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"num_train_epochs\": 20,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"per_device_eval_batch_size\": 32,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lr_scheduler_type\": 'cosine',\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"eval_strategy\": 'epoch',\n",
    "        \"save_strategy\": 'epoch',\n",
    "        \"save_total_limit\": 5,\n",
    "        \"fp16\": True,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"seed\": 42,\n",
    "        \"logging_dir\": \"./logs\",\n",
    "        \"logging_strategy\": \"epoch\",\n",
    "        \"predict_with_generate\": True,\n",
    "        \"generation_max_length\": 100,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_threshold\": 0.001,\n",
    "        \"report_to\": \"wandb\"\n",
    "    },\n",
    "    \"wandb\": {\n",
    "        \"entity\": \"fc_bootcamp\",\n",
    "        \"project\": \"kobart_v1\",\n",
    "        \"name\": \"baseline_v0\"\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"ckt_path\": \"model ckt path\",\n",
    "        \"result_path\": \"./prediction/\",\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "        \"early_stopping\": True,\n",
    "        \"generate_max_length\": 100,\n",
    "        \"num_beams\": 4,\n",
    "        \"batch_size\": 32,\n",
    "        \"remove_tokens\": ['<usr>', f\"{tokenizer.bos_token}\", f\"{tokenizer.eos_token}\", f\"{tokenizer.pad_token}\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# ì„¤ì • ìš”ì•½ ì¶œë ¥\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“‹ ì „ì²´ ì„¤ì • ìš”ì•½:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nğŸ”– 3-1. Special Tokens:\")\n",
    "print(f\"  - ë™ì  ìˆ˜ì§‘: {len(DYNAMIC_SPECIAL_TOKENS)}ê°œ\")\n",
    "\n",
    "print(\"\\nğŸ“ 3-2. í”„ë¡¬í”„íŠ¸ ì„¤ì •:\")\n",
    "print(f\"  - í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {config_data['prompt']['use_prompt']}\")\n",
    "print(f\"  - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: {config_data['prompt']['prompt_style']}\")\n",
    "print(f\"  - TF-IDF í‚¤ì›Œë“œ ìˆ˜: {config_data['prompt']['tfidf_top_k']}ê°œ\")\n",
    "\n",
    "print(\"\\nğŸ” 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš© ì „ëµ:\")\n",
    "print(f\"  - 6-1. í™”ì ìˆ˜ ëª…ì‹œ: {config_data['regex_pattern']['use_speaker_count']}\")\n",
    "print(f\"  - 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: {config_data['regex_pattern']['use_pii_instruction']}\")\n",
    "\n",
    "print(\"\\nğŸ—£ï¸ 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬:\")\n",
    "print(f\"  - ìŠ¬ë­ ì •ê·œí™”: {config_data['preprocess']['normalize_slang']}\")\n",
    "\n",
    "print(\"\\nğŸ“ 5-2. ë™ì  ì¶”ë¡ :\")\n",
    "print(f\"  - ê¸¸ì´ ê¸°ë°˜: {config_data['dynamic_inference']['use_length_based']}\")\n",
    "print(f\"  - ì£¼ì œ ê¸°ë°˜: {config_data['dynamic_inference']['use_topic_based']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ (6-1, 6-2 í†µí•©)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"ğŸ“ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ (6-1, 6-2 ì ìš©):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "sample_dialogue = train_df_for_tokens['dialogue'].iloc[0]\n",
    "print(f\"[1. ì›ë³¸ ëŒ€í™” (ì• 150ì)]:\\n{sample_dialogue[:150]}...\")\n",
    "\n",
    "# ëŒ€í™” êµ¬ì¡° ë¶„ì„\n",
    "sample_analysis = analyze_dialogue_structure(sample_dialogue)\n",
    "print(f\"\\n[2. ëŒ€í™” êµ¬ì¡° ë¶„ì„]:\")\n",
    "print(f\"  - í™”ì ìˆ˜: {sample_analysis['num_speakers']}ëª…\")\n",
    "print(f\"  - PII ë§ˆìŠ¤í‚¹ í† í°: {sample_analysis['pii_mask_examples']}\")\n",
    "\n",
    "sample_preprocessed = preprocess_dialogue(sample_dialogue, normalize_slang_flag=True)\n",
    "print(f\"\\n[3. ìŠ¬ë­ ì •ê·œí™” í›„]:\\n{sample_preprocessed[:150]}...\")\n",
    "\n",
    "sample_keywords = tfidf_extractor.get_keyword_string(sample_preprocessed, top_k=5)\n",
    "print(f\"\\n[4. TF-IDF í‚¤ì›Œë“œ]: {sample_keywords}\")\n",
    "\n",
    "# 6-1, 6-2 í†µí•© í”„ë¡¬í”„íŠ¸ (balanced ìŠ¤íƒ€ì¼)\n",
    "sample_prompted = build_summary_prompt(\n",
    "    sample_preprocessed, \n",
    "    prompt_style=\"balanced\",\n",
    "    use_speaker_count=True,\n",
    "    use_pii_instruction=True\n",
    ")\n",
    "print(f\"\\n[5. 'balanced' í”„ë¡¬í”„íŠ¸ ì ìš© (6-1, 6-2)]:\\n{sample_prompted[:400]}...\")\n",
    "\n",
    "# keyword_aware ìŠ¤íƒ€ì¼ ì˜ˆì‹œ\n",
    "sample_keyword_aware = build_summary_prompt(\n",
    "    sample_preprocessed,\n",
    "    prompt_style=\"keyword_aware\",\n",
    "    tfidf_extractor=tfidf_extractor,\n",
    "    top_k=5,\n",
    "    use_speaker_count=True,\n",
    "    use_pii_instruction=True\n",
    ")\n",
    "print(f\"\\n[6. 'keyword_aware' í”„ë¡¬í”„íŠ¸ ì ìš© (TF-IDF + 6-1, 6-2)]:\\n{sample_keyword_aware[:400]}...\")\n",
    "\n",
    "# ë™ì  ì¶”ë¡  ì„¤ì • ì˜ˆì‹œ\n",
    "dynamic_config = get_dynamic_inference_config(sample_dialogue, use_topic=True, use_length=True)\n",
    "print(f\"\\n[7. ë™ì  ì¶”ë¡  ì„¤ì •]:\")\n",
    "print(f\"  - ê¸¸ì´ ì¹´í…Œê³ ë¦¬: {get_length_category(sample_dialogue)}\")\n",
    "print(f\"  - ê°ì§€ëœ ì£¼ì œ: {detect_topic(sample_dialogue)}\")\n",
    "print(f\"  - max_new_tokens: {dynamic_config['max_new_tokens']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜: ë””ì½”ë”© & í›„ì²˜ë¦¬\n",
    "# ============================================================================\n",
    "\n",
    "def decode_with_speaker_tags(tokenizer, output_ids, remove_tokens=None):\n",
    "    \"\"\"\n",
    "    í™”ì íƒœê·¸(#Person1# ë“±)ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ë””ì½”ë”©\n",
    "    skip_special_tokens=Falseë¡œ ë””ì½”ë”© í›„, ë¶ˆí•„ìš”í•œ í† í°ë§Œ ì œê±°\n",
    "    \"\"\"\n",
    "    if remove_tokens is None:\n",
    "        remove_tokens = ['<usr>', tokenizer.bos_token, tokenizer.eos_token, tokenizer.pad_token]\n",
    "    \n",
    "    # skip_special_tokens=Falseë¡œ ë””ì½”ë”© (í™”ì íƒœê·¸ ë³´ì¡´)\n",
    "    text = tokenizer.decode(output_ids, skip_special_tokens=False)\n",
    "    \n",
    "    # ë¶ˆí•„ìš”í•œ í† í°ë§Œ ì œê±°\n",
    "    for token in remove_tokens:\n",
    "        if token:\n",
    "            text = text.replace(token, '')\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def postprocess_summary(summary):\n",
    "    \"\"\"\n",
    "    ìš”ì•½ë¬¸ í›„ì²˜ë¦¬:\n",
    "    1. ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°\n",
    "    2. í™”ì íƒœê·¸ ì•ë’¤ ê³µë°± ì •ë¦¬\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # ì—°ì† ê³µë°± ì œê±°\n",
    "    summary = ' '.join(summary.split())\n",
    "    \n",
    "    # í™”ì íƒœê·¸ ì•ë’¤ ê³µë°± ì •ë¦¬ (#Person1# ì€ â†’ #Person1#ì€)\n",
    "    summary = re.sub(r'(#Person\\d+#)\\s+', r'\\1 ', summary)\n",
    "    \n",
    "    return summary.strip()\n",
    "\n",
    "print(\"\\nâœ… decode_with_speaker_tags, postprocess_summary í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cm7ob25lHBkR"
   },
   "source": [
    "- ì°¸ê³ âœ…    \n",
    ": wandb ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„  entity, project, nameë¥¼ ì§€ì •í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤. wandb í™ˆí˜ì´ì§€ì— ê°€ì…í•œ í›„ ì–»ì€ ì •ë³´ë¥¼ ì…ë ¥í•˜ì—¬ ì‘ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "REJybO5UCabF"
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ì˜ êµ¬ì„± ì •ë³´ë¥¼ YAML íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "config_path = \"./config.yaml\"\n",
    "with open(config_path, \"w\") as file:\n",
    "    yaml.dump(config_data, file, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObEASD6Wj6pl"
   },
   "source": [
    "### 3) Configuration ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JUBm_6RqlYpV",
    "outputId": "4b1c8c44-c6a9-40f1-adbd-72d48f0c983b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dynamic_inference': {'use_length_based': True, 'use_topic_based': True},\n",
      " 'general': {'data_path': './data/',\n",
      "             'model_name': 'digit82/kobart-summarization',\n",
      "             'output_dir': './prediction_kobart_v1'},\n",
      " 'inference': {'batch_size': 32,\n",
      "               'ckt_path': 'model ckt path',\n",
      "               'early_stopping': True,\n",
      "               'generate_max_length': 100,\n",
      "               'no_repeat_ngram_size': 2,\n",
      "               'num_beams': 4,\n",
      "               'remove_tokens': ['<usr>', '<s>', '</s>', '<pad>'],\n",
      "               'result_path': './prediction/'},\n",
      " 'preprocess': {'normalize_slang': True},\n",
      " 'prompt': {'prompt_style': 'balanced',\n",
      "            'tfidf_top_k': 5,\n",
      "            'turn_separator': '[SEP]',\n",
      "            'use_prompt': True,\n",
      "            'use_turn_separator': False},\n",
      " 'regex_pattern': {'show_pii_examples': True,\n",
      "                   'use_pii_instruction': True,\n",
      "                   'use_speaker_count': True},\n",
      " 'tokenizer': {'bos_token': '<s>',\n",
      "               'decoder_max_len': 100,\n",
      "               'encoder_max_len': 1024,\n",
      "               'eos_token': '</s>',\n",
      "               'special_tokens': ['#Address#',\n",
      "                                  '#Alex#',\n",
      "                                  '#Bob#',\n",
      "                                  '#CarNumber#',\n",
      "                                  '#CardNumber#',\n",
      "                                  '#DateOfBirth#',\n",
      "                                  '#Email#',\n",
      "                                  '#Kristin#',\n",
      "                                  '#Liliana#',\n",
      "                                  '#Name#',\n",
      "                                  '#PassportNumber#',\n",
      "                                  '#Person1#',\n",
      "                                  '#Person2#',\n",
      "                                  '#Person3#',\n",
      "                                  '#Person4#',\n",
      "                                  '#Person5#',\n",
      "                                  '#Person6#',\n",
      "                                  '#Person7#',\n",
      "                                  '#PersonName#',\n",
      "                                  '#PhoneNumber#',\n",
      "                                  '#Price#',\n",
      "                                  '#SSN#']},\n",
      " 'training': {'do_eval': True,\n",
      "              'do_train': True,\n",
      "              'early_stopping_patience': 3,\n",
      "              'early_stopping_threshold': 0.001,\n",
      "              'eval_strategy': 'epoch',\n",
      "              'fp16': True,\n",
      "              'generation_max_length': 100,\n",
      "              'gradient_accumulation_steps': 1,\n",
      "              'learning_rate': 2e-05,\n",
      "              'load_best_model_at_end': True,\n",
      "              'logging_dir': './logs',\n",
      "              'logging_strategy': 'epoch',\n",
      "              'lr_scheduler_type': 'cosine',\n",
      "              'num_train_epochs': 20,\n",
      "              'optim': 'adamw_torch',\n",
      "              'overwrite_output_dir': True,\n",
      "              'per_device_eval_batch_size': 32,\n",
      "              'per_device_train_batch_size': 16,\n",
      "              'predict_with_generate': True,\n",
      "              'report_to': 'wandb',\n",
      "              'save_strategy': 'epoch',\n",
      "              'save_total_limit': 5,\n",
      "              'seed': 42,\n",
      "              'warmup_ratio': 0.1,\n",
      "              'weight_decay': 0.01},\n",
      " 'wandb': {'entity': 'fc_bootcamp',\n",
      "           'name': 'baseline_v0',\n",
      "           'project': 'kobart_v1'}}\n"
     ]
    }
   ],
   "source": [
    "# ì €ì¥ëœ config íŒŒì¼ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "config_path = \"./config.yaml\"\n",
    "\n",
    "with open(config_path, \"r\") as file:\n",
    "    loaded_config = yaml.safe_load(file)\n",
    "\n",
    "# ë¶ˆëŸ¬ì˜¨ config íŒŒì¼ì˜ ì „ì²´ ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "pprint(loaded_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xRSbKEVslhwO",
    "outputId": "40ba5c67-574e-4f86-cbac-13e9f01c588a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_path': './data/',\n",
       " 'model_name': 'digit82/kobart-summarization',\n",
       " 'output_dir': './prediction_kobart_v1'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì‹¤í—˜ì— ì“°ì¼ ë°ì´í„°ì˜ ê²½ë¡œ, ì‚¬ìš©ë  ëª¨ë¸, ëª¨ë¸ì˜ ìµœì¢… ì¶œë ¥ ê²°ê³¼ë¥¼ ì €ì¥í•  ê²½ë¡œì— ëŒ€í•´ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "loaded_config['general']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ê³³ì— ì‚¬ìš©ìê°€ ì €ì¥í•œ ë°ì´í„° dir ì„¤ì •í•˜ê¸°\n",
    "# loaded_config['general']['data_path'] = \"data_path\"\n",
    "\n",
    "# Ensure compatibility with Transformers: they expect the key to be 'evaluation_strategy'.\n",
    "# Some configs may use 'eval_strategy' (short form). Normalize to 'evaluation_strategy'.\n",
    "if 'eval_strategy' in loaded_config['training'] and 'evaluation_strategy' not in loaded_config['training']:\n",
    "    loaded_config['training']['evaluation_strategy'] = loaded_config['training'].pop('eval_strategy')\n",
    "\n",
    "# (legacy) if someone used 'evaluation_strategy' already, keep it as-is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1pvFmIOqljv1",
    "outputId": "958c9b06-90de-4872-b2fb-cf739a655b4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'decoder_max_len': 100,\n",
       " 'encoder_max_len': 1024,\n",
       " 'eos_token': '</s>',\n",
       " 'special_tokens': ['#Address#',\n",
       "  '#Alex#',\n",
       "  '#Bob#',\n",
       "  '#CarNumber#',\n",
       "  '#CardNumber#',\n",
       "  '#DateOfBirth#',\n",
       "  '#Email#',\n",
       "  '#Kristin#',\n",
       "  '#Liliana#',\n",
       "  '#Name#',\n",
       "  '#PassportNumber#',\n",
       "  '#Person1#',\n",
       "  '#Person2#',\n",
       "  '#Person3#',\n",
       "  '#Person4#',\n",
       "  '#Person5#',\n",
       "  '#Person6#',\n",
       "  '#Person7#',\n",
       "  '#PersonName#',\n",
       "  '#PhoneNumber#',\n",
       "  '#Price#',\n",
       "  '#SSN#']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ í•˜ê¸° ìœ„í•´ tokenization ê³¼ì •ì—ì„œ í•„ìš”í•œ ì •ë³´ë“¤ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "loaded_config['tokenizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEvwCIBVll-h",
    "outputId": "ca010ac3-05be-4983-d665-2a653f0ced0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'do_eval': True,\n",
       " 'do_train': True,\n",
       " 'early_stopping_patience': 3,\n",
       " 'early_stopping_threshold': 0.001,\n",
       " 'fp16': True,\n",
       " 'generation_max_length': 100,\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'learning_rate': 2e-05,\n",
       " 'load_best_model_at_end': True,\n",
       " 'logging_dir': './logs',\n",
       " 'logging_strategy': 'epoch',\n",
       " 'lr_scheduler_type': 'cosine',\n",
       " 'num_train_epochs': 20,\n",
       " 'optim': 'adamw_torch',\n",
       " 'overwrite_output_dir': True,\n",
       " 'per_device_eval_batch_size': 32,\n",
       " 'per_device_train_batch_size': 16,\n",
       " 'predict_with_generate': True,\n",
       " 'report_to': 'wandb',\n",
       " 'save_strategy': 'epoch',\n",
       " 'save_total_limit': 5,\n",
       " 'seed': 42,\n",
       " 'warmup_ratio': 0.1,\n",
       " 'weight_decay': 0.01,\n",
       " 'evaluation_strategy': 'epoch'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ëª¨ë¸ì´ í›ˆë ¨ ì‹œ ì ìš©ë  ë§¤ê°œë³€ìˆ˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "loaded_config['training']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xhqHf1njlnyg",
    "outputId": "be9519c6-118b-4e4f-ea11-4bcca0ac42bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity': 'fc_bootcamp', 'name': 'baseline_v0', 'project': 'kobart_v1'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ëª¨ë¸ í•™ìŠµ ê³¼ì •ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ëŠ” wandb ì„¤ì • ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "loaded_config['wandb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ì„ íƒ) ì´ê³³ì— ì‚¬ìš©ìê°€ ì‚¬ìš©í•  wandb config ì„¤ì •\n",
    "loaded_config['wandb']['entity'] = \"fc_bootcamp\"\n",
    "loaded_config['wandb']['project'] = \"kobart_v1\"\n",
    "loaded_config['wandb']['name'] = \"baseline_v0\"\n",
    "\n",
    "# wandb ë¹„í™œì„±í™” (ê¶Œí•œ ë¬¸ì œ í•´ê²°)\n",
    "loaded_config['training']['report_to'] = \"wandb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Wandb setup helper (ì¶”ê°€) ---\n",
    "# ì‚¬ìš©ë²• ì˜ˆì‹œ:\n",
    "# cfg = setup_wandb(api_key=\"<YOUR_API_KEY>\", entity=\"your-entity\", project=\"your-project\", run_name=\"exp-01\")\n",
    "# import wandb\n",
    "# wandb.init(**cfg)\n",
    "\n",
    "import os\n",
    "\n",
    "def setup_wandb(api_key: str = None, entity: str = None, project: str = None, run_name: str = None, mode: str = 'online'):\n",
    "    \"\"\"\n",
    "    Helper to set WANDB API key and login programmatically.\n",
    "\n",
    "    - api_key: your W&B API key (string). If provided, sets `WANDB_API_KEY` env var.\n",
    "    - entity: W&B team or user name (maps to `entity` in `wandb.init`).\n",
    "    - project: W&B project name (maps to `project` in `wandb.init`).\n",
    "    - run_name: a human-friendly run name (maps to `name` in `wandb.init`).\n",
    "    - mode: 'online' or 'offline' (useful when running without internet or CI).\n",
    "\n",
    "    Returns a dict that can be passed to `wandb.init(**cfg)` or used to populate `loaded_config['wandb']`.\n",
    "    \"\"\"\n",
    "    if api_key:\n",
    "        os.environ['WANDB_API_KEY'] = '624410f236117b79b2ee07a4ccef31c7514a5e03'\n",
    "\n",
    "    try:\n",
    "        import wandb\n",
    "    except Exception as e:\n",
    "        raise ImportError(\"wandb is not installed. Install with `pip install wandb` and try again.\") from e\n",
    "\n",
    "    env_key = os.environ.get('WANDB_API_KEY')\n",
    "    if env_key is None:\n",
    "        print(\"Warning: No WANDB_API_KEY found in environment. Call setup_wandb(api_key=...) or set WANDB_API_KEY manually.\")\n",
    "\n",
    "    # Perform login (this will use the env var if present)\n",
    "    try:\n",
    "        wandb.login(key=env_key)\n",
    "    except Exception as e:\n",
    "        print(\"wandb.login() failed:\", e)\n",
    "\n",
    "    cfg = {\n",
    "        'mode': mode\n",
    "    }\n",
    "    if entity:\n",
    "        cfg['entity'] = fc_bootcamp\n",
    "    if project:\n",
    "        cfg['project'] = kobart_v1\n",
    "    if run_name:\n",
    "        cfg['name'] = baseline_v0\n",
    "\n",
    "    print(\"W&B helper ready. Use: import wandb; wandb.init(**cfg)\")\n",
    "    return cfg\n",
    "\n",
    "# ì˜ˆì‹œ (ì£¼ì„ í•´ì œ í›„ ì‚¬ìš©):\n",
    "# cfg = setup_wandb(api_key=\"<YOUR_API_KEY>\", entity=\"your-team-or-username\", project=\"my-project\", run_name=\"run-1\")\n",
    "# import wandb\n",
    "# wandb.init(**cfg)\n",
    "# ----------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fm4gxPRVlppj",
    "outputId": "1342aa36-3934-4f73-c912-e7d35fe6df06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'ckt_path': 'model ckt path',\n",
       " 'early_stopping': True,\n",
       " 'generate_max_length': 100,\n",
       " 'no_repeat_ngram_size': 2,\n",
       " 'num_beams': 4,\n",
       " 'remove_tokens': ['<usr>', '<s>', '</s>', '<pad>'],\n",
       " 'result_path': './prediction/'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ëª¨ë¸ì´ ìµœì¢… ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ê¸° ìœ„í•œ ë§¤ê°œë³€ìˆ˜ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "loaded_config['inference']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2zt0b-8ogCL"
   },
   "source": [
    "### 4) ë°ì´í„° ë¶ˆëŸ¬ì™€ì„œ í™•ì¸í•´ë³´ê¸°\n",
    "- ì‹¤í—˜ì—ì„œ ì“°ì¼ ë°ì´í„°ë¥¼ loadí•˜ì—¬ ë°ì´í„°ì˜ êµ¬ì¡°ì™€ ë‚´ìš©ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "- Train, dev, test ìˆœì„œëŒ€ë¡œ 12457, 499, 250ê°œ ì”© ë°ì´í„°ê°€ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "QFHIE2G04y-K",
    "outputId": "19312d21-f5bf-495f-c626-cc17b82024a4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12452</th>\n",
       "      <td>train_12455</td>\n",
       "      <td>#Person1#: ì•ˆë…•í•˜ì„¸ìš”. í˜¹ì‹œ ë§¨ì²´ìŠ¤í„°ì—ì„œ ì˜¤ì‹  Mr. Green ë§ìœ¼ì‹ ê°€ìš”...</td>\n",
       "      <td>Tan Lingì€ í°ë¨¸ë¦¬ì™€ ìˆ˜ì—¼ì´ íŠ¹ì§•ì¸ Mr. Greenì„ ë§ì´í•˜ì—¬ í˜¸í…”ë¡œ ì•ˆë‚´í•©...</td>\n",
       "      <td>í˜¸í…” ì•ˆë‚´</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12453</th>\n",
       "      <td>train_12456</td>\n",
       "      <td>#Person1#: Mister Ewingì´ ìš°ë¦¬ íšŒì˜ì¥ì— 4ì‹œì— ì˜¤ë¼ê³  í–ˆì§€, ë§...</td>\n",
       "      <td>#Person1#ê³¼ #Person2#ëŠ” Mister Ewingì˜ ìš”ì²­ì— ë”°ë¼ íšŒì˜ì¥...</td>\n",
       "      <td>íšŒì˜ ì¤€ë¹„</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12454</th>\n",
       "      <td>train_12457</td>\n",
       "      <td>#Person1#: ì˜¤ëŠ˜ ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\\n#Person2#: ì°¨ë¥¼ ë¹Œë¦¬ê³  ì‹¶...</td>\n",
       "      <td>#Person2#ëŠ” #Person1#ì˜ ë„ì›€ìœ¼ë¡œ 5ì¼ ë™ì•ˆ ì†Œí˜•ì°¨ë¥¼ ëŒ€ì—¬í•©ë‹ˆë‹¤.</td>\n",
       "      <td>ì°¨ëŸ‰ ëŒ€ì—¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12455</th>\n",
       "      <td>train_12458</td>\n",
       "      <td>#Person1#: ë„ˆ ì˜¤ëŠ˜ ì¢€ ê¸°ë¶„ ì•ˆ ì¢‹ì•„ ë³´ì¸ë‹¤? ë¬´ìŠ¨ ì¼ ìˆì–´?\\n#Pers...</td>\n",
       "      <td>#Person2#ì˜ ì–´ë¨¸ë‹ˆê°€ ì§ì¥ì„ ìƒìœ¼ì…¨ë‹¤. #Person2#ëŠ” ì–´ë¨¸ë‹ˆê°€ ìš°ìš¸í•´í•˜...</td>\n",
       "      <td>ì‹¤ì§ê³¼ ëŒ€ì²˜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12456</th>\n",
       "      <td>train_12459</td>\n",
       "      <td>#Person1#: ì—„ë§ˆ, ë‚˜ ë‹¤ìŒ ì£¼ í† ìš”ì¼ì— ì´ëª¨ë¶€ë„¤ ê°€ì¡± ë³´ëŸ¬ ê°€ëŠ”ë°, ì˜¤ëŠ˜ ...</td>\n",
       "      <td>#Person1#ì€ ë‹¤ìŒ ì£¼ í† ìš”ì¼ì— ì´ëª¨ë¶€ë„¤ ê°€ì¡±ì„ ë°©ë¬¸í•˜ê¸° ìœ„í•´ ì§ì„ ì‹¸ì•¼ í•˜ëŠ”...</td>\n",
       "      <td>ê°€ì¡± ë°©ë¬¸ ì¤€ë¹„</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             fname                                           dialogue  \\\n",
       "12452  train_12455  #Person1#: ì•ˆë…•í•˜ì„¸ìš”. í˜¹ì‹œ ë§¨ì²´ìŠ¤í„°ì—ì„œ ì˜¤ì‹  Mr. Green ë§ìœ¼ì‹ ê°€ìš”...   \n",
       "12453  train_12456  #Person1#: Mister Ewingì´ ìš°ë¦¬ íšŒì˜ì¥ì— 4ì‹œì— ì˜¤ë¼ê³  í–ˆì§€, ë§...   \n",
       "12454  train_12457  #Person1#: ì˜¤ëŠ˜ ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\\n#Person2#: ì°¨ë¥¼ ë¹Œë¦¬ê³  ì‹¶...   \n",
       "12455  train_12458  #Person1#: ë„ˆ ì˜¤ëŠ˜ ì¢€ ê¸°ë¶„ ì•ˆ ì¢‹ì•„ ë³´ì¸ë‹¤? ë¬´ìŠ¨ ì¼ ìˆì–´?\\n#Pers...   \n",
       "12456  train_12459  #Person1#: ì—„ë§ˆ, ë‚˜ ë‹¤ìŒ ì£¼ í† ìš”ì¼ì— ì´ëª¨ë¶€ë„¤ ê°€ì¡± ë³´ëŸ¬ ê°€ëŠ”ë°, ì˜¤ëŠ˜ ...   \n",
       "\n",
       "                                                 summary     topic  \n",
       "12452  Tan Lingì€ í°ë¨¸ë¦¬ì™€ ìˆ˜ì—¼ì´ íŠ¹ì§•ì¸ Mr. Greenì„ ë§ì´í•˜ì—¬ í˜¸í…”ë¡œ ì•ˆë‚´í•©...     í˜¸í…” ì•ˆë‚´  \n",
       "12453  #Person1#ê³¼ #Person2#ëŠ” Mister Ewingì˜ ìš”ì²­ì— ë”°ë¼ íšŒì˜ì¥...     íšŒì˜ ì¤€ë¹„  \n",
       "12454       #Person2#ëŠ” #Person1#ì˜ ë„ì›€ìœ¼ë¡œ 5ì¼ ë™ì•ˆ ì†Œí˜•ì°¨ë¥¼ ëŒ€ì—¬í•©ë‹ˆë‹¤.     ì°¨ëŸ‰ ëŒ€ì—¬  \n",
       "12455  #Person2#ì˜ ì–´ë¨¸ë‹ˆê°€ ì§ì¥ì„ ìƒìœ¼ì…¨ë‹¤. #Person2#ëŠ” ì–´ë¨¸ë‹ˆê°€ ìš°ìš¸í•´í•˜...    ì‹¤ì§ê³¼ ëŒ€ì²˜  \n",
       "12456  #Person1#ì€ ë‹¤ìŒ ì£¼ í† ìš”ì¼ì— ì´ëª¨ë¶€ë„¤ ê°€ì¡±ì„ ë°©ë¬¸í•˜ê¸° ìœ„í•´ ì§ì„ ì‹¸ì•¼ í•˜ëŠ”...  ê°€ì¡± ë°©ë¬¸ ì¤€ë¹„  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configì— ì €ì¥ëœ ë°ì´í„° ê²½ë¡œë¥¼ í†µí•´ trainê³¼ validation dataë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "data_path = loaded_config['general']['data_path']\n",
    "\n",
    "# train dataì˜ êµ¬ì¡°ì™€ ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "train_df = pd.read_csv(os.path.join(data_path,'train.csv'))\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "FAGaYvNZ09Sq",
    "outputId": "bf8bf286-19e7-469d-ffae-41e6ad795ae6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>dev_495</td>\n",
       "      <td>#Person1#: ìƒˆí•´ê°€ ë˜ë‹ˆê¹Œ ë‚˜ë„ ìƒˆ ì¶œë°œì„ í•˜ê¸°ë¡œ í–ˆì–´.\\n#Person2#...</td>\n",
       "      <td>#Person1#ì€ ìƒˆí•´ì— ë‹´ë°°ë¥¼ ëŠê³  ì»¤ë°ì•„ì›ƒ í•˜ê¸°ë¡œ ê²°ì‹¬í–ˆìŠµë‹ˆë‹¤. #Person...</td>\n",
       "      <td>ìƒˆí•´ ê²°ì‹¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>dev_496</td>\n",
       "      <td>#Person1#: ë„ˆ Joeë‘ ê²°í˜¼í–ˆì§€?\\n#Person2#: Joe? ë¬´ìŠ¨ ë§ì´...</td>\n",
       "      <td>#Person1#ì€ #Person2#ê°€ Joeì™€ ê²°í˜¼í–ˆë‹¤ê³  ìƒê°í•˜ì§€ë§Œ, #Perso...</td>\n",
       "      <td>ì‚¬ë‘ê³¼ ê²°í˜¼ ì˜¤í•´</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>dev_497</td>\n",
       "      <td>#Person1#: ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”, ì•„ì¤Œë§ˆ?\\n#Person2#: ì œ ì°¨ì—ì„œ ...</td>\n",
       "      <td>#Person2#ì˜ ì°¨ì—ì„œ ì†Œë¦¬ê°€ ë‚˜ë©°, ë¸Œë ˆì´í¬ ìˆ˜ë¦¬ê°€ í•„ìš”í•œ ìƒí™©ì…ë‹ˆë‹¤. #Pe...</td>\n",
       "      <td>ì°¨ëŸ‰ ì†ŒìŒ ë° ìˆ˜ë¦¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>dev_498</td>\n",
       "      <td>#Person1#: ì—¬ë³´ì„¸ìš”, ì•„ë§ˆì¡´ ê³ ê° ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\\n#...</td>\n",
       "      <td>#Person2#ê°€ ì•„ë§ˆì¡´ ê³ ê° ì„œë¹„ìŠ¤ì— ì „í™”í•˜ì—¬ ì•„ë§ˆì¡´ì—ì„œ êµ¬ë§¤í•œ ì±…ì— 53í˜ì´ì§€...</td>\n",
       "      <td>ì±… í˜ì´ì§€ ëˆ„ë½</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>dev_499</td>\n",
       "      <td>#Person1#: ë²Œì¨ ì—¬ë¦„ì´ ë‹¤ê°€ì˜¤ë‹¤ë‹ˆ ë¯¿ê¸°ì§€ ì•Šì•„. \\n#Person2#: ë§...</td>\n",
       "      <td>#Person2#ëŠ” ì—¬ë¦„ë°©í•™ ë™ì•ˆ íŒŒí‹°ì—ì„œ ì¼í•˜ëŠ” íšŒì‚¬ì—ì„œ ì¼í•˜ë©°, ì£¼ë¡œ ìŒì‹ ì¤€ë¹„...</td>\n",
       "      <td>ì—¬ë¦„ë°©í•™ ì¼ìë¦¬</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fname                                           dialogue  \\\n",
       "494  dev_495  #Person1#: ìƒˆí•´ê°€ ë˜ë‹ˆê¹Œ ë‚˜ë„ ìƒˆ ì¶œë°œì„ í•˜ê¸°ë¡œ í–ˆì–´.\\n#Person2#...   \n",
       "495  dev_496  #Person1#: ë„ˆ Joeë‘ ê²°í˜¼í–ˆì§€?\\n#Person2#: Joe? ë¬´ìŠ¨ ë§ì´...   \n",
       "496  dev_497  #Person1#: ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”, ì•„ì¤Œë§ˆ?\\n#Person2#: ì œ ì°¨ì—ì„œ ...   \n",
       "497  dev_498  #Person1#: ì—¬ë³´ì„¸ìš”, ì•„ë§ˆì¡´ ê³ ê° ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\\n#...   \n",
       "498  dev_499  #Person1#: ë²Œì¨ ì—¬ë¦„ì´ ë‹¤ê°€ì˜¤ë‹¤ë‹ˆ ë¯¿ê¸°ì§€ ì•Šì•„. \\n#Person2#: ë§...   \n",
       "\n",
       "                                               summary       topic  \n",
       "494  #Person1#ì€ ìƒˆí•´ì— ë‹´ë°°ë¥¼ ëŠê³  ì»¤ë°ì•„ì›ƒ í•˜ê¸°ë¡œ ê²°ì‹¬í–ˆìŠµë‹ˆë‹¤. #Person...       ìƒˆí•´ ê²°ì‹¬  \n",
       "495  #Person1#ì€ #Person2#ê°€ Joeì™€ ê²°í˜¼í–ˆë‹¤ê³  ìƒê°í•˜ì§€ë§Œ, #Perso...   ì‚¬ë‘ê³¼ ê²°í˜¼ ì˜¤í•´  \n",
       "496  #Person2#ì˜ ì°¨ì—ì„œ ì†Œë¦¬ê°€ ë‚˜ë©°, ë¸Œë ˆì´í¬ ìˆ˜ë¦¬ê°€ í•„ìš”í•œ ìƒí™©ì…ë‹ˆë‹¤. #Pe...  ì°¨ëŸ‰ ì†ŒìŒ ë° ìˆ˜ë¦¬  \n",
       "497  #Person2#ê°€ ì•„ë§ˆì¡´ ê³ ê° ì„œë¹„ìŠ¤ì— ì „í™”í•˜ì—¬ ì•„ë§ˆì¡´ì—ì„œ êµ¬ë§¤í•œ ì±…ì— 53í˜ì´ì§€...    ì±… í˜ì´ì§€ ëˆ„ë½  \n",
       "498  #Person2#ëŠ” ì—¬ë¦„ë°©í•™ ë™ì•ˆ íŒŒí‹°ì—ì„œ ì¼í•˜ëŠ” íšŒì‚¬ì—ì„œ ì¼í•˜ë©°, ì£¼ë¡œ ìŒì‹ ì¤€ë¹„...    ì—¬ë¦„ë°©í•™ ì¼ìë¦¬  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation dataì˜ êµ¬ì¡°ì™€ ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "val_df = pd.read_csv(os.path.join(data_path,'dev.csv'))\n",
    "val_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IIaIrpH4kWo"
   },
   "source": [
    "## 1. ë°ì´í„° ê°€ê³µ ë° ë°ì´í„°ì…‹ í´ë˜ìŠ¤ êµ¬ì¶•\n",
    "- csv file ì„ ë¶ˆëŸ¬ì™€ì„œ encoder ì™€ decoderì˜ ì…ë ¥í˜•íƒœë¡œ ê°€ê³µí•´ì¤ë‹ˆë‹¤.\n",
    "- ê°€ê³µëœ ë°ì´í„°ë¥¼ torch dataset class ë¡œ êµ¬ì¶•í•˜ì—¬ ëª¨ë¸ì— ì…ë ¥ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oWPawUUflwHa"
   },
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤ë¡œ, ë°ì´í„°ì…‹ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì…ë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "class Preprocess:\n",
    "    def __init__(self,\n",
    "            bos_token: str,\n",
    "            eos_token: str,\n",
    "            prompt_config: dict = None,           # 3-2. í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "            preprocess_config: dict = None,       # 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •\n",
    "            tfidf_extractor = None,               # 5-1. TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œê¸°\n",
    "            regex_pattern_config: dict = None,    # 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš© ì„¤ì • (NEW)\n",
    "        ) -> None:\n",
    "\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ ì„¤ì • ì €ì¥\n",
    "        self.prompt_config = prompt_config or {\n",
    "            \"use_prompt\": False,\n",
    "            \"prompt_style\": \"none\",\n",
    "            \"use_turn_separator\": False,\n",
    "            \"turn_separator\": \"[SEP]\",\n",
    "            \"tfidf_top_k\": 5,\n",
    "        }\n",
    "        \n",
    "        # 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì • ì €ì¥\n",
    "        self.preprocess_config = preprocess_config or {\n",
    "            \"normalize_slang\": False,\n",
    "        }\n",
    "        \n",
    "        # 5-1. TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œê¸°\n",
    "        self.tfidf_extractor = tfidf_extractor\n",
    "        \n",
    "        # 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš© ì„¤ì • (NEW)\n",
    "        self.regex_pattern_config = regex_pattern_config or {\n",
    "            \"use_speaker_count\": True,       # 6-1. í™”ì ìˆ˜ ëª…ì‹œ\n",
    "            \"use_pii_instruction\": True,     # 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ\n",
    "            \"show_pii_examples\": True,       # ë§ˆìŠ¤í‚¹ ì˜ˆì‹œ í‘œì‹œ\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def make_set_as_df(file_path, is_train=True):\n",
    "        \"\"\"ì‹¤í—˜ì— í•„ìš”í•œ ì»¬ëŸ¼ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\"\"\"\n",
    "        if is_train:\n",
    "            df = pd.read_csv(file_path)\n",
    "            train_df = df[['fname', 'dialogue', 'summary']]\n",
    "            return train_df\n",
    "        else:\n",
    "            df = pd.read_csv(file_path)\n",
    "            test_df = df[['fname', 'dialogue']]\n",
    "            return test_df\n",
    "\n",
    "    def apply_slang_normalization(self, text):\n",
    "        \"\"\"4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬: ìŠ¬ë­/ê°íƒ„ì‚¬/ì˜ì„±ì–´ ì •ê·œí™”\"\"\"\n",
    "        if self.preprocess_config.get(\"normalize_slang\", False):\n",
    "            return preprocess_dialogue(text, normalize_slang_flag=True)\n",
    "        return text\n",
    "\n",
    "    def apply_prompt_to_dialogue(self, dialogue_text):\n",
    "        \"\"\"\n",
    "        3-2, 5-1, 6-1, 6-2 í†µí•© í”„ë¡¬í”„íŠ¸ ì ìš©\n",
    "        \n",
    "        - 3-2: í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ ì ìš©\n",
    "        - 5-1: TF-IDF í‚¤ì›Œë“œ í¬í•¨\n",
    "        - 6-1: í™”ì ìˆ˜ ëª…ì‹œ (ê· í˜• ìˆëŠ” ìš”ì•½ ìœ ë„)\n",
    "        - 6-2: PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ (ROUGE ì ìˆ˜ í–¥ìƒ)\n",
    "        \"\"\"\n",
    "        text = dialogue_text\n",
    "        \n",
    "        # ë°œí™” êµ¬ë¶„ì ì ìš© (ì„ íƒì )\n",
    "        if self.prompt_config.get(\"use_turn_separator\", False):\n",
    "            text = add_turn_separators(text, self.prompt_config.get(\"turn_separator\", \"[SEP]\"))\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ ì ìš©\n",
    "        if self.prompt_config.get(\"use_prompt\", False):\n",
    "            prompt_style = self.prompt_config.get(\"prompt_style\", \"default\")\n",
    "            tfidf_top_k = self.prompt_config.get(\"tfidf_top_k\", 5)\n",
    "            \n",
    "            # 6-1, 6-2 ì„¤ì • ê°€ì ¸ì˜¤ê¸°\n",
    "            use_speaker_count = self.regex_pattern_config.get(\"use_speaker_count\", True)\n",
    "            use_pii_instruction = self.regex_pattern_config.get(\"use_pii_instruction\", True)\n",
    "            \n",
    "            # 5-1. TF-IDF í‚¤ì›Œë“œ ìŠ¤íƒ€ì¼ì¸ ê²½ìš° extractor ì „ë‹¬\n",
    "            if prompt_style in [\"keyword\", \"keyword_aware\"] and self.tfidf_extractor is not None:\n",
    "                text = build_summary_prompt(\n",
    "                    text, \n",
    "                    prompt_style=prompt_style, \n",
    "                    tfidf_extractor=self.tfidf_extractor, \n",
    "                    top_k=tfidf_top_k,\n",
    "                    use_speaker_count=use_speaker_count,      # 6-1\n",
    "                    use_pii_instruction=use_pii_instruction    # 6-2\n",
    "                )\n",
    "            else:\n",
    "                # ê¸°íƒ€ ìŠ¤íƒ€ì¼ì—ì„œë„ 6-1, 6-2 ì ìš©\n",
    "                text = build_summary_prompt(\n",
    "                    text, \n",
    "                    prompt_style=prompt_style,\n",
    "                    use_speaker_count=use_speaker_count,      # 6-1\n",
    "                    use_pii_instruction=use_pii_instruction    # 6-2\n",
    "                )\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def full_preprocess_pipeline(self, dialogue_text):\n",
    "        \"\"\"\n",
    "        ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸: ìŠ¬ë­ ì •ê·œí™” â†’ í”„ë¡¬í”„íŠ¸ ì ìš©\n",
    "        \n",
    "        ì²˜ë¦¬ ìˆœì„œ:\n",
    "        1. ìŠ¬ë­/ê°íƒ„ì‚¬/ì˜ì„±ì–´ ì •ê·œí™” (4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬)\n",
    "        2. ë°œí™” êµ¬ë¶„ì ì¶”ê°€ (ì„ íƒì )\n",
    "        3. í”„ë¡¬í”„íŠ¸ ì¶”ê°€ (3-2. í”„ë¡¬í”„íŠ¸ ì„¤ì •)\n",
    "           - TF-IDF í‚¤ì›Œë“œ í¬í•¨ (5-1)\n",
    "           - í™”ì ìˆ˜ ëª…ì‹œ (6-1)\n",
    "           - PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ (6-2)\n",
    "        \"\"\"\n",
    "        text = dialogue_text\n",
    "        \n",
    "        # Step 1: ìŠ¬ë­ ì •ê·œí™”\n",
    "        text = self.apply_slang_normalization(text)\n",
    "        \n",
    "        # Step 2 & 3: í”„ë¡¬í”„íŠ¸ ì ìš© (êµ¬ë¶„ì + TF-IDF + 6-1 + 6-2)\n",
    "        text = self.apply_prompt_to_dialogue(text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def make_input(self, dataset, is_test=False, tokenizer=None, encoder_max_len=None, \n",
    "                   prompt=None, apply_dynamic_prompt=True):\n",
    "        \"\"\"\n",
    "        BART ëª¨ë¸ì˜ ì…ë ¥, ì¶œë ¥ í˜•íƒœë¥¼ ë§ì¶”ê¸° ìœ„í•´ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸:\n",
    "        1. ìŠ¬ë­/ê°íƒ„ì‚¬ ì •ê·œí™” (4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬)\n",
    "        2. TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ (5-1)\n",
    "        3. í”„ë¡¬í”„íŠ¸ ì¶”ê°€ (3-2. í”„ë¡¬í”„íŠ¸ ì„¤ì •)\n",
    "        4. 6-1 í™”ì ìˆ˜ ëª…ì‹œ + 6-2 PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ\n",
    "        5. í† í° ê¸°ë°˜ truncation (ì„ íƒì )\n",
    "        \"\"\"\n",
    "        if is_test:\n",
    "            encoder_input = []\n",
    "            decoder_input = []\n",
    "            for d in dataset['dialogue']:\n",
    "                text = d\n",
    "                \n",
    "                # ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš©\n",
    "                if apply_dynamic_prompt:\n",
    "                    text = self.full_preprocess_pipeline(text)\n",
    "                elif prompt:\n",
    "                    text = self.apply_slang_normalization(text)\n",
    "                    text = apply_summary_prompt(text, prompt)\n",
    "                else:\n",
    "                    text = self.apply_slang_normalization(text)\n",
    "                \n",
    "                # í† í° ê¸°ë°˜ truncation\n",
    "                if tokenizer is not None and encoder_max_len is not None:\n",
    "                    text = smart_truncate_dialogue(text, tokenizer, encoder_max_len)\n",
    "                \n",
    "                encoder_input.append(text)\n",
    "                decoder_input.append(self.bos_token)\n",
    "            return encoder_input, decoder_input\n",
    "        else:\n",
    "            # í•™ìŠµ ë°ì´í„° ì²˜ë¦¬\n",
    "            encoder_input = []\n",
    "            for d in dataset['dialogue']:\n",
    "                text = d\n",
    "                \n",
    "                # ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš©\n",
    "                if apply_dynamic_prompt:\n",
    "                    text = self.full_preprocess_pipeline(text)\n",
    "                else:\n",
    "                    text = self.apply_slang_normalization(text)\n",
    "                \n",
    "                encoder_input.append(text)\n",
    "            \n",
    "            # Gold targetì„ ë””ì½”ë” ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "            decoder_input = dataset['summary'].apply(lambda x: self.bos_token + str(x))\n",
    "            decoder_output = dataset['summary'].apply(lambda x: str(x) + self.eos_token)\n",
    "            return encoder_input, decoder_input.tolist(), decoder_output.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6GDvodoF8sED"
   },
   "outputs": [],
   "source": [
    "# Trainì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "class DatasetForTrain(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, labels, len):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.labels = labels\n",
    "        self.len = len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()} # item[input_ids], item[attention_mask]\n",
    "        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()} # item2[input_ids], item2[attention_mask]\n",
    "        item2['decoder_input_ids'] = item2['input_ids']\n",
    "        item2['decoder_attention_mask'] = item2['attention_mask']\n",
    "        item2.pop('input_ids')\n",
    "        item2.pop('attention_mask')\n",
    "        item.update(item2) #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask]\n",
    "        item['labels'] = self.labels['input_ids'][idx] #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask], item[labels]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# Validationì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "class DatasetForVal(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, labels, len):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.labels = labels\n",
    "        self.len = len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()} # item[input_ids], item[attention_mask]\n",
    "        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()} # item2[input_ids], item2[attention_mask]\n",
    "        item2['decoder_input_ids'] = item2['input_ids']\n",
    "        item2['decoder_attention_mask'] = item2['attention_mask']\n",
    "        item2.pop('input_ids')\n",
    "        item2.pop('attention_mask')\n",
    "        item.update(item2) #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask]\n",
    "        item['labels'] = self.labels['input_ids'][idx] #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask], item[labels]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# Testì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "class DatasetForInference(Dataset):\n",
    "    def __init__(self, encoder_input, test_id, len):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.test_id = test_id\n",
    "        self.len = len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
    "        item['ID'] = self.test_id[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "hT9z4vvS2CCb"
   },
   "outputs": [],
   "source": [
    "# tokenization ê³¼ì •ê¹Œì§€ ì§„í–‰ëœ ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ì— ì…ë ¥ë  ë°ì´í„°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "def prepare_train_dataset(config, preprocessor, data_path, tokenizer):\n",
    "    \"\"\"\n",
    "    3-2 ê°œì„ : í”„ë¡¬í”„íŠ¸ ì„¤ì •ì´ ì ìš©ëœ ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "    \"\"\"\n",
    "    train_file_path = os.path.join(data_path,'train.csv')\n",
    "    val_file_path = os.path.join(data_path,'dev.csv')\n",
    "\n",
    "    # train, validationì— ëŒ€í•´ ê°ê° ë°ì´í„°í”„ë ˆì„ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
    "    train_data = preprocessor.make_set_as_df(train_file_path)\n",
    "    val_data = preprocessor.make_set_as_df(val_file_path)\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f'[ì›ë³¸] train_data dialogue:\\n {train_data[\"dialogue\"][0][:200]}...')\n",
    "    print(f'train_label:\\n {train_data[\"summary\"][0]}')\n",
    "\n",
    "    # 3-2. í”„ë¡¬í”„íŠ¸ ì ìš©ëœ ë°ì´í„°ë¡œ ë³€í™˜\n",
    "    encoder_input_train, decoder_input_train, decoder_output_train = preprocessor.make_input(\n",
    "        train_data, \n",
    "        apply_dynamic_prompt=True  # í”„ë¡¬í”„íŠ¸ ì„¤ì • ì‚¬ìš©\n",
    "    )\n",
    "    encoder_input_val, decoder_input_val, decoder_output_val = preprocessor.make_input(\n",
    "        val_data,\n",
    "        apply_dynamic_prompt=True\n",
    "    )\n",
    "    \n",
    "    # í”„ë¡¬í”„íŠ¸ ì ìš© í™•ì¸\n",
    "    print('-'*150)\n",
    "    print(f'[í”„ë¡¬í”„íŠ¸ ì ìš© í›„] encoder_input_train[0]:\\n {encoder_input_train[0][:300]}...')\n",
    "    print('-'*10, 'Load data complete', '-'*10)\n",
    "\n",
    "    tokenized_encoder_inputs = tokenizer(encoder_input_train, return_tensors=\"pt\", padding=True,\n",
    "                            add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
    "    tokenized_decoder_inputs = tokenizer(decoder_input_train, return_tensors=\"pt\", padding=True,\n",
    "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "    tokenized_decoder_ouputs = tokenizer(decoder_output_train, return_tensors=\"pt\", padding=True,\n",
    "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    train_inputs_dataset = DatasetForTrain(tokenized_encoder_inputs, tokenized_decoder_inputs, tokenized_decoder_ouputs,len(encoder_input_train))\n",
    "\n",
    "    val_tokenized_encoder_inputs = tokenizer(encoder_input_val, return_tensors=\"pt\", padding=True,\n",
    "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
    "    val_tokenized_decoder_inputs = tokenizer(decoder_input_val, return_tensors=\"pt\", padding=True,\n",
    "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "    val_tokenized_decoder_ouputs = tokenizer(decoder_output_val, return_tensors=\"pt\", padding=True,\n",
    "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    val_inputs_dataset = DatasetForVal(val_tokenized_encoder_inputs, val_tokenized_decoder_inputs, val_tokenized_decoder_ouputs,len(encoder_input_val))\n",
    "\n",
    "    print('-'*10, 'Make dataset complete', '-'*10)\n",
    "    \n",
    "    # í† í°í™” í›„ special token í™•ì¸\n",
    "    print(\"\\nğŸ“Š í† í°í™” ê²°ê³¼ í™•ì¸:\")\n",
    "    sample_tokens = tokenized_encoder_inputs['input_ids'][0][:50]\n",
    "    decoded_sample = tokenizer.decode(sample_tokens)\n",
    "    print(f\"   ì²« 50 í† í° ë””ì½”ë”©: {decoded_sample}...\")\n",
    "    \n",
    "    return train_inputs_dataset, val_inputs_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5sKIJ5K5Pz1"
   },
   "source": [
    "## 2. Trainer ë° Trainingargs êµ¬ì¶•í•˜ê¸°\n",
    "- Huggingface ì˜ Trainer ì™€ Training argumentsë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµì„ ì¼ê´„ì ìœ¼ë¡œ ì²˜ë¦¬í•´ì£¼ëŠ” í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "aQk8ILcEeGNz"
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì„±ëŠ¥ì— ëŒ€í•œ í‰ê°€ ì§€í‘œë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ë³¸ ëŒ€íšŒì—ì„œëŠ” ROUGE ì ìˆ˜ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "def compute_metrics(config,tokenizer,pred):\n",
    "    rouge = Rouge()\n",
    "    predictions = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    predictions[predictions == -100] = tokenizer.pad_token_id\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, clean_up_tokenization_spaces=True)\n",
    "    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # ì •í™•í•œ í‰ê°€ë¥¼ ìœ„í•´ ë¯¸ë¦¬ ì •ì˜ëœ ë¶ˆí•„ìš”í•œ ìƒì„±í† í°ë“¤ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "    replaced_predictions = decoded_preds.copy()\n",
    "    replaced_labels = labels.copy()\n",
    "    remove_tokens = config['inference']['remove_tokens']\n",
    "    for token in remove_tokens:\n",
    "        replaced_predictions = [sentence.replace(token,\" \") for sentence in replaced_predictions]\n",
    "        replaced_labels = [sentence.replace(token,\" \") for sentence in replaced_labels]\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[0]}\")\n",
    "    print(f\"GOLD: {replaced_labels[0]}\")\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[1]}\")\n",
    "    print(f\"GOLD: {replaced_labels[1]}\")\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[2]}\")\n",
    "    print(f\"GOLD: {replaced_labels[2]}\")\n",
    "\n",
    "    # ìµœì¢…ì ì¸ ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    results = rouge.get_scores(replaced_predictions, replaced_labels,avg=True)\n",
    "\n",
    "    # ROUGE ì ìˆ˜ ì¤‘ F-1 scoreë¥¼ í†µí•´ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "    result = {key: value[\"f\"] for key, value in results.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "RInkG8g-HjBi"
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì„±ëŠ¥ì— ëŒ€í•œ í‰ê°€ ì§€í‘œë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ë³¸ ëŒ€íšŒì—ì„œëŠ” ROUGE ì ìˆ˜ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "def compute_metrics(config,tokenizer,pred):\n",
    "    rouge = Rouge()\n",
    "    predictions = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    predictions[predictions == -100] = tokenizer.pad_token_id\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, clean_up_tokenization_spaces=True)\n",
    "    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # ì •í™•í•œ í‰ê°€ë¥¼ ìœ„í•´ ë¯¸ë¦¬ ì •ì˜ëœ ë¶ˆí•„ìš”í•œ ìƒì„±í† í°ë“¤ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "    replaced_predictions = decoded_preds.copy()\n",
    "    replaced_labels = labels.copy()\n",
    "    remove_tokens = config['inference']['remove_tokens']\n",
    "    for token in remove_tokens:\n",
    "        replaced_predictions = [sentence.replace(token,\" \") for sentence in replaced_predictions]\n",
    "        replaced_labels = [sentence.replace(token,\" \") for sentence in replaced_labels]\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[0]}\")\n",
    "    print(f\"GOLD: {replaced_labels[0]}\")\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[1]}\")\n",
    "    print(f\"GOLD: {replaced_labels[1]}\")\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[2]}\")\n",
    "    print(f\"GOLD: {replaced_labels[2]}\")\n",
    "\n",
    "    # ìµœì¢…ì ì¸ ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    results = rouge.get_scores(replaced_predictions, replaced_labels,avg=True)\n",
    "\n",
    "    # ROUGE ì ìˆ˜ ì¤‘ F-1 scoreë¥¼ í†µí•´ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "    result = {key: value[\"f\"] for key, value in results.items()}\n",
    "    return result\n",
    "\n",
    "\n",
    "# í•™ìŠµì„ ìœ„í•œ trainer í´ë˜ìŠ¤ì™€ ë§¤ê°œë³€ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "def load_trainer_for_train(config,generate_model,tokenizer,train_inputs_dataset,val_inputs_dataset):\n",
    "    print('-'*10, 'Make training arguments', '-'*10,)\n",
    "    # set training args\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=config['general']['output_dir'], # model output directory\n",
    "                overwrite_output_dir=config['training']['overwrite_output_dir'],\n",
    "                num_train_epochs=config['training']['num_train_epochs'],  # total number of training epochs\n",
    "                learning_rate=config['training']['learning_rate'], # learning_rate\n",
    "                per_device_train_batch_size=config['training']['per_device_train_batch_size'], # batch size per device during training\n",
    "                per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],# batch size for evaluation\n",
    "                warmup_ratio=config['training']['warmup_ratio'],  # number of warmup steps for learning rate scheduler\n",
    "                weight_decay=config['training']['weight_decay'],  # strength of weight decay\n",
    "                lr_scheduler_type=config['training']['lr_scheduler_type'],\n",
    "                optim =config['training']['optim'],\n",
    "                gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
    "                evaluation_strategy=config['training'].get('evaluation_strategy', 'epoch'), # transformers expects this key\n",
    "                save_strategy =config['training']['save_strategy'],\n",
    "                save_total_limit=config['training']['save_total_limit'], # number of total save model.\n",
    "                fp16=config['training']['fp16'],\n",
    "                load_best_model_at_end=config['training']['load_best_model_at_end'], # ìµœì¢…ì ìœ¼ë¡œ ê°€ì¥ ë†’ì€ ì ìˆ˜ ì €ì¥\n",
    "                seed=config['training']['seed'],\n",
    "                logging_dir=config['training']['logging_dir'], # directory for storing logs\n",
    "                logging_strategy=config['training']['logging_strategy'],\n",
    "                predict_with_generate=config['training']['predict_with_generate'], #To use BLEU or ROUGE score\n",
    "                generation_max_length=config['training']['generation_max_length'],\n",
    "                do_train=config['training']['do_train'],\n",
    "                do_eval=config['training']['do_eval'],\n",
    "                report_to=config['training']['report_to'] # (ì„ íƒ) wandbë¥¼ ì‚¬ìš©í•  ë•Œ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "            )\n",
    "\n",
    "    # (ì„ íƒ) wandb ì‚¬ìš© ì‹œì—ë§Œ ì´ˆê¸°í™”\n",
    "    if config['training']['report_to'] == 'wandb':\n",
    "        wandb.init(\n",
    "            entity=config['wandb']['entity'],\n",
    "            project=config['wandb']['project'],\n",
    "            name=config['wandb']['name'],\n",
    "        )\n",
    "        # ëª¨ë¸ checkpointë¥¼ wandbì— ì €ì¥í•˜ë„ë¡ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "        os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "        os.environ[\"WANDB_WATCH\"]=\"false\"\n",
    "\n",
    "    # Validation lossê°€ ë” ì´ìƒ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œ í•™ìŠµì„ ì¤‘ë‹¨ì‹œí‚¤ëŠ” EarlyStopping ê¸°ëŠ¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    MyCallback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=config['training']['early_stopping_patience'],\n",
    "        early_stopping_threshold=config['training']['early_stopping_threshold']\n",
    "    )\n",
    "    print('-'*10, 'Make training arguments complete', '-'*10,)\n",
    "    print('-'*10, 'Make trainer', '-'*10,)\n",
    "\n",
    "    # Trainer í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=generate_model, # ì‚¬ìš©ìê°€ ì‚¬ì „ í•™ìŠµí•˜ê¸° ìœ„í•´ ì‚¬ìš©í•  ëª¨ë¸ì„ ì…ë ¥í•©ë‹ˆë‹¤.\n",
    "        args=training_args,\n",
    "        train_dataset=train_inputs_dataset,\n",
    "        eval_dataset=val_inputs_dataset,\n",
    "        compute_metrics = lambda pred: compute_metrics(config,tokenizer, pred),\n",
    "        callbacks = [MyCallback]\n",
    "    )\n",
    "    print('-'*10, 'Make trainer complete', '-'*10,)\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "KKWHe8dE5fSx"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3-1. í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ (Special Token ë™ì  ë“±ë¡ í¬í•¨)\n",
    "# ============================================================================\n",
    "\n",
    "def load_tokenizer_and_model_for_train(config, device, verbose=True):\n",
    "    \"\"\"\n",
    "    í•™ìŠµì„ ìœ„í•œ tokenizerì™€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "    \n",
    "    3-1 ê°œì„ ì‚¬í•­:\n",
    "    - í•™ìŠµ ë°ì´í„°ì—ì„œ ë™ì ìœ¼ë¡œ ìˆ˜ì§‘í•œ special tokens ë“±ë¡\n",
    "    - model.resize_token_embeddings() í˜¸ì¶œë¡œ ì„ë² ë”© ì‚¬ì´ì¦ˆ ì¡°ì •\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print('-' * 10, 'Load tokenizer & model', '-' * 10)\n",
    "        print('-' * 10, f'Model Name : {config[\"general\"][\"model_name\"]}', '-' * 10)\n",
    "    \n",
    "    model_name = config['general']['model_name']\n",
    "    bart_config = BartConfig().from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    generate_model = BartForConditionalGeneration.from_pretrained(\n",
    "        config['general']['model_name'], \n",
    "        config=bart_config\n",
    "    )\n",
    "\n",
    "    # ============================================================\n",
    "    # 3-1. Special Tokens ë“±ë¡ (ë™ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ í† í° ì‚¬ìš©)\n",
    "    # ============================================================\n",
    "    special_tokens = config['tokenizer']['special_tokens']\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nğŸ“Œ ë“±ë¡í•  Special Tokens ({len(special_tokens)}ê°œ):\")\n",
    "        print(f\"   {special_tokens}\")\n",
    "    \n",
    "    # additional_special_tokensë¡œ ë“±ë¡\n",
    "    special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "    num_added = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nâœ… ìƒˆë¡œ ì¶”ê°€ëœ í† í° ìˆ˜: {num_added}\")\n",
    "        print(f\"   í† í¬ë‚˜ì´ì € vocab size: {len(tokenizer)}\")\n",
    "\n",
    "    # âš ï¸ ì¤‘ìš”: ëª¨ë¸ ì„ë² ë”© ì‚¬ì´ì¦ˆ ì¡°ì •\n",
    "    generate_model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   ëª¨ë¸ ì„ë² ë”© ì‚¬ì´ì¦ˆ ì¡°ì • ì™„ë£Œ: {generate_model.config.vocab_size}\")\n",
    "    \n",
    "    generate_model.to(device)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "        print(\"ğŸ” Special Token í™•ì¸:\")\n",
    "        print(\"-\" * 60)\n",
    "        for token in special_tokens[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥\n",
    "            token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "            print(f\"   {token} â†’ ID: {token_id}\")\n",
    "        if len(special_tokens) > 5:\n",
    "            print(f\"   ... ì™¸ {len(special_tokens) - 5}ê°œ\")\n",
    "        \n",
    "        print('-' * 10, 'Load tokenizer & model complete', '-' * 10)\n",
    "    \n",
    "    return generate_model, tokenizer\n",
    "\n",
    "\n",
    "def load_tokenizer_and_model_for_inference(config, checkpoint_path, device, verbose=True):\n",
    "    \"\"\"\n",
    "    ì¶”ë¡ ì„ ìœ„í•œ tokenizerì™€ fine-tuned ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "    \n",
    "    3-1 ê°œì„ ì‚¬í•­:\n",
    "    - checkpointì—ì„œ ë¡œë“œ ì‹œì—ë„ special tokens ì¼ê´€ì„± ìœ ì§€\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print('-' * 10, 'Load tokenizer & model for inference', '-' * 10)\n",
    "        print('-' * 10, f'Checkpoint: {checkpoint_path}', '-' * 10)\n",
    "    \n",
    "    # Checkpointì—ì„œ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "    \n",
    "    # Special tokens í™•ì¸ ë° ì¶”ê°€ (checkpointì— ì—†ì„ ê²½ìš°)\n",
    "    special_tokens = config['tokenizer']['special_tokens']\n",
    "    existing_special = tokenizer.additional_special_tokens or []\n",
    "    \n",
    "    missing_tokens = [t for t in special_tokens if t not in existing_special]\n",
    "    if missing_tokens:\n",
    "        if verbose:\n",
    "            print(f\"âš ï¸ ëˆ„ë½ëœ special tokens ì¶”ê°€: {missing_tokens}\")\n",
    "        special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "        tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    model = BartForConditionalGeneration.from_pretrained(checkpoint_path)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "        print(f\"   í† í¬ë‚˜ì´ì € vocab size: {len(tokenizer)}\")\n",
    "        print(f\"   ëª¨ë¸ ì„ë² ë”© ì‚¬ì´ì¦ˆ: {model.config.vocab_size}\")\n",
    "        print('-' * 10, 'Load complete', '-' * 10)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvutzKQYvQgl"
   },
   "source": [
    "## 3. ëª¨ë¸ í•™ìŠµí•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImZUb-BC42J-"
   },
   "source": [
    "- ì•ì—ì„œ êµ¬ì¶•í•œ í´ë˜ìŠ¤ ë° í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµ ì§„í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "qnA96wmR44is"
   },
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ í•™ìŠµ ë©”ì¸ í•¨ìˆ˜\n",
    "    3-1, 3-2, 4, 5-1, 6-1, 6-2 ê°œì„ : \n",
    "    - Special tokens + í”„ë¡¬í”„íŠ¸ + êµ¬ì–´ì²´ ì „ì²˜ë¦¬ + TF-IDF í‚¤ì›Œë“œ\n",
    "    - í™”ì ìˆ˜ ëª…ì‹œ + PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ\n",
    "    \"\"\"\n",
    "    # ì‚¬ìš©í•  deviceë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print('-' * 10, f'device : {device}', '-' * 10)\n",
    "    print(torch.__version__)\n",
    "\n",
    "    # ì‚¬ìš©í•  ëª¨ë¸ê³¼ tokenizerë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "    generate_model, tokenizer = load_tokenizer_and_model_for_train(config, device)\n",
    "    print('-' * 10, \"tokenizer special tokens : \", tokenizer.special_tokens_map, '-' * 10)\n",
    "\n",
    "    # 3-2. í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "    prompt_config = config.get('prompt', {\n",
    "        \"use_prompt\": False,\n",
    "        \"prompt_style\": \"none\",\n",
    "        \"use_turn_separator\": False,\n",
    "        \"turn_separator\": \"[SEP]\",\n",
    "        \"tfidf_top_k\": 5,\n",
    "    })\n",
    "    \n",
    "    # 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •\n",
    "    preprocess_config = config.get('preprocess', {\n",
    "        \"normalize_slang\": False,\n",
    "    })\n",
    "    \n",
    "    # 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš© ì„¤ì • (NEW)\n",
    "    regex_pattern_config = config.get('regex_pattern', {\n",
    "        \"use_speaker_count\": True,       # 6-1. í™”ì ìˆ˜ ëª…ì‹œ\n",
    "        \"use_pii_instruction\": True,     # 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ\n",
    "        \"show_pii_examples\": True,       # ë§ˆìŠ¤í‚¹ ì˜ˆì‹œ í‘œì‹œ\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ í”„ë¡¬í”„íŠ¸ ì„¤ì •:\")\n",
    "    print(f\"   - í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {prompt_config.get('use_prompt', False)}\")\n",
    "    print(f\"   - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: {prompt_config.get('prompt_style', 'none')}\")\n",
    "    print(f\"   - ë°œí™” êµ¬ë¶„ì ì‚¬ìš©: {prompt_config.get('use_turn_separator', False)}\")\n",
    "    if prompt_config.get('prompt_style') in ['keyword', 'keyword_aware']:\n",
    "        print(f\"   - TF-IDF í‚¤ì›Œë“œ ìˆ˜: {prompt_config.get('tfidf_top_k', 5)}ê°œ\")\n",
    "    \n",
    "    print(f\"\\nğŸ—£ï¸ êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •:\")\n",
    "    print(f\"   - ìŠ¬ë­ ì •ê·œí™”: {preprocess_config.get('normalize_slang', False)}\")\n",
    "    \n",
    "    print(f\"\\nğŸ” 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš©:\")\n",
    "    print(f\"   - 6-1. í™”ì ìˆ˜ ëª…ì‹œ: {regex_pattern_config.get('use_speaker_count', True)}\")\n",
    "    print(f\"   - 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: {regex_pattern_config.get('use_pii_instruction', True)}\")\n",
    "    \n",
    "    # 5-1. TF-IDF extractor ì‚¬ìš© ì—¬ë¶€ í™•ì¸\n",
    "    use_tfidf = prompt_config.get('prompt_style') in ['keyword', 'keyword_aware']\n",
    "    tfidf_ext = tfidf_extractor if use_tfidf else None\n",
    "    \n",
    "    if use_tfidf:\n",
    "        print(f\"\\nğŸ“Š TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ í™œì„±í™”\")\n",
    "    \n",
    "    preprocessor = Preprocess(\n",
    "        config['tokenizer']['bos_token'], \n",
    "        config['tokenizer']['eos_token'],\n",
    "        prompt_config=prompt_config,\n",
    "        preprocess_config=preprocess_config,\n",
    "        tfidf_extractor=tfidf_ext,           # 5-1. TF-IDF extractor ì „ë‹¬\n",
    "        regex_pattern_config=regex_pattern_config  # 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ ì„¤ì • ì „ë‹¬\n",
    "    )\n",
    "    \n",
    "    data_path = config['general']['data_path']\n",
    "    train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(config, preprocessor, data_path, tokenizer)\n",
    "\n",
    "    # Trainer í´ë˜ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "    trainer = load_trainer_for_train(config, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset)\n",
    "    trainer.train()   # ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
    "\n",
    "    # (ì„ íƒ) wandb ì‚¬ìš© ì‹œì—ë§Œ ì¢…ë£Œ\n",
    "    if config['training']['report_to'] == 'wandb':\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1DMS60wL-Dhv",
    "outputId": "cbb6aba7-18ff-4d12-b9e7-2a2ef31d94d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- device : cuda:0 ----------\n",
      "2.1.0\n",
      "---------- Load tokenizer & model ----------\n",
      "---------- Model Name : digit82/kobart-summarization ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Œ ë“±ë¡í•  Special Tokens (22ê°œ):\n",
      "   ['#Address#', '#Alex#', '#Bob#', '#CarNumber#', '#CardNumber#', '#DateOfBirth#', '#Email#', '#Kristin#', '#Liliana#', '#Name#', '#PassportNumber#', '#Person1#', '#Person2#', '#Person3#', '#Person4#', '#Person5#', '#Person6#', '#Person7#', '#PersonName#', '#PhoneNumber#', '#Price#', '#SSN#']\n",
      "\n",
      "âœ… ìƒˆë¡œ ì¶”ê°€ëœ í† í° ìˆ˜: 22\n",
      "   í† í¬ë‚˜ì´ì € vocab size: 30022\n",
      "   ëª¨ë¸ ì„ë² ë”© ì‚¬ì´ì¦ˆ ì¡°ì • ì™„ë£Œ: 30022\n",
      "\n",
      "------------------------------------------------------------\n",
      "ğŸ” Special Token í™•ì¸:\n",
      "------------------------------------------------------------\n",
      "   #Address# â†’ ID: 30002\n",
      "   #Alex# â†’ ID: 30007\n",
      "   #Bob# â†’ ID: 30008\n",
      "   #CarNumber# â†’ ID: 30006\n",
      "   #CardNumber# â†’ ID: 30020\n",
      "   ... ì™¸ 17ê°œ\n",
      "---------- Load tokenizer & model complete ----------\n",
      "---------- tokenizer special tokens :  {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>', 'additional_special_tokens': ['#Email#', '#Name#', '#Address#', '#Person7#', '#Person3#', '#PhoneNumber#', '#CarNumber#', '#Alex#', '#Bob#', '#PersonName#', '#Price#', '#DateOfBirth#', '#PassportNumber#', '#Person2#', '#Person1#', '#Person6#', '#Kristin#', '#SSN#', '#Person5#', '#Liliana#', '#CardNumber#', '#Person4#']} ----------\n",
      "\n",
      "ğŸ“‹ í”„ë¡¬í”„íŠ¸ ì„¤ì •:\n",
      "   - í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: True\n",
      "   - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: balanced\n",
      "   - ë°œí™” êµ¬ë¶„ì ì‚¬ìš©: False\n",
      "\n",
      "ğŸ—£ï¸ êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •:\n",
      "   - ìŠ¬ë­ ì •ê·œí™”: True\n",
      "\n",
      "ğŸ” 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš©:\n",
      "   - 6-1. í™”ì ìˆ˜ ëª…ì‹œ: True\n",
      "   - 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: True\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[ì›ë³¸] train_data dialogue:\n",
      " #Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? \n",
      "#Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. \n",
      "#Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. \n",
      "#Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”. \n",
      "#Person...\n",
      "train_label:\n",
      " Mr. SmithëŠ” Dr. Hawkinsì—ê²Œ ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ëŸ¬ ì™€ì„œ, ë§¤ë…„ ê²€ì§„ í•„ìš”ì„±ì„ ì•ˆë‚´ë°›ê³  í¡ì—° ìŠµê´€ ê°œì„ ì„ ìœ„í•œ ë„ì›€ì„ ì œì•ˆë°›ì•˜ìŠµë‹ˆë‹¤.\n",
      "\n",
      "------------------------------------------------------------\n",
      "ğŸ” Special Token í™•ì¸:\n",
      "------------------------------------------------------------\n",
      "   #Address# â†’ ID: 30002\n",
      "   #Alex# â†’ ID: 30007\n",
      "   #Bob# â†’ ID: 30008\n",
      "   #CarNumber# â†’ ID: 30006\n",
      "   #CardNumber# â†’ ID: 30020\n",
      "   ... ì™¸ 17ê°œ\n",
      "---------- Load tokenizer & model complete ----------\n",
      "---------- tokenizer special tokens :  {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>', 'additional_special_tokens': ['#Email#', '#Name#', '#Address#', '#Person7#', '#Person3#', '#PhoneNumber#', '#CarNumber#', '#Alex#', '#Bob#', '#PersonName#', '#Price#', '#DateOfBirth#', '#PassportNumber#', '#Person2#', '#Person1#', '#Person6#', '#Kristin#', '#SSN#', '#Person5#', '#Liliana#', '#CardNumber#', '#Person4#']} ----------\n",
      "\n",
      "ğŸ“‹ í”„ë¡¬í”„íŠ¸ ì„¤ì •:\n",
      "   - í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: True\n",
      "   - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: balanced\n",
      "   - ë°œí™” êµ¬ë¶„ì ì‚¬ìš©: False\n",
      "\n",
      "ğŸ—£ï¸ êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •:\n",
      "   - ìŠ¬ë­ ì •ê·œí™”: True\n",
      "\n",
      "ğŸ” 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš©:\n",
      "   - 6-1. í™”ì ìˆ˜ ëª…ì‹œ: True\n",
      "   - 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: True\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[ì›ë³¸] train_data dialogue:\n",
      " #Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? \n",
      "#Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. \n",
      "#Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. \n",
      "#Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”. \n",
      "#Person...\n",
      "train_label:\n",
      " Mr. SmithëŠ” Dr. Hawkinsì—ê²Œ ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ëŸ¬ ì™€ì„œ, ë§¤ë…„ ê²€ì§„ í•„ìš”ì„±ì„ ì•ˆë‚´ë°›ê³  í¡ì—° ìŠµê´€ ê°œì„ ì„ ìœ„í•œ ë„ì›€ì„ ì œì•ˆë°›ì•˜ìŠµë‹ˆë‹¤.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[í”„ë¡¬í”„íŠ¸ ì ìš© í›„] encoder_input_train[0]:\n",
      " ë‹¤ìŒì€ 2ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ë‘ ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \n",
      "\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? #Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. #Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. #Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”. #Person1#: ìŒ, ì‹¬ê°í•œ ì§ˆë³‘ì„ í”¼í•˜ë ¤ë©´ ë¯¸ë¦¬ ë°œê²¬í•˜ëŠ” ê²Œ ì œì¼ ì¢‹ê±°ë“ ìš”. ë³¸ì¸ì„ ìœ„í•´ì„œë¼ë„ ë§¤ë…„ í•œ ...\n",
      "---------- Load data complete ----------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[í”„ë¡¬í”„íŠ¸ ì ìš© í›„] encoder_input_train[0]:\n",
      " ë‹¤ìŒì€ 2ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ë‘ ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \n",
      "\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? #Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. #Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. #Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”. #Person1#: ìŒ, ì‹¬ê°í•œ ì§ˆë³‘ì„ í”¼í•˜ë ¤ë©´ ë¯¸ë¦¬ ë°œê²¬í•˜ëŠ” ê²Œ ì œì¼ ì¢‹ê±°ë“ ìš”. ë³¸ì¸ì„ ìœ„í•´ì„œë¼ë„ ë§¤ë…„ í•œ ...\n",
      "---------- Load data complete ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Make dataset complete ----------\n",
      "\n",
      "ğŸ“Š í† í°í™” ê²°ê³¼ í™•ì¸:\n",
      "   ì²« 50 í† í° ë””ì½”ë”©: ë‹¤ìŒì€ 2ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ë‘ ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \n",
      "\n",
      "#Person1# : ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? ...\n",
      "---------- Make training arguments ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mquriquri7\u001b[0m (\u001b[33mfc_bootcamp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/wandb/run-20251204_172350-ydsutslq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fc_bootcamp/kobart_v1/runs/ydsutslq' target=\"_blank\">baseline_v0</a></strong> to <a href='https://wandb.ai/fc_bootcamp/kobart_v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fc_bootcamp/kobart_v1' target=\"_blank\">https://wandb.ai/fc_bootcamp/kobart_v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fc_bootcamp/kobart_v1/runs/ydsutslq' target=\"_blank\">https://wandb.ai/fc_bootcamp/kobart_v1/runs/ydsutslq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Make training arguments complete ----------\n",
      "---------- Make trainer ----------\n",
      "---------- Make trainer complete ----------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5453' max='15580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5453/15580 28:19 < 52:36, 3.21 it/s, Epoch 7/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.825000</td>\n",
       "      <td>0.566371</td>\n",
       "      <td>0.344045</td>\n",
       "      <td>0.109752</td>\n",
       "      <td>0.325590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.570300</td>\n",
       "      <td>0.525475</td>\n",
       "      <td>0.363801</td>\n",
       "      <td>0.128972</td>\n",
       "      <td>0.340726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.495200</td>\n",
       "      <td>0.510620</td>\n",
       "      <td>0.366163</td>\n",
       "      <td>0.130008</td>\n",
       "      <td>0.344374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.430900</td>\n",
       "      <td>0.509380</td>\n",
       "      <td>0.369940</td>\n",
       "      <td>0.139470</td>\n",
       "      <td>0.345888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.378400</td>\n",
       "      <td>0.517669</td>\n",
       "      <td>0.366900</td>\n",
       "      <td>0.136874</td>\n",
       "      <td>0.344868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.334300</td>\n",
       "      <td>0.528289</td>\n",
       "      <td>0.373542</td>\n",
       "      <td>0.140957</td>\n",
       "      <td>0.351235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.295500</td>\n",
       "      <td>0.543136</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.140288</td>\n",
       "      <td>0.347263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ #Person2# ì—ê²Œ ê°ê¸°ì— ê±¸ë ¸ìœ¼ë©°, ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ë¥¼ ë°©ë¬¸í•˜ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                                            \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ ì•¼ Jimmyì—ê²Œ ìš´ë™í•˜ëŸ¬ ê°€ìê³  ì œì•ˆí•©ë‹ˆë‹¤. ì•¼ëŠ” í† ìš”ì¼ì— ìš´ë™í•  ì˜ˆì •ì…ë‹ˆë‹¤.                                                                             \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ #Person2# ì—ê²Œ ê³¼ì¼ê³¼ ì±„ì†Œ, ë‹­ê³ ê¸°ë¥¼ ë¨¹ìœ¼ë¼ê³  ê¶Œí•©ë‹ˆë‹¤.                                                                                 \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ê°ê¸°ì— ê±¸ë ¸ì§€ë§Œ, #Person1# ì€ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                                      \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ Jimmyì—ê²Œ ìš´ë™ ë‚ ì§œë¥¼ ë³€ê²½í•´ ë‹¬ë¼ê³  ìš”ì²­í•˜ê³ , JimmyëŠ” ê¸ˆìš”ì¼ì— í•˜ê¸°ë¡œ ì•½ì†í•œë‹¤.                                                                     \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ #Person2# ì—ê²Œ ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ìœ¼ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                                             \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ê°ê¸°ì— ê±¸ë ¸ì§€ë§Œ, #Person1# ì€ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                                      \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ Jimmyì—ê²Œ ìš´ë™ ë‚ ì§œë¥¼ ë³€ê²½í•´ ë‹¬ë¼ê³  ìš”ì²­í•˜ê³ , JimmyëŠ” ê¸ˆìš”ì¼ì— í•˜ê¸°ë¡œ ì•½ì†í•œë‹¤.                                                                     \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ #Person2# ì—ê²Œ ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ìœ¼ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                                             \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ìˆ¨ì‰¬ê¸° í˜ë“¤ë‹¤ê³  í˜¸ì†Œí•©ë‹ˆë‹¤. #Person1# ì€ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ì¡°ì–¸í•©ë‹ˆë‹¤.                                                           \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ Jimmyì—ê²Œ ìš´ë™ ì‹œê°„ì„ ë‹¬ë¼ê³  ìš”ì²­í•˜ê³ , JimmyëŠ” ë™ì˜í•œë‹¤.                                                                 \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ì¤„ì´ê³  ì‹¶ì–´ í•œë‹¤. #Person2# ëŠ” ê³¼ì¼ê³¼ ì±„ì†Œë¥¼ ì¢‹ì•„í•œë‹¤ê³  ë§í•œë‹¤.                                                                \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ìˆ¨ì‰¬ê¸° í˜ë“¤ë‹¤ê³  í˜¸ì†Œí•©ë‹ˆë‹¤. #Person1# ì€ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ì¡°ì–¸í•©ë‹ˆë‹¤.                                                           \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ Jimmyì—ê²Œ ìš´ë™ ì‹œê°„ì„ ë‹¬ë¼ê³  ìš”ì²­í•˜ê³ , JimmyëŠ” ë™ì˜í•œë‹¤.                                                                 \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ì¤„ì´ê³  ì‹¶ì–´ í•œë‹¤. #Person2# ëŠ” ê³¼ì¼ê³¼ ì±„ì†Œë¥¼ ì¢‹ì•„í•œë‹¤ê³  ë§í•œë‹¤.                                                                \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ìˆ¨ì‰¬ê¸° í˜ë“¤ë‹¤ê³  í˜¸ì†Œí•˜ë©°, #Person1# ì€ #Person2# ì—ê²Œ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ê¶Œí•©ë‹ˆë‹¤.                                                          \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì´ Jimmyì—ê²Œ ìš´ë™ ì‹œê°„ì„ ë‹¬ë¼ê³  ìš”ì²­í•˜ê³ , JimmyëŠ” ê¸ˆìš”ì¼ì— í•˜ê¸°ë¡œ ì•½ì†í•œë‹¤.                                                                 \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ì¤„ì´ê³  ì‹¶ì–´ í•©ë‹ˆë‹¤. #Person1# ì€ #Person2# ì—ê²Œ ê³¼ì¼, ì±„ì†Œ, ë‹­ê³ ê¸°ë¥¼ êµ¬ì›Œ ë¨¹ìœ¼ë©´ ê±´ê°•ì— ì¢‹ë‹¤ê³  ë§í•©ë‹ˆë‹¤.                                                         \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ìˆ¨ì‰¬ê¸° í˜ë“¤ë‹¤ê³  í˜¸ì†Œí•˜ë©°, #Person1# ì€ #Person2# ì—ê²Œ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ê¶Œí•©ë‹ˆë‹¤.                                                          \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì´ Jimmyì—ê²Œ ìš´ë™ ì‹œê°„ì„ ë‹¬ë¼ê³  ìš”ì²­í•˜ê³ , JimmyëŠ” ê¸ˆìš”ì¼ì— í•˜ê¸°ë¡œ ì•½ì†í•œë‹¤.                                                                 \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ì¤„ì´ê³  ì‹¶ì–´ í•©ë‹ˆë‹¤. #Person1# ì€ #Person2# ì—ê²Œ ê³¼ì¼, ì±„ì†Œ, ë‹­ê³ ê¸°ë¥¼ êµ¬ì›Œ ë¨¹ìœ¼ë©´ ê±´ê°•ì— ì¢‹ë‹¤ê³  ë§í•©ë‹ˆë‹¤.                                                         \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›€ì„ í˜¸ì†Œí•˜ë©°, #Person1# ì€ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                    \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ Jimmyì—ê²Œ ìš´ë™ ë‚ ì§œë¥¼ ì•Œë ¤ì£¼ê³ , JimmyëŠ” í† ìš”ì¼ì— í•˜ê¸°ë¡œ ì•½ì†í•œë‹¤.                                                      \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ì¤„ì´ê³  ì‹¶ì–´ í•œë‹¤. #Person2# ëŠ” #Person1# ì—ê²Œ êµ¬ìš´ ë‹­ê³ ê¸°ë¥¼ ì¶”ì²œí•œë‹¤.                                                      \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›€ì„ í˜¸ì†Œí•˜ë©°, #Person1# ì€ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                    \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ Jimmyì—ê²Œ ìš´ë™ ë‚ ì§œë¥¼ ì•Œë ¤ì£¼ê³ , JimmyëŠ” í† ìš”ì¼ì— í•˜ê¸°ë¡œ ì•½ì†í•œë‹¤.                                                      \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ì¤„ì´ê³  ì‹¶ì–´ í•œë‹¤. #Person2# ëŠ” #Person1# ì—ê²Œ êµ¬ìš´ ë‹­ê³ ê¸°ë¥¼ ì¶”ì²œí•œë‹¤.                                                      \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›€ì„ í˜¸ì†Œí•˜ë©°, #Person1# ì€ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                     \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì´ Jimmyì—ê²Œ ìš´ë™ ë‚ ì§œë¥¼ ì•Œë ¤ì£¼ê³ , JimmyëŠ” ë™ì˜í•©ë‹ˆë‹¤.                                                          \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ì¤„ì´ê³  ì‹¶ì–´ í•©ë‹ˆë‹¤. #Person1# ì€ #Person2# ì—ê²Œ ë‹¤ë¥¸ ê²ƒì€ ë¨¹ì§€ ë§ë¼ê³  ê¶Œí•©ë‹ˆë‹¤.                                                      \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›€ì„ í˜¸ì†Œí•˜ë©°, #Person1# ì€ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                     \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì´ Jimmyì—ê²Œ ìš´ë™ ë‚ ì§œë¥¼ ì•Œë ¤ì£¼ê³ , JimmyëŠ” ë™ì˜í•©ë‹ˆë‹¤.                                                          \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ì¤„ì´ê³  ì‹¶ì–´ í•©ë‹ˆë‹¤. #Person1# ì€ #Person2# ì—ê²Œ ë‹¤ë¥¸ ê²ƒì€ ë¨¹ì§€ ë§ë¼ê³  ê¶Œí•©ë‹ˆë‹¤.                                                      \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›€ì„ í˜¸ì†Œí•˜ë©°, #Person1# ì€ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                                          \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:   JimmyëŠ” ë‹¤ë¦¬ ìš´ë™ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë ¤ í•˜ì§€ë§Œ #Person1# ì€ ì£¼ê°„ ì¼ì •ì— ë³€ë™ì´ ìˆì–´ ê±°ì ˆí•œë‹¤.                                                                         \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ì¤„ì´ê³  ì‹¶ì–´ í•©ë‹ˆë‹¤. #Person1# ì€ #Person2# ì—ê²Œ êµ¬ìš´ ë‹­ê³ ê¸°ë¥¼ ê¶Œí•©ë‹ˆë‹¤.                                                                            \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›€ì„ í˜¸ì†Œí•˜ë©°, #Person1# ì€ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                                          \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:   JimmyëŠ” ë‹¤ë¦¬ ìš´ë™ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë ¤ í•˜ì§€ë§Œ #Person1# ì€ ì£¼ê°„ ì¼ì •ì— ë³€ë™ì´ ìˆì–´ ê±°ì ˆí•œë‹¤.                                                                         \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ì¤„ì´ê³  ì‹¶ì–´ í•©ë‹ˆë‹¤. #Person1# ì€ #Person2# ì—ê²Œ êµ¬ìš´ ë‹­ê³ ê¸°ë¥¼ ê¶Œí•©ë‹ˆë‹¤.                                                                            \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d57ff742ad14e2c95ae336c8cea9a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='472.707 MB of 472.707 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>â–ˆâ–ƒâ–â–â–‚â–ƒâ–…</td></tr><tr><td>eval/rouge-1</td><td>â–â–†â–†â–‡â–†â–ˆâ–‡</td></tr><tr><td>eval/rouge-2</td><td>â–â–…â–†â–ˆâ–‡â–ˆâ–ˆ</td></tr><tr><td>eval/rouge-l</td><td>â–â–…â–†â–‡â–†â–ˆâ–‡</td></tr><tr><td>eval/runtime</td><td>â–„â–â–„â–„â–‚â–ƒâ–ˆ</td></tr><tr><td>eval/samples_per_second</td><td>â–…â–ˆâ–…â–…â–‡â–†â–</td></tr><tr><td>eval/steps_per_second</td><td>â–…â–ˆâ–…â–…â–‡â–†â–</td></tr><tr><td>train/epoch</td><td>â–â–â–‚â–‚â–ƒâ–ƒâ–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–‚â–‚â–ƒâ–ƒâ–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/learning_rate</td><td>â–â–ˆâ–ˆâ–ˆâ–‡â–†â–†</td></tr><tr><td>train/loss</td><td>â–ˆâ–‚â–‚â–â–â–â–</td></tr><tr><td>train/total_flos</td><td>â–</td></tr><tr><td>train/train_loss</td><td>â–</td></tr><tr><td>train/train_runtime</td><td>â–</td></tr><tr><td>train/train_samples_per_second</td><td>â–</td></tr><tr><td>train/train_steps_per_second</td><td>â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.54314</td></tr><tr><td>eval/rouge-1</td><td>0.371</td></tr><tr><td>eval/rouge-2</td><td>0.14029</td></tr><tr><td>eval/rouge-l</td><td>0.34726</td></tr><tr><td>eval/runtime</td><td>10.5327</td></tr><tr><td>eval/samples_per_second</td><td>47.376</td></tr><tr><td>eval/steps_per_second</td><td>1.519</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>5453</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>0.2955</td></tr><tr><td>train/total_flos</td><td>4.927423881010176e+16</td></tr><tr><td>train/train_loss</td><td>0.76137</td></tr><tr><td>train/train_runtime</td><td>1697.3706</td></tr><tr><td>train/train_samples_per_second</td><td>146.78</td></tr><tr><td>train/train_steps_per_second</td><td>9.179</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">baseline_v0</strong> at: <a href='https://wandb.ai/fc_bootcamp/kobart_v1/runs/ydsutslq' target=\"_blank\">https://wandb.ai/fc_bootcamp/kobart_v1/runs/ydsutslq</a><br/>Synced 6 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251204_172350-ydsutslq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(loaded_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFtWqowCGzEc"
   },
   "source": [
    "## 4. ëª¨ë¸ ì¶”ë¡ í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ê³³ì— ë‚´ê°€ ì‚¬ìš©í•  wandb config ì„¤ì •\n",
    "loaded_config['inference']['ckt_path'] = \"./prediction_kobart_v1/checkpoint-4674\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFGul3-rSscf"
   },
   "source": [
    "- test dataë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "lV1Do7nlTylG"
   },
   "outputs": [],
   "source": [
    "# tokenization ê³¼ì •ê¹Œì§€ ì§„í–‰ëœ ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ì— ì…ë ¥ë  ë°ì´í„°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "def prepare_test_dataset(config, preprocessor, tokenizer):\n",
    "    \"\"\"\n",
    "    3-2 ê°œì„ : í”„ë¡¬í”„íŠ¸ ì„¤ì •ì´ ì ìš©ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "    \"\"\"\n",
    "    test_file_path = os.path.join(config['general']['data_path'],'test.csv')\n",
    "\n",
    "    test_data = preprocessor.make_set_as_df(test_file_path, is_train=False)\n",
    "    test_id = test_data['fname']\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f'[ì›ë³¸] test_data dialogue:\\n{test_data[\"dialogue\"][0][:200]}...')\n",
    "    print('-'*150)\n",
    "\n",
    "    # 3-2. í”„ë¡¬í”„íŠ¸ ì ìš©ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë³€í™˜\n",
    "    encoder_input_test, decoder_input_test = preprocessor.make_input(\n",
    "        test_data, \n",
    "        is_test=True,\n",
    "        apply_dynamic_prompt=True  # í”„ë¡¬í”„íŠ¸ ì„¤ì • ì‚¬ìš©\n",
    "    )\n",
    "    \n",
    "    # í”„ë¡¬í”„íŠ¸ ì ìš© í™•ì¸\n",
    "    print(f'[í”„ë¡¬í”„íŠ¸ ì ìš© í›„] encoder_input_test[0]:\\n{encoder_input_test[0][:300]}...')\n",
    "    print('-'*10, 'Load data complete', '-'*10)\n",
    "\n",
    "    test_tokenized_encoder_inputs = tokenizer(encoder_input_test, return_tensors=\"pt\", padding=True,\n",
    "                    add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False,)\n",
    "    test_tokenized_decoder_inputs = tokenizer(decoder_input_test, return_tensors=\"pt\", padding=True,\n",
    "                    add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False,)\n",
    "\n",
    "    test_encoder_inputs_dataset = DatasetForInference(test_tokenized_encoder_inputs, test_id, len(encoder_input_test))\n",
    "    print('-'*10, 'Make dataset complete', '-'*10)\n",
    "\n",
    "    return test_data, test_encoder_inputs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "eb49bLULT3aS"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ì¶”ë¡ ì„ ìœ„í•œ tokenizerì™€ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "# 3-1 ê°œì„ : ë™ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ special tokens ì‚¬ìš©\n",
    "# ============================================================================\n",
    "\n",
    "def load_tokenizer_and_model_for_test(config, device, verbose=True):\n",
    "    \"\"\"\n",
    "    ì¶”ë¡ ìš© í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ\n",
    "    3-1: ë™ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ special tokens ë“±ë¡\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print('-' * 10, 'Load tokenizer & model', '-' * 10)\n",
    "\n",
    "    model_name = config['general']['model_name']\n",
    "    ckt_path = config['inference']['ckt_path']\n",
    "    \n",
    "    if verbose:\n",
    "        print('-' * 10, f'Model Name : {model_name}', '-' * 10)\n",
    "        print('-' * 10, f'Checkpoint : {ckt_path}', '-' * 10)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # 3-1. ë™ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ special tokens ë“±ë¡\n",
    "    special_tokens = config['tokenizer']['special_tokens']\n",
    "    special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "    num_added = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nğŸ“Œ ë“±ë¡ëœ Special Tokens ({len(special_tokens)}ê°œ)\")\n",
    "        print(f\"   ìƒˆë¡œ ì¶”ê°€ëœ í† í°: {num_added}ê°œ\")\n",
    "\n",
    "    generate_model = BartForConditionalGeneration.from_pretrained(ckt_path)\n",
    "    generate_model.resize_token_embeddings(len(tokenizer))\n",
    "    generate_model.to(device)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   í† í¬ë‚˜ì´ì € vocab size: {len(tokenizer)}\")\n",
    "        print(f\"   ëª¨ë¸ ì„ë² ë”© ì‚¬ì´ì¦ˆ: {generate_model.config.vocab_size}\")\n",
    "        print('-' * 10, 'Load tokenizer & model complete', '-' * 10)\n",
    "\n",
    "    return generate_model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Axzu9rsoGLgJ"
   },
   "outputs": [],
   "source": [
    "# í•™ìŠµëœ ëª¨ë¸ì´ ìƒì„±í•œ ìš”ì•½ë¬¸ì˜ ì¶œë ¥ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "def inference(config):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ ì¶”ë¡  í•¨ìˆ˜\n",
    "    3-1, 3-2, 4, 5-1, 5-2, 6-1, 6-2 ê°œì„ : \n",
    "    - Special tokens ë™ì  ë“±ë¡\n",
    "    - í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "    - êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì ìš©\n",
    "    - TF-IDF í‚¤ì›Œë“œ í”„ë¡¬í”„íŠ¸\n",
    "    - ë™ì  ì¶”ë¡  ì„¤ì • (ê¸¸ì´ ê¸°ë°˜)\n",
    "    - í™”ì ìˆ˜ ëª…ì‹œ (6-1)\n",
    "    - PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ (6-2)\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print('-' * 10, f'device : {device}', '-' * 10)\n",
    "    print(torch.__version__)\n",
    "\n",
    "    generate_model, tokenizer = load_tokenizer_and_model_for_test(config, device)\n",
    "\n",
    "    # 3-2. í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "    prompt_config = config.get('prompt', {\n",
    "        \"use_prompt\": False,\n",
    "        \"prompt_style\": \"none\",\n",
    "        \"use_turn_separator\": False,\n",
    "        \"turn_separator\": \"[SEP]\",\n",
    "        \"tfidf_top_k\": 5,\n",
    "    })\n",
    "    \n",
    "    # 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •\n",
    "    preprocess_config = config.get('preprocess', {\n",
    "        \"normalize_slang\": False,\n",
    "    })\n",
    "    \n",
    "    # 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš© ì„¤ì • (NEW)\n",
    "    regex_pattern_config = config.get('regex_pattern', {\n",
    "        \"use_speaker_count\": True,       # 6-1. í™”ì ìˆ˜ ëª…ì‹œ\n",
    "        \"use_pii_instruction\": True,     # 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ\n",
    "        \"show_pii_examples\": True,       # ë§ˆìŠ¤í‚¹ ì˜ˆì‹œ í‘œì‹œ\n",
    "    })\n",
    "    \n",
    "    # 5-2. ë™ì  ì¶”ë¡  ì„¤ì • (ê¸¸ì´ ê¸°ë°˜)\n",
    "    dynamic_inference_config = config.get('dynamic_inference', {})\n",
    "    use_dynamic = dynamic_inference_config.get('use_length_based', False) or dynamic_inference_config.get('use_dynamic_settings', False)\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ ì¶”ë¡  ì‹œ í”„ë¡¬í”„íŠ¸ ì„¤ì •:\")\n",
    "    print(f\"   - í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {prompt_config.get('use_prompt', False)}\")\n",
    "    print(f\"   - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: {prompt_config.get('prompt_style', 'none')}\")\n",
    "    if prompt_config.get('prompt_style') in ['keyword', 'keyword_aware']:\n",
    "        print(f\"   - TF-IDF í‚¤ì›Œë“œ ìˆ˜: {prompt_config.get('tfidf_top_k', 5)}ê°œ\")\n",
    "    \n",
    "    print(f\"\\nğŸ—£ï¸ ì¶”ë¡  ì‹œ êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •:\")\n",
    "    print(f\"   - ìŠ¬ë­ ì •ê·œí™”: {preprocess_config.get('normalize_slang', False)}\")\n",
    "    \n",
    "    print(f\"\\nğŸ” 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš©:\")\n",
    "    print(f\"   - 6-1. í™”ì ìˆ˜ ëª…ì‹œ: {regex_pattern_config.get('use_speaker_count', True)}\")\n",
    "    print(f\"   - 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: {regex_pattern_config.get('use_pii_instruction', True)}\")\n",
    "    \n",
    "    print(f\"\\nâš™ï¸ ë™ì  ì¶”ë¡  ì„¤ì •:\")\n",
    "    print(f\"   - ë™ì  ì„¤ì • ì‚¬ìš©: {use_dynamic}\")\n",
    "    if use_dynamic:\n",
    "        print(f\"   - ê¸¸ì´ ê¸°ë°˜: {dynamic_inference_config.get('use_length_based', False)}\")\n",
    "        print(f\"   - ì£¼ì œ ê¸°ë°˜: {dynamic_inference_config.get('use_topic_based', False)}\")\n",
    "    \n",
    "    # 5-1. TF-IDF extractor ì‚¬ìš© ì—¬ë¶€ í™•ì¸\n",
    "    use_tfidf = prompt_config.get('prompt_style') in ['keyword', 'keyword_aware']\n",
    "    tfidf_ext = tfidf_extractor if use_tfidf else None\n",
    "    \n",
    "    if use_tfidf:\n",
    "        print(f\"\\nğŸ“Š TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ í™œì„±í™”\")\n",
    "    \n",
    "    preprocessor = Preprocess(\n",
    "        config['tokenizer']['bos_token'], \n",
    "        config['tokenizer']['eos_token'],\n",
    "        prompt_config=prompt_config,               # 3-2. í”„ë¡¬í”„íŠ¸ ì„¤ì • ì „ë‹¬\n",
    "        preprocess_config=preprocess_config,       # 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì • ì „ë‹¬\n",
    "        tfidf_extractor=tfidf_ext,                 # 5-1. TF-IDF extractor ì „ë‹¬\n",
    "        regex_pattern_config=regex_pattern_config  # 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ ì„¤ì • ì „ë‹¬\n",
    "    )\n",
    "\n",
    "    data_path = config['general']['data_path']\n",
    "    test_data, test_encoder_inputs_dataset = prepare_test_dataset(config, preprocessor, tokenizer)\n",
    "    \n",
    "    # 5-2. ë™ì  ì¶”ë¡ ì„ ìœ„í•´ ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´ ì •ë³´ í•„ìš”\n",
    "    dialogue_lengths = []\n",
    "    if use_dynamic:\n",
    "        # test_dataì—ì„œ ëŒ€í™”ë¬¸ ê¸¸ì´ ê³„ì‚°\n",
    "        for idx in range(len(test_data)):\n",
    "            dialogue = test_data.iloc[idx]['dialogue']\n",
    "            dialogue_lengths.append(len(dialogue))\n",
    "        print(f\"\\nğŸ“ ëŒ€í™”ë¬¸ ê¸¸ì´ ë¶„í¬: min={min(dialogue_lengths)}, max={max(dialogue_lengths)}, avg={sum(dialogue_lengths)/len(dialogue_lengths):.0f}\")\n",
    "    \n",
    "    dataloader = DataLoader(test_encoder_inputs_dataset, batch_size=config['inference']['batch_size'])\n",
    "\n",
    "    summary = []\n",
    "    text_ids = []\n",
    "    \n",
    "    # 5-2. ë™ì  ì¶”ë¡  ì‹œ ë°°ì¹˜ ë‹¨ìœ„ê°€ ì•„ë‹Œ ê°œë³„ ìƒ˜í”Œ ì²˜ë¦¬ í•„ìš”\n",
    "    if use_dynamic:\n",
    "        print(\"\\nğŸš€ ë™ì  ì¶”ë¡  ëª¨ë“œë¡œ ìƒì„± ì‹œì‘...\")\n",
    "        sample_idx = 0\n",
    "        use_topic = dynamic_inference_config.get('use_topic_based', True)\n",
    "        use_length = dynamic_inference_config.get('use_length_based', True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for item in tqdm(dataloader):\n",
    "                batch_size = len(item['ID'])\n",
    "                text_ids.extend(item['ID'])\n",
    "                \n",
    "                for i in range(batch_size):\n",
    "                    # ê°œë³„ ìƒ˜í”Œì— ëŒ€í•œ ë™ì  ì„¤ì • ê³„ì‚° (ì›ë³¸ ëŒ€í™”ë¬¸ ì‚¬ìš©)\n",
    "                    dialogue_text = test_data.iloc[sample_idx]['dialogue']\n",
    "                    dynamic_cfg = get_dynamic_inference_config(dialogue_text, use_topic=use_topic, use_length=use_length)\n",
    "                    \n",
    "                    # ë‹¨ì¼ ìƒ˜í”Œ ìƒì„±\n",
    "                    single_input_ids = item['input_ids'][i:i+1].to(device)\n",
    "                    generated_ids = generate_model.generate(\n",
    "                        input_ids=single_input_ids,\n",
    "                        no_repeat_ngram_size=dynamic_cfg['no_repeat_ngram_size'],\n",
    "                        early_stopping=config['inference']['early_stopping'],\n",
    "                        max_length=dynamic_cfg['max_new_tokens'],\n",
    "                        num_beams=dynamic_cfg['num_beams'],\n",
    "                        length_penalty=dynamic_cfg['length_penalty'],\n",
    "                    )\n",
    "                    \n",
    "                    # 3-1 ê°œì„ : í™”ì íƒœê·¸ ë³´ì¡´í•˜ë©´ì„œ ë””ì½”ë”©\n",
    "                    result = decode_with_speaker_tags(tokenizer, generated_ids[0])\n",
    "                    summary.append(result)\n",
    "                    sample_idx += 1\n",
    "    else:\n",
    "        # ê¸°ì¡´ ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹\n",
    "        with torch.no_grad():\n",
    "            for item in tqdm(dataloader):\n",
    "                text_ids.extend(item['ID'])\n",
    "                generated_ids = generate_model.generate(\n",
    "                    input_ids=item['input_ids'].to(device),\n",
    "                    no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],\n",
    "                    early_stopping=config['inference']['early_stopping'],\n",
    "                    max_length=config['inference']['generate_max_length'],\n",
    "                    num_beams=config['inference']['num_beams'],\n",
    "                )\n",
    "                for ids in generated_ids:\n",
    "                    # 3-1 ê°œì„ : í™”ì íƒœê·¸ ë³´ì¡´í•˜ë©´ì„œ ë””ì½”ë”©\n",
    "                    result = decode_with_speaker_tags(tokenizer, ids)\n",
    "                    summary.append(result)\n",
    "\n",
    "    # í›„ì²˜ë¦¬ ì ìš© (ë„ì–´ì“°ê¸° êµì •)\n",
    "    preprocessed_summary = [postprocess_summary(s) for s in summary]\n",
    "\n",
    "    output = pd.DataFrame(\n",
    "        {\n",
    "            \"fname\": test_data['fname'],\n",
    "            \"summary\": preprocessed_summary,\n",
    "        }\n",
    "    )\n",
    "    result_path = config['inference']['result_path']\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    output.to_csv(os.path.join(result_path, \"output.csv\"), index=False)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "-pJ1ZXf-5V50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- device : cuda:0 ----------\n",
      "2.1.0\n",
      "---------- Load tokenizer & model ----------\n",
      "---------- Model Name : digit82/kobart-summarization ----------\n",
      "---------- Checkpoint : ./prediction_kobart_v1/checkpoint-4674 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Œ ë“±ë¡ëœ Special Tokens (22ê°œ)\n",
      "   ìƒˆë¡œ ì¶”ê°€ëœ í† í°: 22ê°œ\n",
      "   í† í¬ë‚˜ì´ì € vocab size: 30022\n",
      "   ëª¨ë¸ ì„ë² ë”© ì‚¬ì´ì¦ˆ: 30022\n",
      "---------- Load tokenizer & model complete ----------\n",
      "\n",
      "ğŸ“‹ ì¶”ë¡  ì‹œ í”„ë¡¬í”„íŠ¸ ì„¤ì •:\n",
      "   - í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: True\n",
      "   - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: balanced\n",
      "\n",
      "ğŸ—£ï¸ ì¶”ë¡  ì‹œ êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •:\n",
      "   - ìŠ¬ë­ ì •ê·œí™”: True\n",
      "\n",
      "ğŸ” 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš©:\n",
      "   - 6-1. í™”ì ìˆ˜ ëª…ì‹œ: True\n",
      "   - 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: True\n",
      "\n",
      "âš™ï¸ ë™ì  ì¶”ë¡  ì„¤ì •:\n",
      "   - ë™ì  ì„¤ì • ì‚¬ìš©: True\n",
      "   - ê¸¸ì´ ê¸°ë°˜: True\n",
      "   - ì£¼ì œ ê¸°ë°˜: True\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[ì›ë³¸] test_data dialogue:\n",
      "#Person1#: Ms. Dawson, ë°›ì•„ì“°ê¸° ì¢€ ë¶€íƒë“œë ¤ì•¼ê² ì–´ìš”. \n",
      "#Person2#: ë„¤, ë§ì”€í•˜ì„¸ìš”... \n",
      "#Person1#: ì´ê±¸ ì˜¤ëŠ˜ ì˜¤í›„ê¹Œì§€ ëª¨ë“  ì§ì›ë“¤ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¡œ ë³´ë‚´ì•¼ í•´ìš”. ì¤€ë¹„ëë‚˜ìš”? \n",
      "#Person2#: ë„¤, ë§ì”€í•˜ì„¸ìš”. \n",
      "#Person1#: ëª¨ë“  ì§ì›ì—ê²Œ ì•Œë¦½ë‹ˆë‹¤... ì¦‰ì‹œ ë°œíš¨ë˜ì–´ ëª¨ë“  ì‚¬ë‚´ í†µì‹ ì€ ì´ë©”ì¼ê³¼ ê³µì‹ ë©”ëª¨ë¡œë§Œ ì œ...\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[í”„ë¡¬í”„íŠ¸ ì ìš© í›„] encoder_input_test[0]:\n",
      "ë‹¤ìŒì€ 2ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ë‘ ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \n",
      "\n",
      "#Person1#: Ms. Dawson, ë°›ì•„ì“°ê¸° ì¢€ ë¶€íƒë“œë ¤ì•¼ê² ì–´ìš”. #Person2#: ë„¤, ë§ì”€í•˜ì„¸ìš”... #Person1#: ì´ê±¸ ì˜¤ëŠ˜ ì˜¤í›„ê¹Œì§€ ëª¨ë“  ì§ì›ë“¤ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¡œ ë³´ë‚´ì•¼ í•´ìš”. ì¤€ë¹„ëë‚˜ìš”? #Person2#: ë„¤, ë§ì”€í•˜ì„¸ìš”. #Person1#: ëª¨ë“  ì§ì›ì—ê²Œ ì•Œë¦½ë‹ˆë‹¤... ì¦‰ì‹œ ë°œíš¨ë˜ì–´ ëª¨ë“  ì‚¬ë‚´ í†µì‹ ì€ ì´ë©”ì¼ê³¼ ê³µì‹ ë©”ëª¨ë¡œë§Œ ì œí•œë©ë‹ˆë‹¤. ê·¼ë¬´ ì‹œê°„ ë™ì•ˆ ì¦‰ì‹œ ë©”ì‹œì§€ í”„ë¡œê·¸ë¨ ì‚¬ ìš” ì€ ê¸ˆì§€ë©ë‹ˆë‹¤. #Person2#: ì´ ì •...\n",
      "---------- Load data complete ----------\n",
      "---------- Make dataset complete ----------\n",
      "\n",
      "ğŸ“ ëŒ€í™”ë¬¸ ê¸¸ì´ ë¶„í¬: min=111, max=2275, avg=422\n",
      "\n",
      "ğŸš€ ë™ì  ì¶”ë¡  ëª¨ë“œë¡œ ìƒì„± ì‹œì‘...\n",
      "   í† í¬ë‚˜ì´ì € vocab size: 30022\n",
      "   ëª¨ë¸ ì„ë² ë”© ì‚¬ì´ì¦ˆ: 30022\n",
      "---------- Load tokenizer & model complete ----------\n",
      "\n",
      "ğŸ“‹ ì¶”ë¡  ì‹œ í”„ë¡¬í”„íŠ¸ ì„¤ì •:\n",
      "   - í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: True\n",
      "   - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: balanced\n",
      "\n",
      "ğŸ—£ï¸ ì¶”ë¡  ì‹œ êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •:\n",
      "   - ìŠ¬ë­ ì •ê·œí™”: True\n",
      "\n",
      "ğŸ” 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš©:\n",
      "   - 6-1. í™”ì ìˆ˜ ëª…ì‹œ: True\n",
      "   - 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: True\n",
      "\n",
      "âš™ï¸ ë™ì  ì¶”ë¡  ì„¤ì •:\n",
      "   - ë™ì  ì„¤ì • ì‚¬ìš©: True\n",
      "   - ê¸¸ì´ ê¸°ë°˜: True\n",
      "   - ì£¼ì œ ê¸°ë°˜: True\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[ì›ë³¸] test_data dialogue:\n",
      "#Person1#: Ms. Dawson, ë°›ì•„ì“°ê¸° ì¢€ ë¶€íƒë“œë ¤ì•¼ê² ì–´ìš”. \n",
      "#Person2#: ë„¤, ë§ì”€í•˜ì„¸ìš”... \n",
      "#Person1#: ì´ê±¸ ì˜¤ëŠ˜ ì˜¤í›„ê¹Œì§€ ëª¨ë“  ì§ì›ë“¤ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¡œ ë³´ë‚´ì•¼ í•´ìš”. ì¤€ë¹„ëë‚˜ìš”? \n",
      "#Person2#: ë„¤, ë§ì”€í•˜ì„¸ìš”. \n",
      "#Person1#: ëª¨ë“  ì§ì›ì—ê²Œ ì•Œë¦½ë‹ˆë‹¤... ì¦‰ì‹œ ë°œíš¨ë˜ì–´ ëª¨ë“  ì‚¬ë‚´ í†µì‹ ì€ ì´ë©”ì¼ê³¼ ê³µì‹ ë©”ëª¨ë¡œë§Œ ì œ...\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[í”„ë¡¬í”„íŠ¸ ì ìš© í›„] encoder_input_test[0]:\n",
      "ë‹¤ìŒì€ 2ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ë‘ ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \n",
      "\n",
      "#Person1#: Ms. Dawson, ë°›ì•„ì“°ê¸° ì¢€ ë¶€íƒë“œë ¤ì•¼ê² ì–´ìš”. #Person2#: ë„¤, ë§ì”€í•˜ì„¸ìš”... #Person1#: ì´ê±¸ ì˜¤ëŠ˜ ì˜¤í›„ê¹Œì§€ ëª¨ë“  ì§ì›ë“¤ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¡œ ë³´ë‚´ì•¼ í•´ìš”. ì¤€ë¹„ëë‚˜ìš”? #Person2#: ë„¤, ë§ì”€í•˜ì„¸ìš”. #Person1#: ëª¨ë“  ì§ì›ì—ê²Œ ì•Œë¦½ë‹ˆë‹¤... ì¦‰ì‹œ ë°œíš¨ë˜ì–´ ëª¨ë“  ì‚¬ë‚´ í†µì‹ ì€ ì´ë©”ì¼ê³¼ ê³µì‹ ë©”ëª¨ë¡œë§Œ ì œí•œë©ë‹ˆë‹¤. ê·¼ë¬´ ì‹œê°„ ë™ì•ˆ ì¦‰ì‹œ ë©”ì‹œì§€ í”„ë¡œê·¸ë¨ ì‚¬ ìš” ì€ ê¸ˆì§€ë©ë‹ˆë‹¤. #Person2#: ì´ ì •...\n",
      "---------- Load data complete ----------\n",
      "---------- Make dataset complete ----------\n",
      "\n",
      "ğŸ“ ëŒ€í™”ë¬¸ ê¸¸ì´ ë¶„í¬: min=111, max=2275, avg=422\n",
      "\n",
      "ğŸš€ ë™ì  ì¶”ë¡  ëª¨ë“œë¡œ ìƒì„± ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:24<00:00,  5.29s/it]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµëœ ëª¨ë¸ì˜ testë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "if __name__ == \"__main__\":\n",
    "    output = inference(loaded_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "OsPmLfhbzZqS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>#Person1# ì€ Ms. Dawsonì—ê²Œ ì´ë©”ì¼ê³¼ ê³µì‹ ë©”ëª¨ë¡œë§Œ ì œí•œë˜ëŠ” ì‚¬ë‚´ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>#Person1# ê³¼ #Person2# ëŠ” êµí†µì²´ì¦ìœ¼ë¡œ ì¸í•´ ì¶œí‡´ê·¼ì— ì–´ë ¤ì›€ì„ ê²ªê³ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>#Person1# ì€ Kateì—ê²Œ Mashaì™€ Heroê°€ ë‘ ë‹¬ ë™ì•ˆ ë³„ê±°í•˜ë‹¤ê°€ ê²°...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>#Person1# ì€ Brianì—ê²Œ ìƒì¼ íŒŒí‹°ì—ì„œ í•¨ê»˜ ì¶¤ì„ ì¶”ìê³  ì´ˆëŒ€í•©ë‹ˆë‹¤.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>#Person1# ê³¼ #Person2# ëŠ” ì˜¬ë¦¼í”½ ê³µì›ì˜ í¬ê¸°ì™€ ì‹œì„¤ì— ëŒ€í•´ ì´ì•¼ê¸°...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>test_495</td>\n",
       "      <td>Jackì€ Charlieì—ê²Œ í•™êµ í›„ ì§‘ì—ì„œ ê·¸ë…€ì™€ ë¹„ë””ì˜¤ ê²Œì„ì„ í•˜ìê³  ì œì•ˆí•˜ê³ ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>test_496</td>\n",
       "      <td>#Person2# ëŠ” ì‹œê³¨ ìŒì•…ì— ê´€ì‹¬ì„ ê°€ì§€ê²Œ ëœ ê³„ê¸°ì™€ ë¼ë””ì˜¤ ë°©ì†¡êµ­ì—ì„œ ì¼í•˜ê²Œ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>test_497</td>\n",
       "      <td>AliceëŠ” #Person1# ì—ê²Œ ì„¸íƒê¸°ì— ë¹„ëˆ„ê°€ ë“¤ì–´ ìˆì§€ ì•Šì•„ ë”°ë¡œ ë„£ì–´ì•¼ í•œ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>test_498</td>\n",
       "      <td>MatthewëŠ” ì„ëŒ€ ê³„ì•½ì„ ê°±ì‹ í•˜ê³  ì‹¶ì§€ ì•Šì•„ ì§‘ì„ ì°¾ê³  ìˆìœ¼ë©°, Mrs. Tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>test_499</td>\n",
       "      <td>FrankëŠ” Betsyì—ê²Œ ìŠ¹ì§„ íŒŒí‹°ì— ì°¸ì„í•  150ëª…ì„ ì´ˆëŒ€í–ˆë‹¤ê³  ì•Œë¦½ë‹ˆë‹¤. íŒŒí‹°...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        fname                                            summary\n",
       "0      test_0  #Person1# ì€ Ms. Dawsonì—ê²Œ ì´ë©”ì¼ê³¼ ê³µì‹ ë©”ëª¨ë¡œë§Œ ì œí•œë˜ëŠ” ì‚¬ë‚´ ...\n",
       "1      test_1  #Person1# ê³¼ #Person2# ëŠ” êµí†µì²´ì¦ìœ¼ë¡œ ì¸í•´ ì¶œí‡´ê·¼ì— ì–´ë ¤ì›€ì„ ê²ªê³ ...\n",
       "2      test_2  #Person1# ì€ Kateì—ê²Œ Mashaì™€ Heroê°€ ë‘ ë‹¬ ë™ì•ˆ ë³„ê±°í•˜ë‹¤ê°€ ê²°...\n",
       "3      test_3       #Person1# ì€ Brianì—ê²Œ ìƒì¼ íŒŒí‹°ì—ì„œ í•¨ê»˜ ì¶¤ì„ ì¶”ìê³  ì´ˆëŒ€í•©ë‹ˆë‹¤.\n",
       "4      test_4  #Person1# ê³¼ #Person2# ëŠ” ì˜¬ë¦¼í”½ ê³µì›ì˜ í¬ê¸°ì™€ ì‹œì„¤ì— ëŒ€í•´ ì´ì•¼ê¸°...\n",
       "..        ...                                                ...\n",
       "494  test_495  Jackì€ Charlieì—ê²Œ í•™êµ í›„ ì§‘ì—ì„œ ê·¸ë…€ì™€ ë¹„ë””ì˜¤ ê²Œì„ì„ í•˜ìê³  ì œì•ˆí•˜ê³ ,...\n",
       "495  test_496  #Person2# ëŠ” ì‹œê³¨ ìŒì•…ì— ê´€ì‹¬ì„ ê°€ì§€ê²Œ ëœ ê³„ê¸°ì™€ ë¼ë””ì˜¤ ë°©ì†¡êµ­ì—ì„œ ì¼í•˜ê²Œ...\n",
       "496  test_497  AliceëŠ” #Person1# ì—ê²Œ ì„¸íƒê¸°ì— ë¹„ëˆ„ê°€ ë“¤ì–´ ìˆì§€ ì•Šì•„ ë”°ë¡œ ë„£ì–´ì•¼ í•œ...\n",
       "497  test_498  MatthewëŠ” ì„ëŒ€ ê³„ì•½ì„ ê°±ì‹ í•˜ê³  ì‹¶ì§€ ì•Šì•„ ì§‘ì„ ì°¾ê³  ìˆìœ¼ë©°, Mrs. Tho...\n",
       "498  test_499  FrankëŠ” Betsyì—ê²Œ ìŠ¹ì§„ íŒŒí‹°ì— ì°¸ì„í•  150ëª…ì„ ì´ˆëŒ€í–ˆë‹¤ê³  ì•Œë¦½ë‹ˆë‹¤. íŒŒí‹°...\n",
       "\n",
       "[499 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output  # ê° ëŒ€í™”ë¬¸ì— ëŒ€í•œ ìš”ì•½ë¬¸ì´ ì¶œë ¥ë¨ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘                    ğŸ¯ ë¦¬ë”ë³´ë“œ ì ìˆ˜ í–¥ìƒ 4ë‹¨ê³„ ì „ëµ                           â•‘\n",
      "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
      "â•‘ Phase 1: Quick Wins (ì¦‰ì‹œ ì ìš©)                     ì˜ˆìƒ í–¥ìƒ: +0.5~1.5ì     â•‘\n",
      "â•‘   â””â”€ í˜•íƒœì†Œ ê¸°ë°˜ ROUGE í‰ê°€, ë„ì–´ì“°ê¸° êµì •, N-Best Reranking                 â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•‘ Phase 2: Hyperparameter Optimization               ì˜ˆìƒ í–¥ìƒ: +0.3~0.8ì     â•‘\n",
      "â•‘   â””â”€ Grid Search (num_beams, length_penalty, repetition_penalty)            â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•‘ Phase 3: Model Ensemble                            ì˜ˆìƒ í–¥ìƒ: +0.5~1.0ì     â•‘\n",
      "â•‘   â””â”€ digit82 + gogamza ëª¨ë¸ ì•™ìƒë¸”                                          â•‘\n",
      "â•‘                                                                              â•‘# ============================================================================\n",
      "# ğŸ“¦ Phase 1: Quick Wins - í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ìœ í‹¸ë¦¬í‹°\n",
      "# ============================================================================\n",
      "\n",
      "import numpy as np\n",
      "import random\n",
      "from kiwipiepy import Kiwi\n",
      "from rouge import Rouge\n",
      "â•‘ Phase 4: Advanced Techniques                       ì˜ˆìƒ í–¥ìƒ: +0.5~1.5ì     â•‘\n",
      "â•‘   â””â”€ Knowledge Distillation, Back-translation Augmentation                  â•‘\n",
      "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
      "â•‘ ì˜ˆìƒ ìµœì¢… ì ìˆ˜: 49.5~51.0ì                                                   â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸš€ ë¦¬ë”ë³´ë“œ ì ìˆ˜ í–¥ìƒ ì¢…í•© ì „ëµ\n",
    "# ============================================================================\n",
    "# í˜„ì¬ ëª¨ë¸: digit82/kobart-summarization (ë¦¬ë”ë³´ë“œ 48.32ì  ë‹¬ì„± ê°€ëŠ¥)\n",
    "# ëª©í‘œ: ì²´ê³„ì ì¸ ë‹¨ê³„ë³„ ìµœì í™”ë¡œ ì ìˆ˜ ê·¹ëŒ€í™”\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    ğŸ¯ ë¦¬ë”ë³´ë“œ ì ìˆ˜ í–¥ìƒ 4ë‹¨ê³„ ì „ëµ                           â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘ Phase 1: Quick Wins (ì¦‰ì‹œ ì ìš©)                     ì˜ˆìƒ í–¥ìƒ: +0.5~1.5ì     â•‘\n",
    "â•‘   â””â”€ í˜•íƒœì†Œ ê¸°ë°˜ ROUGE í‰ê°€, ë„ì–´ì“°ê¸° êµì •, N-Best Reranking                 â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘ Phase 2: Hyperparameter Optimization               ì˜ˆìƒ í–¥ìƒ: +0.3~0.8ì     â•‘\n",
    "â•‘   â””â”€ Grid Search (num_beams, length_penalty, repetition_penalty)            â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘ Phase 3: Model Ensemble                            ì˜ˆìƒ í–¥ìƒ: +0.5~1.0ì     â•‘\n",
    "â•‘   â””â”€ digit82 + gogamza ëª¨ë¸ ì•™ìƒë¸”                                          â•‘\n",
    "â•‘                                                                            â•‘\n",
    "â•‘ Phase 4: Advanced Techniques                       ì˜ˆìƒ í–¥ìƒ: +0.5~1.5ì     â•‘\n",
    "â•‘   â””â”€ Knowledge Distillation, Back-translation Augmentation                  â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘ ì˜ˆìƒ ìµœì¢… ì ìˆ˜: 49.5~51.0ì                                                   â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Phase 1 ìœ í‹¸ë¦¬í‹° ë¡œë“œ ì™„ë£Œ\n",
      "   - Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
      "   - í˜•íƒœì†Œ ê¸°ë°˜ ROUGE í•¨ìˆ˜\n",
      "   - ë„ì–´ì“°ê¸° êµì • í•¨ìˆ˜\n",
      "   - âš ï¸ í™”ì íƒœê·¸ ë³´ì¡´ ë””ì½”ë”© í•¨ìˆ˜\n",
      "   - ğŸ”§ íŠ¸ë ì¼€ì´ì…˜ ì „ëµ í•¨ìˆ˜ë“¤:\n",
      "       â€¢ truncate_dialogue_tail (ë’·ë¶€ë¶„ ë³´ì¡´)\n",
      "       â€¢ truncate_dialogue_hybrid (ì•30%+ë’¤70%)\n",
      "       â€¢ truncate_dialogue_tokens (í† í° ê¸°ë°˜)\n",
      "       â€¢ get_dynamic_max_length (ë™ì  ê¸¸ì´)\n",
      "   - ì œê±° í† í°: ['<usr>', '<s>', '</s>', '<pad>']\n",
      "   - ë³´ì¡´ í† í°: #Person1#, #Person2#, ... (í™”ì íƒœê·¸)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“¦ Phase 1: Quick Wins - í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ìœ í‹¸ë¦¬í‹°\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from kiwipiepy import Kiwi\n",
    "from rouge import Rouge\n",
    "\n",
    "# Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "kiwi = Kiwi()\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "# âš ï¸ ì¤‘ìš”: ì œê±°í•  í† í° (í™”ì íƒœê·¸ëŠ” ì œì™¸!)\n",
    "# í™”ì íƒœê·¸ (#Person1#, #Person2# ë“±)ëŠ” ë³´ì¡´í•´ì•¼ í•¨\n",
    "REMOVE_TOKENS = ['<usr>', '<s>', '</s>', '<pad>']\n",
    "\n",
    "# Seed ê³ ì • í•¨ìˆ˜\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# âš ï¸ ìˆ˜ì •ëœ ë””ì½”ë”© í•¨ìˆ˜: í™”ì íƒœê·¸ ë³´ì¡´!\n",
    "def decode_with_speaker_tags(tokenizer, output_ids, remove_tokens=None):\n",
    "    \"\"\"\n",
    "    í™”ì íƒœê·¸(#Person1#, #Person2# ë“±)ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ë””ì½”ë”©\n",
    "    \"\"\"\n",
    "    if remove_tokens is None:\n",
    "        remove_tokens = REMOVE_TOKENS\n",
    "    \n",
    "    text = tokenizer.decode(output_ids, skip_special_tokens=False)\n",
    "    for token in remove_tokens:\n",
    "        text = text.replace(token, \" \")\n",
    "    text = ' '.join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "# í˜•íƒœì†Œ í† í°í™” í•¨ìˆ˜\n",
    "def morpheme_tokenize(text):\n",
    "    \"\"\"Kiwi í˜•íƒœì†Œ ë¶„ì„\"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return \"\"\n",
    "    tokens = kiwi.tokenize(text.strip())\n",
    "    return \" \".join([t.form for t in tokens])\n",
    "\n",
    "# í˜•íƒœì†Œ ê¸°ë°˜ ROUGE ê³„ì‚°\n",
    "def compute_morpheme_rouge(pred, ref):\n",
    "    \"\"\"í˜•íƒœì†Œ ê¸°ë°˜ ROUGE-L F1 ê³„ì‚°\"\"\"\n",
    "    pred_morph = morpheme_tokenize(pred)\n",
    "    ref_morph = morpheme_tokenize(ref)\n",
    "    \n",
    "    if not pred_morph or not ref_morph:\n",
    "        return {'rouge-1': 0.0, 'rouge-2': 0.0, 'rouge-l': 0.0}\n",
    "    \n",
    "    try:\n",
    "        scores = rouge_scorer.get_scores(pred_morph, ref_morph)\n",
    "        return {\n",
    "            'rouge-1': scores[0]['rouge-1']['f'],\n",
    "            'rouge-2': scores[0]['rouge-2']['f'],\n",
    "            'rouge-l': scores[0]['rouge-l']['f']\n",
    "        }\n",
    "    except:\n",
    "        return {'rouge-1': 0.0, 'rouge-2': 0.0, 'rouge-l': 0.0}\n",
    "\n",
    "# ë„ì–´ì“°ê¸° êµì • í•¨ìˆ˜\n",
    "def postprocess_spacing(text):\n",
    "    \"\"\"Kiwië¥¼ ì´ìš©í•œ ë„ì–´ì“°ê¸° êµì •\"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return text\n",
    "    return kiwi.space(text, reset_whitespace=False)\n",
    "\n",
    "# í›„ì²˜ë¦¬ í•¨ìˆ˜ (í™”ì íƒœê·¸ ë³´ì¡´)\n",
    "def postprocess_summary(text):\n",
    "    \"\"\"ìš”ì•½ë¬¸ í›„ì²˜ë¦¬ (í™”ì íƒœê·¸ ë³´ì¡´)\"\"\"\n",
    "    text = text.strip()\n",
    "    text = postprocess_spacing(text)\n",
    "    text = ' '.join(text.split())\n",
    "    text = text.replace(' .', '.').replace(' ,', ',').replace(' ?', '?').replace(' !', '!')\n",
    "    return text\n",
    "\n",
    "# ==============================================================================\n",
    "# ğŸ”§ ê¸¸ì´ ë¶„ì„ ê¸°ë°˜ íŠ¸ë ì¼€ì´ì…˜ ì „ëµ\n",
    "# ==============================================================================\n",
    "\n",
    "# âœ¨ ì „ëµ 1: ë’¤ì—ì„œë¶€í„° ìë¥´ê¸° (í›„ë°˜ë¶€ ê²°ë¡  ë³´ì¡´)\n",
    "def truncate_dialogue_tail(text, max_char=1500):\n",
    "    \"\"\"ê¸´ ëŒ€í™”ëŠ” ë’·ë¶€ë¶„(ê²°ë¡ )ì„ ìš°ì„  ë³´ì¡´í•˜ì—¬ ìë¦„\"\"\"\n",
    "    if len(text) <= max_char:\n",
    "        return text\n",
    "    return text[-max_char:]\n",
    "\n",
    "# âœ¨ ì „ëµ 2: ì•30% + ë’¤70% ë³´ì¡´ (ë„ì…ë¶€ + ê²°ë¡  ëª¨ë‘ ì‚´ë¦¼)\n",
    "def truncate_dialogue_hybrid(text, max_char=1500, front_ratio=0.3):\n",
    "    \"\"\"ì•ë¶€ë¶„ 30% + ë’·ë¶€ë¶„ 70% ë³´ì¡´ (ë„ì… + ê²°ë¡  ë‘˜ ë‹¤ ì‚´ë¦¼)\"\"\"\n",
    "    if len(text) <= max_char:\n",
    "        return text\n",
    "    front_chars = int(max_char * front_ratio)\n",
    "    back_chars = max_char - front_chars\n",
    "    return text[:front_chars] + \" ... \" + text[-back_chars:]\n",
    "\n",
    "# âœ¨ ì „ëµ 3: í† í° ê¸°ë°˜ íŠ¸ë ì¼€ì´ì…˜ (ì •í™•í•œ í† í° ìˆ˜ ì œì–´)\n",
    "def truncate_dialogue_tokens(text, tokenizer, max_tokens, strategy='tail'):\n",
    "    \"\"\"\n",
    "    í† í° ê¸°ë°˜ìœ¼ë¡œ íŠ¸ë ì¼€ì´ì…˜ (ë¬¸ì ê¸°ë°˜ë³´ë‹¤ ì •í™•)\n",
    "    strategy: 'tail' (ë’·ë¶€ë¶„ ë³´ì¡´), 'head' (ì•ë¶€ë¶„ ë³´ì¡´), 'hybrid' (ì•30%+ë’¤70%)\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    if len(token_ids) <= max_tokens:\n",
    "        return text\n",
    "    \n",
    "    if strategy == 'tail':\n",
    "        # ë’·ë¶€ë¶„ ë³´ì¡´ (ê²°ë¡  ì¤‘ì‹¬)\n",
    "        truncated_ids = token_ids[-max_tokens:]\n",
    "    elif strategy == 'head':\n",
    "        # ì•ë¶€ë¶„ ë³´ì¡´\n",
    "        truncated_ids = token_ids[:max_tokens]\n",
    "    elif strategy == 'hybrid':\n",
    "        # ì• 30% + ë’¤ 70%\n",
    "        front_tokens = int(max_tokens * 0.3)\n",
    "        back_tokens = max_tokens - front_tokens\n",
    "        truncated_ids = token_ids[:front_tokens] + token_ids[-back_tokens:]\n",
    "    else:\n",
    "        truncated_ids = token_ids[-max_tokens:]\n",
    "    \n",
    "    try:\n",
    "        return tokenizer.decode(truncated_ids, skip_special_tokens=True)\n",
    "    except:\n",
    "        return text[-max_tokens*3:]  # fallback: ë¬¸ì ê¸°ë°˜\n",
    "\n",
    "# âœ¨ ì „ëµ 4: ëŒ€í™” ê¸¸ì´ì— ë”°ë¥¸ ë™ì  max_length ì„¤ì •\n",
    "def get_dynamic_max_length(text_length, thresholds=None):\n",
    "    \"\"\"\n",
    "    ëŒ€í™” ê¸¸ì´ì— ë”°ë¼ ë™ì ìœ¼ë¡œ encoder_max_len ê²°ì •\n",
    "    ì§§ì€ ëŒ€í™”: 256 í† í°, ê¸´ ëŒ€í™”: 512 í† í°\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = {\n",
    "            500: 256,    # 500ì ì´í•˜ â†’ 256 í† í°\n",
    "            1000: 384,   # 1000ì ì´í•˜ â†’ 384 í† í°\n",
    "            2000: 512,   # 2000ì ì´í•˜ â†’ 512 í† í°\n",
    "            float('inf'): 768  # ê·¸ ì´ìƒ â†’ 768 í† í°\n",
    "        }\n",
    "    \n",
    "    for char_limit, token_limit in sorted(thresholds.items()):\n",
    "        if text_length <= char_limit:\n",
    "            return token_limit\n",
    "    return 512  # default\n",
    "\n",
    "# âœ¨ í”„ë¡¬í”„íŠ¸ ì¶”ê°€ í•¨ìˆ˜\n",
    "def apply_summary_prompt(dialogue, prompt=None):\n",
    "    \"\"\"ìš”ì•½ ìœ ë„ í”„ë¡¬í”„íŠ¸ë¥¼ ëŒ€í™” ë’¤ì— ì¶”ê°€\"\"\"\n",
    "    if prompt is None:\n",
    "        return dialogue\n",
    "    return f\"{dialogue} {prompt}\"\n",
    "\n",
    "# ==============================================================================\n",
    "# ğŸ“Š EDA ê¸°ë°˜ ìµœì  ì„¤ì • (ì‹¤í—˜ ê²°ê³¼ ë°˜ì˜)\n",
    "# ==============================================================================\n",
    "\n",
    "# ê¶Œì¥ ì„¤ì • (EDA ê¸°ë°˜)\n",
    "LENGTH_CONFIG = {\n",
    "    # dialogue ì²˜ë¦¬\n",
    "    'truncation_strategy': 'tail',  # 'tail', 'head', 'hybrid' ì¤‘ ì„ íƒ\n",
    "    'max_dialogue_chars': 1500,     # ë¬¸ì ê¸°ë°˜ íŠ¸ë ì¼€ì´ì…˜ ê¸°ì¤€\n",
    "    'max_dialogue_tokens': 512,     # í† í° ê¸°ë°˜ íŠ¸ë ì¼€ì´ì…˜ ê¸°ì¤€\n",
    "    'use_dynamic_length': False,    # ë™ì  ê¸¸ì´ ì‚¬ìš© ì—¬ë¶€\n",
    "    \n",
    "    # summary ì²˜ë¦¬\n",
    "    'decoder_max_len': 100,         # ìš”ì•½ ìµœëŒ€ í† í° (95% ì»¤ë²„)\n",
    "    \n",
    "    # í”„ë¡¬í”„íŠ¸\n",
    "    'use_prompt': False,\n",
    "    'prompt_text': 'ìš”ì•½:'\n",
    "}\n",
    "\n",
    "print(\"âœ… Phase 1 ìœ í‹¸ë¦¬í‹° ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"   - Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”\")\n",
    "print(f\"   - í˜•íƒœì†Œ ê¸°ë°˜ ROUGE í•¨ìˆ˜\")\n",
    "print(f\"   - ë„ì–´ì“°ê¸° êµì • í•¨ìˆ˜\")\n",
    "print(f\"   - âš ï¸ í™”ì íƒœê·¸ ë³´ì¡´ ë””ì½”ë”© í•¨ìˆ˜\")\n",
    "print(f\"   - ğŸ”§ íŠ¸ë ì¼€ì´ì…˜ ì „ëµ í•¨ìˆ˜ë“¤:\")\n",
    "print(f\"       â€¢ truncate_dialogue_tail (ë’·ë¶€ë¶„ ë³´ì¡´)\")\n",
    "print(f\"       â€¢ truncate_dialogue_hybrid (ì•30%+ë’¤70%)\")\n",
    "print(f\"       â€¢ truncate_dialogue_tokens (í† í° ê¸°ë°˜)\")\n",
    "print(f\"       â€¢ get_dynamic_max_length (ë™ì  ê¸¸ì´)\")\n",
    "print(f\"   - ì œê±° í† í°: {REMOVE_TOKENS}\")\n",
    "print(f\"   - ë³´ì¡´ í† í°: #Person1#, #Person2#, ... (í™”ì íƒœê·¸)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¯ Phase 1-A: í˜„ì¬ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (í˜•íƒœì†Œ ROUGE)\n",
      "================================================================================\n",
      "Device: cuda:0\n",
      "ğŸ“¥ ëª¨ë¸ ë¡œë“œ: ./prediction_kobart_v1/checkpoint-4674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
      "ğŸ“Š Dev ë°ì´í„°: 499ê°œ\n",
      "\n",
      "ğŸ”¬ ê¸°ë³¸ ì„±ëŠ¥ í‰ê°€ (í›„ì²˜ë¦¬ ì—†ì´)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í‰ê°€: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [01:21<00:00,  6.16it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ê¸°ë³¸ ì„±ëŠ¥ (í˜•íƒœì†Œ ROUGE):\n",
      "   ROUGE-1: 41.05\n",
      "   ROUGE-2: 16.18\n",
      "   ROUGE-L: 35.61\n",
      "   í‰ê·  ê¸¸ì´: 73.3ì\n",
      "\n",
      "ğŸ¯ ì˜ˆìƒ ë¦¬ë”ë³´ë“œ ì ìˆ˜: 47.00\n",
      "\n",
      "ğŸ“‹ ì˜ˆì¸¡ ìƒ˜í”Œ (í™”ì íƒœê·¸ í™•ì¸):\n",
      "\n",
      "[Sample 0]\n",
      "  Pred: #Person1# ì€ ìµœê·¼ì— ê°ê¸°ì— ê±¸ë ¸ì§€ë§Œ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ì‹œê¸¸ ê¶Œì¥í•©ë‹ˆë‹¤....\n",
      "  Gold: #Person2#ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2#ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤....\n",
      "\n",
      "[Sample 1]\n",
      "  Pred: #Person1# ê³¼ ì•¼ JimmyëŠ” 3ì‹œ 30ë¶„ì— ì²´ìœ¡ê´€ì—ì„œ ìš´ë™í•˜ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤....\n",
      "  Gold: #Person1#ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤....\n",
      "\n",
      "[Sample 2]\n",
      "  Pred: #Person1# ê³¼ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ëŠê³  ê±´ê°•ì„ ìœ ì§€í•˜ë ¤ê³  í•©ë‹ˆë‹¤....\n",
      "  Gold: #Person1#ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2#ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1#ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤....\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ¯ Phase 1-A: Dev ë°ì´í„°ë¡œ í˜„ì¬ ì„±ëŠ¥ í‰ê°€ (í˜•íƒœì†Œ ROUGE)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ¯ Phase 1-A: í˜„ì¬ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (í˜•íƒœì†Œ ROUGE)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ (checkpoint ê²½ë¡œ ì„¤ì • í•„ìš”)\n",
    "model_path = loaded_config['inference']['ckt_path']\n",
    "print(f\"ğŸ“¥ ëª¨ë¸ ë¡œë“œ: {model_path}\")\n",
    "\n",
    "model_name = loaded_config['general']['model_name']\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "eval_tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': loaded_config['tokenizer']['special_tokens']\n",
    "})\n",
    "\n",
    "eval_model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "eval_model.resize_token_embeddings(len(eval_tokenizer))\n",
    "eval_model = eval_model.to(device)\n",
    "eval_model.eval()\n",
    "\n",
    "print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# Dev ë°ì´í„° ë¡œë“œ\n",
    "dev_df = pd.read_csv(\"./data/dev.csv\")\n",
    "print(f\"ğŸ“Š Dev ë°ì´í„°: {len(dev_df)}ê°œ\")\n",
    "\n",
    "# ê¸°ë³¸ ì¶”ë¡  ì„¤ì •\n",
    "BASE_GEN_CONFIG = {\n",
    "    \"num_beams\": loaded_config['inference']['num_beams'],\n",
    "    \"no_repeat_ngram_size\": loaded_config['inference']['no_repeat_ngram_size'],\n",
    "    \"max_length\": loaded_config['inference']['generate_max_length'],\n",
    "    \"early_stopping\": loaded_config['inference']['early_stopping'],\n",
    "}\n",
    "\n",
    "def evaluate_on_dev(model, tokenizer, df, gen_config, apply_postprocess=False):\n",
    "    \"\"\"Dev ë°ì´í„°ë¡œ ëª¨ë¸ í‰ê°€ (í™”ì íƒœê·¸ ë³´ì¡´)\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"í‰ê°€\"):\n",
    "        inputs = tokenizer(\n",
    "            row['dialogue'],\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=loaded_config['tokenizer']['encoder_max_len'],\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                **gen_config\n",
    "            )\n",
    "        \n",
    "        # âš ï¸ ìˆ˜ì •: í™”ì íƒœê·¸ ë³´ì¡´í•˜ë©´ì„œ ë””ì½”ë”©\n",
    "        pred = decode_with_speaker_tags(tokenizer, outputs[0])\n",
    "        \n",
    "        # í›„ì²˜ë¦¬ ì ìš© (ë„ì–´ì“°ê¸° êµì • ë“±)\n",
    "        if apply_postprocess:\n",
    "            pred = postprocess_summary(pred)\n",
    "        \n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # í˜•íƒœì†Œ ROUGE ê³„ì‚°\n",
    "    rouge1_scores, rouge2_scores, rougel_scores = [], [], []\n",
    "    \n",
    "    for pred, ref in zip(predictions, df['summary'].tolist()):\n",
    "        scores = compute_morpheme_rouge(pred, ref)\n",
    "        rouge1_scores.append(scores['rouge-1'])\n",
    "        rouge2_scores.append(scores['rouge-2'])\n",
    "        rougel_scores.append(scores['rouge-l'])\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'rouge-1': np.mean(rouge1_scores) * 100,\n",
    "        'rouge-2': np.mean(rouge2_scores) * 100,\n",
    "        'rouge-l': np.mean(rougel_scores) * 100,\n",
    "        'avg_len': np.mean([len(p) for p in predictions])\n",
    "    }\n",
    "\n",
    "# ê¸°ë³¸ ì„±ëŠ¥ í‰ê°€ (í›„ì²˜ë¦¬ ì—†ì´)\n",
    "print(\"\\nğŸ”¬ ê¸°ë³¸ ì„±ëŠ¥ í‰ê°€ (í›„ì²˜ë¦¬ ì—†ì´)...\")\n",
    "base_results = evaluate_on_dev(eval_model, eval_tokenizer, dev_df, BASE_GEN_CONFIG, apply_postprocess=False)\n",
    "\n",
    "print(f\"\\nğŸ“Š ê¸°ë³¸ ì„±ëŠ¥ (í˜•íƒœì†Œ ROUGE):\")\n",
    "print(f\"   ROUGE-1: {base_results['rouge-1']:.2f}\")\n",
    "print(f\"   ROUGE-2: {base_results['rouge-2']:.2f}\")\n",
    "print(f\"   ROUGE-L: {base_results['rouge-l']:.2f}\")\n",
    "print(f\"   í‰ê·  ê¸¸ì´: {base_results['avg_len']:.1f}ì\")\n",
    "\n",
    "# ì˜ˆìƒ ë¦¬ë”ë³´ë“œ ì ìˆ˜ (í˜•íƒœì†Œ ROUGE-L Ã— 1.32)\n",
    "estimated_leaderboard = base_results['rouge-l'] * 1.32\n",
    "print(f\"\\nğŸ¯ ì˜ˆìƒ ë¦¬ë”ë³´ë“œ ì ìˆ˜: {estimated_leaderboard:.2f}\")\n",
    "\n",
    "# ìƒ˜í”Œ ì¶œë ¥ (í™”ì íƒœê·¸ í™•ì¸)\n",
    "print(\"\\nğŸ“‹ ì˜ˆì¸¡ ìƒ˜í”Œ (í™”ì íƒœê·¸ í™•ì¸):\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n[Sample {i}]\")\n",
    "    print(f\"  Pred: {base_results['predictions'][i][:100]}...\")\n",
    "    print(f\"  Gold: {dev_df['summary'].iloc[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¯ Phase 1-B: ë„ì–´ì“°ê¸° êµì • í›„ì²˜ë¦¬ íš¨ê³¼ ì¸¡ì •\n",
      "================================================================================\n",
      "\n",
      "ğŸ”¬ í›„ì²˜ë¦¬ ì ìš© ì„±ëŠ¥ í‰ê°€...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í‰ê°€: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [01:21<00:00,  6.15it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š í›„ì²˜ë¦¬ ì ìš© í›„ ì„±ëŠ¥ (í˜•íƒœì†Œ ROUGE):\n",
      "   ROUGE-1: 41.12 (+0.07)\n",
      "   ROUGE-2: 16.27 (+0.09)\n",
      "   ROUGE-L: 35.67 (+0.06)\n",
      "   í‰ê·  ê¸¸ì´: 73.6ì\n",
      "\n",
      "ğŸ¯ ì˜ˆìƒ ë¦¬ë”ë³´ë“œ ì ìˆ˜: 47.08 (+0.08)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ¯ Phase 1-B: í›„ì²˜ë¦¬ ì ìš© íš¨ê³¼ ì¸¡ì •\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ¯ Phase 1-B: ë„ì–´ì“°ê¸° êµì • í›„ì²˜ë¦¬ íš¨ê³¼ ì¸¡ì •\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# í›„ì²˜ë¦¬ ì ìš© í‰ê°€\n",
    "print(\"\\nğŸ”¬ í›„ì²˜ë¦¬ ì ìš© ì„±ëŠ¥ í‰ê°€...\")\n",
    "postproc_results = evaluate_on_dev(eval_model, eval_tokenizer, dev_df, BASE_GEN_CONFIG, apply_postprocess=True)\n",
    "\n",
    "print(f\"\\nğŸ“Š í›„ì²˜ë¦¬ ì ìš© í›„ ì„±ëŠ¥ (í˜•íƒœì†Œ ROUGE):\")\n",
    "print(f\"   ROUGE-1: {postproc_results['rouge-1']:.2f} ({postproc_results['rouge-1'] - base_results['rouge-1']:+.2f})\")\n",
    "print(f\"   ROUGE-2: {postproc_results['rouge-2']:.2f} ({postproc_results['rouge-2'] - base_results['rouge-2']:+.2f})\")\n",
    "print(f\"   ROUGE-L: {postproc_results['rouge-l']:.2f} ({postproc_results['rouge-l'] - base_results['rouge-l']:+.2f})\")\n",
    "print(f\"   í‰ê·  ê¸¸ì´: {postproc_results['avg_len']:.1f}ì\")\n",
    "\n",
    "# ì˜ˆìƒ ë¦¬ë”ë³´ë“œ ì ìˆ˜\n",
    "estimated_leaderboard_pp = postproc_results['rouge-l'] * 1.32\n",
    "print(f\"\\nğŸ¯ ì˜ˆìƒ ë¦¬ë”ë³´ë“œ ì ìˆ˜: {estimated_leaderboard_pp:.2f} ({estimated_leaderboard_pp - estimated_leaderboard:+.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ”¬ Phase 1-C: íŠ¸ë ì¼€ì´ì…˜ ì „ëµ ë¹„êµ ì‹¤í—˜\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Train ë°ì´í„° ê¸¸ì´ ë¶„í¬:\n",
      "  Dialogue: mean=406, 75%=500, max=2165\n",
      "  Summary:  mean=86, 75%=104, 95%=149\n",
      "\n",
      "ğŸ“Š Dialogue í† í° ìˆ˜ ë¶„í¬ (ìƒ˜í”Œ 200ê°œ):\n",
      "  mean=157, 75%=190, max=777\n",
      "\n",
      "================================================================================\n",
      "ğŸ§ª íŠ¸ë ì¼€ì´ì…˜ ì „ëµë³„ Dev ROUGE ë¹„êµ (50ê°œ ìƒ˜í”Œ)\n",
      "================================================================================\n",
      "\n",
      "ğŸ”§ Testing: no_trunc (strategy=None, max_tokens=1024)\n",
      "\n",
      "ğŸ“Š Dialogue í† í° ìˆ˜ ë¶„í¬ (ìƒ˜í”Œ 200ê°œ):\n",
      "  mean=157, 75%=190, max=777\n",
      "\n",
      "================================================================================\n",
      "ğŸ§ª íŠ¸ë ì¼€ì´ì…˜ ì „ëµë³„ Dev ROUGE ë¹„êµ (50ê°œ ìƒ˜í”Œ)\n",
      "================================================================================\n",
      "\n",
      "ğŸ”§ Testing: no_trunc (strategy=None, max_tokens=1024)\n",
      "   ROUGE-L: 33.72, ì˜ˆìƒ LB: 44.51\n",
      "\n",
      "ğŸ”§ Testing: tail_512 (strategy=tail, max_tokens=512)\n",
      "   ROUGE-L: 33.72, ì˜ˆìƒ LB: 44.51\n",
      "\n",
      "ğŸ”§ Testing: tail_512 (strategy=tail, max_tokens=512)\n",
      "   ROUGE-L: 33.72, ì˜ˆìƒ LB: 44.51\n",
      "\n",
      "ğŸ”§ Testing: tail_384 (strategy=tail, max_tokens=384)\n",
      "   ROUGE-L: 33.72, ì˜ˆìƒ LB: 44.51\n",
      "\n",
      "ğŸ”§ Testing: tail_384 (strategy=tail, max_tokens=384)\n",
      "   ROUGE-L: 33.72, ì˜ˆìƒ LB: 44.51\n",
      "\n",
      "ğŸ”§ Testing: hybrid_512 (strategy=hybrid, max_tokens=512)\n",
      "   ROUGE-L: 33.72, ì˜ˆìƒ LB: 44.51\n",
      "\n",
      "ğŸ”§ Testing: hybrid_512 (strategy=hybrid, max_tokens=512)\n",
      "   ROUGE-L: 33.72, ì˜ˆìƒ LB: 44.51\n",
      "\n",
      "ğŸ”§ Testing: head_512 (strategy=head, max_tokens=512)\n",
      "   ROUGE-L: 33.72, ì˜ˆìƒ LB: 44.51\n",
      "\n",
      "ğŸ”§ Testing: head_512 (strategy=head, max_tokens=512)\n",
      "   ROUGE-L: 33.72, ì˜ˆìƒ LB: 44.51\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š íŠ¸ë ì¼€ì´ì…˜ ì „ëµ ë¹„êµ ê²°ê³¼ (ROUGE-L ìˆœ)\n",
      "================================================================================\n",
      "ğŸ† 1. no_trunc        | ROUGE-L: 33.72 | ì˜ˆìƒ LB: 44.51\n",
      "   2. tail_512        | ROUGE-L: 33.72 | ì˜ˆìƒ LB: 44.51\n",
      "   3. tail_384        | ROUGE-L: 33.72 | ì˜ˆìƒ LB: 44.51\n",
      "   4. hybrid_512      | ROUGE-L: 33.72 | ì˜ˆìƒ LB: 44.51\n",
      "   5. head_512        | ROUGE-L: 33.72 | ì˜ˆìƒ LB: 44.51\n",
      "\n",
      "âœ… ìµœì  ì „ëµ: no_trunc (strategy=None, max_tokens=1024)\n",
      "   ROUGE-L: 33.72, ì˜ˆìƒ LB: 44.51\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š íŠ¸ë ì¼€ì´ì…˜ ì „ëµ ë¹„êµ ê²°ê³¼ (ROUGE-L ìˆœ)\n",
      "================================================================================\n",
      "ğŸ† 1. no_trunc        | ROUGE-L: 33.72 | ì˜ˆìƒ LB: 44.51\n",
      "   2. tail_512        | ROUGE-L: 33.72 | ì˜ˆìƒ LB: 44.51\n",
      "   3. tail_384        | ROUGE-L: 33.72 | ì˜ˆìƒ LB: 44.51\n",
      "   4. hybrid_512      | ROUGE-L: 33.72 | ì˜ˆìƒ LB: 44.51\n",
      "   5. head_512        | ROUGE-L: 33.72 | ì˜ˆìƒ LB: 44.51\n",
      "\n",
      "âœ… ìµœì  ì „ëµ: no_trunc (strategy=None, max_tokens=1024)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ”¬ Phase 1-C: íŠ¸ë ì¼€ì´ì…˜ ì „ëµ ë¹„êµ ì‹¤í—˜\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ”¬ Phase 1-C: íŠ¸ë ì¼€ì´ì…˜ ì „ëµ ë¹„êµ ì‹¤í—˜\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. ë¨¼ì € ë°ì´í„° ê¸¸ì´ ë¶„í¬ í™•ì¸\n",
    "train_df_for_eda = pd.read_csv(\"./data/train.csv\")\n",
    "dev_df_for_eda = pd.read_csv(\"./data/dev.csv\")\n",
    "\n",
    "print(\"\\nğŸ“Š Train ë°ì´í„° ê¸¸ì´ ë¶„í¬:\")\n",
    "train_dialog_len = train_df_for_eda['dialogue'].apply(len)\n",
    "train_summary_len = train_df_for_eda['summary'].apply(len)\n",
    "print(f\"  Dialogue: mean={train_dialog_len.mean():.0f}, 75%={train_dialog_len.quantile(0.75):.0f}, max={train_dialog_len.max()}\")\n",
    "print(f\"  Summary:  mean={train_summary_len.mean():.0f}, 75%={train_summary_len.quantile(0.75):.0f}, 95%={train_summary_len.quantile(0.95):.0f}\")\n",
    "\n",
    "# í† í° ìˆ˜ ë¶„í¬ (ìƒ˜í”Œë§)\n",
    "sample_dialogs = train_df_for_eda['dialogue'].sample(min(200, len(train_df_for_eda)), random_state=42)\n",
    "token_counts = [len(eval_tokenizer.encode(d, add_special_tokens=False)) for d in sample_dialogs]\n",
    "print(f\"\\nğŸ“Š Dialogue í† í° ìˆ˜ ë¶„í¬ (ìƒ˜í”Œ 200ê°œ):\")\n",
    "print(f\"  mean={np.mean(token_counts):.0f}, 75%={np.percentile(token_counts, 75):.0f}, max={np.max(token_counts)}\")\n",
    "\n",
    "# 2. íŠ¸ë ì¼€ì´ì…˜ ì „ëµë³„ Dev ì„±ëŠ¥ ë¹„êµ\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ§ª íŠ¸ë ì¼€ì´ì…˜ ì „ëµë³„ Dev ROUGE ë¹„êµ (50ê°œ ìƒ˜í”Œ)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "TRUNCATION_EXPERIMENTS = [\n",
    "    {'name': 'no_trunc', 'strategy': None, 'max_tokens': 1024},\n",
    "    {'name': 'tail_512', 'strategy': 'tail', 'max_tokens': 512},\n",
    "    {'name': 'tail_384', 'strategy': 'tail', 'max_tokens': 384},\n",
    "    {'name': 'hybrid_512', 'strategy': 'hybrid', 'max_tokens': 512},\n",
    "    {'name': 'head_512', 'strategy': 'head', 'max_tokens': 512},\n",
    "]\n",
    "\n",
    "def evaluate_truncation_strategy(model, tokenizer, df, strategy, max_tokens, sample_size=50):\n",
    "    \"\"\"íŠ¸ë ì¼€ì´ì…˜ ì „ëµë³„ í‰ê°€\"\"\"\n",
    "    sample_df = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
    "    predictions = []\n",
    "    \n",
    "    for idx, row in sample_df.iterrows():\n",
    "        dialogue = row['dialogue']\n",
    "        \n",
    "        # íŠ¸ë ì¼€ì´ì…˜ ì ìš©\n",
    "        if strategy is not None:\n",
    "            dialogue = truncate_dialogue_tokens(dialogue, tokenizer, max_tokens, strategy=strategy)\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            dialogue,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=max_tokens,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                **BASE_GEN_CONFIG\n",
    "            )\n",
    "        \n",
    "        pred = decode_with_speaker_tags(tokenizer, outputs[0])\n",
    "        pred = postprocess_summary(pred)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # ROUGE ê³„ì‚°\n",
    "    rougel_scores = []\n",
    "    for pred, ref in zip(predictions, sample_df['summary'].tolist()):\n",
    "        scores = compute_morpheme_rouge(pred, ref)\n",
    "        rougel_scores.append(scores['rouge-l'])\n",
    "    \n",
    "    return {\n",
    "        'rouge-l': np.mean(rougel_scores) * 100,\n",
    "        'predictions': predictions\n",
    "    }\n",
    "\n",
    "# ì‹¤í—˜ ì‹¤í–‰\n",
    "trunc_results = []\n",
    "for exp in TRUNCATION_EXPERIMENTS:\n",
    "    print(f\"\\nğŸ”§ Testing: {exp['name']} (strategy={exp['strategy']}, max_tokens={exp['max_tokens']})\")\n",
    "    result = evaluate_truncation_strategy(\n",
    "        eval_model, eval_tokenizer, dev_df,\n",
    "        strategy=exp['strategy'],\n",
    "        max_tokens=exp['max_tokens'],\n",
    "        sample_size=50\n",
    "    )\n",
    "    trunc_results.append({\n",
    "        'name': exp['name'],\n",
    "        'strategy': exp['strategy'],\n",
    "        'max_tokens': exp['max_tokens'],\n",
    "        'rouge-l': result['rouge-l'],\n",
    "        'estimated_lb': result['rouge-l'] * 1.32\n",
    "    })\n",
    "    print(f\"   ROUGE-L: {result['rouge-l']:.2f}, ì˜ˆìƒ LB: {result['rouge-l'] * 1.32:.2f}\")\n",
    "\n",
    "# ê²°ê³¼ ì •ë ¬\n",
    "trunc_results_sorted = sorted(trunc_results, key=lambda x: x['rouge-l'], reverse=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š íŠ¸ë ì¼€ì´ì…˜ ì „ëµ ë¹„êµ ê²°ê³¼ (ROUGE-L ìˆœ)\")\n",
    "print(\"=\" * 80)\n",
    "for i, r in enumerate(trunc_results_sorted):\n",
    "    marker = \"ğŸ†\" if i == 0 else \"  \"\n",
    "    print(f\"{marker} {i+1}. {r['name']:15} | ROUGE-L: {r['rouge-l']:.2f} | ì˜ˆìƒ LB: {r['estimated_lb']:.2f}\")\n",
    "\n",
    "BEST_TRUNCATION = trunc_results_sorted[0]\n",
    "print(f\"\\nâœ… ìµœì  ì „ëµ: {BEST_TRUNCATION['name']} (strategy={BEST_TRUNCATION['strategy']}, max_tokens={BEST_TRUNCATION['max_tokens']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“¤ Phase 1-D: ìµœì  íŠ¸ë ì¼€ì´ì…˜ ì „ëµ ì ìš© ì œì¶œ íŒŒì¼ ìƒì„±\n",
      "================================================================================\n",
      "ğŸ“Š Test ë°ì´í„°: 499ê°œ\n",
      "ğŸ“Œ ì ìš© ì „ëµ: no_trunc\n",
      "\n",
      "ğŸš€ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘... (strategy=None, max_tokens=1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì¶”ë¡ :   0%|          | 0/499 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì¶”ë¡ : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [01:23<00:00,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: ./prediction/submit_phase1d_truncation.csv\n",
      "   í‰ê·  ê¸¸ì´: 73.4ì\n",
      "   í™”ì íƒœê·¸ í¬í•¨: 360/499ê°œ (72.1%)\n",
      "\n",
      "ğŸ“‹ ì œì¶œ ìƒ˜í”Œ:\n",
      "\n",
      "[Test 0]\n",
      "  âŒ Ms. Dawsonì€ ì‚¬ë‚´ ë©”ëª¨ë¥¼ ì‘ì„±í•˜ê³  ë°°í¬í•˜ê¸° ìœ„í•´ ë¶€ì„œì¥ì—ê²Œ ìš”ì²­í•©ë‹ˆë‹¤....\n",
      "\n",
      "[Test 1]\n",
      "  âœ… #Person1# ê³¼ êµí†µ ì²´ì¦ìœ¼ë¡œ ì¸í•´ ëŒ€ì¤‘êµí†µìœ¼ë¡œ ì¶œí‡´ê·¼í•˜ëŠ” ê²ƒì´ ë” ë‚˜ì€ ì„ íƒì´ ë  ìˆ˜ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ë˜í•œ, ì¶œê·¼í•  ë•Œ ì°¨ë¥¼ íƒ€ì§€ ì•Šê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤....\n",
      "\n",
      "[Test 2]\n",
      "  âŒ KateëŠ” Mashaì™€ Heroê°€ ë‘ ë‹¬ ë™ì•ˆ ë³„ê±°í•˜ë‹¤ê°€ ê²°êµ­ ì´í˜¼ì„ í•˜ê²Œ ë˜ì—ˆë‹¤ê³  ì „í•˜ë©°, ê·¸ë“¤ì€ ì–‘ìœ¡ê¶Œì„ ê°€ì§„ë‹¤....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“¤ Phase 1-D: ìµœì  íŠ¸ë ì¼€ì´ì…˜ ì „ëµìœ¼ë¡œ ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“¤ Phase 1-D: ìµœì  íŠ¸ë ì¼€ì´ì…˜ ì „ëµ ì ìš© ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test ë°ì´í„° ë¡œë“œ (ì»¤ë„ ì¬ì‹œì‘ í›„ì—ë„ ë™ì‘í•˜ë„ë¡)\n",
    "test_df = pd.read_csv(os.path.join(data_path, 'test.csv'))\n",
    "print(f\"ğŸ“Š Test ë°ì´í„°: {len(test_df)}ê°œ\")\n",
    "\n",
    "def generate_submission_with_truncation(model, tokenizer, test_df, gen_config, \n",
    "                                        truncation_strategy, max_tokens, output_path):\n",
    "    \"\"\"íŠ¸ë ì¼€ì´ì…˜ ì „ëµì„ ì ìš©í•œ ì œì¶œ íŒŒì¼ ìƒì„±\"\"\"\n",
    "    summaries = []\n",
    "    \n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"ì¶”ë¡ \"):\n",
    "        dialogue = row['dialogue']\n",
    "        \n",
    "        # íŠ¸ë ì¼€ì´ì…˜ ì ìš©\n",
    "        if truncation_strategy is not None:\n",
    "            dialogue = truncate_dialogue_tokens(dialogue, tokenizer, max_tokens, strategy=truncation_strategy)\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            dialogue,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=max_tokens,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                **gen_config\n",
    "            )\n",
    "        \n",
    "        pred = decode_with_speaker_tags(tokenizer, outputs[0])\n",
    "        pred = postprocess_summary(pred)\n",
    "        summaries.append(pred)\n",
    "    \n",
    "    # ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "    submission = pd.DataFrame({\n",
    "        'fname': test_df['fname'],\n",
    "        'summary': summaries\n",
    "    })\n",
    "    \n",
    "    submission.to_csv(output_path, index=False)\n",
    "    \n",
    "    # í†µê³„ ì¶œë ¥\n",
    "    has_speaker = sum(1 for s in summaries if '#Person' in s)\n",
    "    print(f\"âœ… ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "    print(f\"   í‰ê·  ê¸¸ì´: {np.mean([len(s) for s in summaries]):.1f}ì\")\n",
    "    print(f\"   í™”ì íƒœê·¸ í¬í•¨: {has_speaker}/{len(summaries)}ê°œ ({has_speaker/len(summaries)*100:.1f}%)\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# ìµœì  ì „ëµ ì‚¬ìš© (ìœ„ ì‹¤í—˜ ê²°ê³¼ ê¸°ë°˜)\n",
    "# ì‹¤í—˜ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©\n",
    "try:\n",
    "    best_strategy = BEST_TRUNCATION['strategy']\n",
    "    best_max_tokens = BEST_TRUNCATION['max_tokens']\n",
    "    print(f\"ğŸ“Œ ì ìš© ì „ëµ: {BEST_TRUNCATION['name']}\")\n",
    "except:\n",
    "    best_strategy = 'tail'\n",
    "    best_max_tokens = 512\n",
    "    print(f\"ğŸ“Œ ê¸°ë³¸ ì „ëµ ì‚¬ìš©: tail, 512 tokens\")\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "print(f\"\\nğŸš€ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘... (strategy={best_strategy}, max_tokens={best_max_tokens})\")\n",
    "\n",
    "submission_trunc = generate_submission_with_truncation(\n",
    "    eval_model, eval_tokenizer, test_df, \n",
    "    BASE_GEN_CONFIG,\n",
    "    truncation_strategy=best_strategy,\n",
    "    max_tokens=best_max_tokens,\n",
    "    output_path=\"./prediction/submit_phase1d_truncation.csv\"\n",
    ")\n",
    "\n",
    "# ìƒ˜í”Œ í™•ì¸\n",
    "print(\"\\nğŸ“‹ ì œì¶œ ìƒ˜í”Œ:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n[Test {i}]\")\n",
    "    summary = submission_trunc['summary'].iloc[i]\n",
    "    has_tag = \"âœ…\" if \"#Person\" in summary else \"âŒ\"\n",
    "    print(f\"  {has_tag} {summary[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¯ Phase 2-A: Grid Search í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
      "================================================================================\n",
      "\n",
      "ğŸš€ Grid Search ì‹¤í–‰ ì¤‘ (100ê°œ ìƒ˜í”Œ)...\n",
      "\n",
      "ğŸ”§ Testing: baseline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "baseline: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:16<00:00,  6.16it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ROUGE-L: 34.43, ê¸¸ì´: 73.4, ì˜ˆìƒ LB: 45.44\n",
      "\n",
      "ğŸ”§ Testing: nb6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nb6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:16<00:00,  5.93it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ROUGE-L: 35.49, ê¸¸ì´: 72.5, ì˜ˆìƒ LB: 46.84\n",
      "\n",
      "ğŸ”§ Testing: nb8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nb8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:18<00:00,  5.36it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ROUGE-L: 35.61, ê¸¸ì´: 74.8, ì˜ˆìƒ LB: 47.00\n",
      "\n",
      "ğŸ”§ Testing: lp0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lp0.9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:17<00:00,  5.87it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ROUGE-L: 35.57, ê¸¸ì´: 71.2, ì˜ˆìƒ LB: 46.95\n",
      "\n",
      "ğŸ”§ Testing: lp1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lp1.1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:16<00:00,  5.97it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ROUGE-L: 35.45, ê¸¸ì´: 74.4, ì˜ˆìƒ LB: 46.80\n",
      "\n",
      "ğŸ”§ Testing: rp1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rp1.1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:16<00:00,  5.94it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ROUGE-L: 35.25, ê¸¸ì´: 71.5, ì˜ˆìƒ LB: 46.53\n",
      "\n",
      "ğŸ”§ Testing: rp1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rp1.2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:17<00:00,  5.86it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ROUGE-L: 35.53, ê¸¸ì´: 72.0, ì˜ˆìƒ LB: 46.90\n",
      "\n",
      "ğŸ”§ Testing: best_combo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "best_combo: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:17<00:00,  5.60it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ROUGE-L: 35.25, ê¸¸ì´: 72.1, ì˜ˆìƒ LB: 46.54\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š Grid Search ê²°ê³¼ (ROUGE-L ìˆœ)\n",
      "================================================================================\n",
      "1. nb8             | ROUGE-L: 35.61 | ì˜ˆìƒ LB: 47.00 | ê¸¸ì´: 74.8\n",
      "2. lp0.9           | ROUGE-L: 35.57 | ì˜ˆìƒ LB: 46.95 | ê¸¸ì´: 71.2\n",
      "3. rp1.2           | ROUGE-L: 35.53 | ì˜ˆìƒ LB: 46.90 | ê¸¸ì´: 72.0\n",
      "4. nb6             | ROUGE-L: 35.49 | ì˜ˆìƒ LB: 46.84 | ê¸¸ì´: 72.5\n",
      "5. lp1.1           | ROUGE-L: 35.45 | ì˜ˆìƒ LB: 46.80 | ê¸¸ì´: 74.4\n",
      "6. best_combo      | ROUGE-L: 35.25 | ì˜ˆìƒ LB: 46.54 | ê¸¸ì´: 72.1\n",
      "7. rp1.1           | ROUGE-L: 35.25 | ì˜ˆìƒ LB: 46.53 | ê¸¸ì´: 71.5\n",
      "8. baseline        | ROUGE-L: 34.43 | ì˜ˆìƒ LB: 45.44 | ê¸¸ì´: 73.4\n",
      "\n",
      "ğŸ† ìµœì  ì„¤ì •: nb8\n",
      "   Config: {'name': 'nb8', 'num_beams': 8, 'length_penalty': 1.0, 'repetition_penalty': 1.0}\n",
      "\n",
      "ğŸ“‹ í™”ì íƒœê·¸ í™•ì¸ (ìµœì  ì„¤ì • ìƒ˜í”Œ):\n",
      "  [0] #Person1# ì€ ê·¸ ì‚¬ëŒì„ ê±´ë“œë¦¬ë©´ ë˜‘ê°™ì€ ë‹¹í•  ê²ƒì´ë¼ë©° ê·¸ ì‚¬ëŒê³¼ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤....\n",
      "  [1] #Person1# ì€ ì§€ë‚œ ëª‡ ë‹¬ ë™ì•ˆ ê³„íšë˜ì§€ ì•Šì€ ì…§ë‹¤ìš´ì´ ë§ì•„ ì •ë¹„ íŒ€ì´ ì˜ˆë°© ì •ë¹„ë¥¼ í•  ì‹œê°„ì´ ë¶€ì¡±í•˜ë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ì •ë¹„ íŒ€ì€ ì˜ˆë°© ì •...\n",
      "  [2] Dr. CardanoëŠ” í•œë°¤ì¤‘ì— ì˜¤ë¥¸ìª½ ë°œì— ì‹¬í•œ í†µì¦ì´ ìƒê²¼ìŠµë‹ˆë‹¤. BobbyëŠ” ì–‘ë§ì„ ë‹¤ ë²—ê³  ì–‘ìª½ ë°œì„ ë¹„êµí•˜ë©°, í˜ˆì•¡ ê²€ì‚¬ë¥¼ ìœ„í•´ 3 ì¸µ...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ¯ Phase 2-A: Grid Search for í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ¯ Phase 2-A: Grid Search í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Grid Search ì„¤ì •\n",
    "GRID_SEARCH_CONFIGS = [\n",
    "    {\"name\": \"baseline\", \"num_beams\": 4, \"length_penalty\": 1.0, \"repetition_penalty\": 1.0},\n",
    "    {\"name\": \"nb6\", \"num_beams\": 6, \"length_penalty\": 1.0, \"repetition_penalty\": 1.0},\n",
    "    {\"name\": \"nb8\", \"num_beams\": 8, \"length_penalty\": 1.0, \"repetition_penalty\": 1.0},\n",
    "    {\"name\": \"lp0.9\", \"num_beams\": 6, \"length_penalty\": 0.9, \"repetition_penalty\": 1.0},\n",
    "    {\"name\": \"lp1.1\", \"num_beams\": 6, \"length_penalty\": 1.1, \"repetition_penalty\": 1.0},\n",
    "    {\"name\": \"rp1.1\", \"num_beams\": 6, \"length_penalty\": 1.0, \"repetition_penalty\": 1.1},\n",
    "    {\"name\": \"rp1.2\", \"num_beams\": 6, \"length_penalty\": 1.0, \"repetition_penalty\": 1.2},\n",
    "    {\"name\": \"best_combo\", \"num_beams\": 7, \"length_penalty\": 1.0, \"repetition_penalty\": 1.2},\n",
    "]\n",
    "\n",
    "def run_grid_search(model, tokenizer, df, configs, sample_size=100):\n",
    "    \"\"\"Grid Search ì‹¤í–‰ (ìƒ˜í”Œ ë°ì´í„°ë¡œ ë¹ ë¥¸ íƒìƒ‰)\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # ìƒ˜í”Œë§ (ë¹ ë¥¸ íƒìƒ‰)\n",
    "    sample_df = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"\\nğŸ”§ Testing: {config['name']}\")\n",
    "        \n",
    "        gen_config = {\n",
    "            \"num_beams\": config[\"num_beams\"],\n",
    "            \"length_penalty\": config.get(\"length_penalty\", 1.0),\n",
    "            \"repetition_penalty\": config.get(\"repetition_penalty\", 1.0),\n",
    "            \"no_repeat_ngram_size\": 3,\n",
    "            \"max_length\": 100,\n",
    "            \"early_stopping\": True,\n",
    "        }\n",
    "        \n",
    "        predictions = []\n",
    "        for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=config['name']):\n",
    "            inputs = tokenizer(\n",
    "                row['dialogue'],\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=loaded_config['tokenizer']['encoder_max_len'],\n",
    "                truncation=True,\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    input_ids=inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    **gen_config\n",
    "                )\n",
    "            \n",
    "            # âš ï¸ ìˆ˜ì •: í™”ì íƒœê·¸ ë³´ì¡´í•˜ë©´ì„œ ë””ì½”ë”©\n",
    "            pred = decode_with_speaker_tags(tokenizer, outputs[0])\n",
    "            pred = postprocess_summary(pred)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        # í˜•íƒœì†Œ ROUGE ê³„ì‚°\n",
    "        rougel_scores = []\n",
    "        for pred, ref in zip(predictions, sample_df['summary'].tolist()):\n",
    "            scores = compute_morpheme_rouge(pred, ref)\n",
    "            rougel_scores.append(scores['rouge-l'])\n",
    "        \n",
    "        avg_rouge = np.mean(rougel_scores) * 100\n",
    "        avg_len = np.mean([len(p) for p in predictions])\n",
    "        \n",
    "        results.append({\n",
    "            'name': config['name'],\n",
    "            'config': config,\n",
    "            'rouge-l': avg_rouge,\n",
    "            'avg_len': avg_len,\n",
    "            'estimated_lb': avg_rouge * 1.32,\n",
    "            'predictions': predictions  # í™”ì íƒœê·¸ í™•ì¸ìš©\n",
    "        })\n",
    "        \n",
    "        print(f\"   ROUGE-L: {avg_rouge:.2f}, ê¸¸ì´: {avg_len:.1f}, ì˜ˆìƒ LB: {avg_rouge * 1.32:.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Grid Search ì‹¤í–‰\n",
    "print(\"\\nğŸš€ Grid Search ì‹¤í–‰ ì¤‘ (100ê°œ ìƒ˜í”Œ)...\")\n",
    "grid_results = run_grid_search(eval_model, eval_tokenizer, dev_df, GRID_SEARCH_CONFIGS, sample_size=100)\n",
    "\n",
    "# ê²°ê³¼ ì •ë ¬ ë° ì¶œë ¥\n",
    "grid_results_sorted = sorted(grid_results, key=lambda x: x['rouge-l'], reverse=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š Grid Search ê²°ê³¼ (ROUGE-L ìˆœ)\")\n",
    "print(\"=\" * 80)\n",
    "for i, r in enumerate(grid_results_sorted):\n",
    "    print(f\"{i+1}. {r['name']:15} | ROUGE-L: {r['rouge-l']:.2f} | ì˜ˆìƒ LB: {r['estimated_lb']:.2f} | ê¸¸ì´: {r['avg_len']:.1f}\")\n",
    "\n",
    "best_config = grid_results_sorted[0]\n",
    "print(f\"\\nğŸ† ìµœì  ì„¤ì •: {best_config['name']}\")\n",
    "print(f\"   Config: {best_config['config']}\")\n",
    "\n",
    "# í™”ì íƒœê·¸ í™•ì¸\n",
    "print(\"\\nğŸ“‹ í™”ì íƒœê·¸ í™•ì¸ (ìµœì  ì„¤ì • ìƒ˜í”Œ):\")\n",
    "for i in range(3):\n",
    "    print(f\"  [{i}] {best_config.get('predictions', ['N/A'])[i][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“¤ Phase 2-A: Grid Search ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš© ì œì¶œ íŒŒì¼ ìƒì„±\n",
      "================================================================================\n",
      "ğŸ“Œ Grid Search ìµœì  ì„¤ì •: nb8\n",
      "   - num_beams: 8\n",
      "   - length_penalty: 1.0\n",
      "   - repetition_penalty: 1.0\n",
      "   - Dev ROUGE-L: 35.61\n",
      "   - ì˜ˆìƒ LB: 47.00\n",
      "\n",
      "ğŸš€ Grid Search ìµœì  ì„¤ì •ìœ¼ë¡œ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì¶”ë¡ : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [01:32<00:00,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: ./prediction/submit_phase2a_gridsearch.csv\n",
      "   í‰ê·  ê¸¸ì´: 75.8ì\n",
      "   í™”ì íƒœê·¸ í¬í•¨: 357/499ê°œ (71.5%)\n",
      "\n",
      "ğŸ“‹ ì œì¶œ ìƒ˜í”Œ (Grid Search ìµœì í™”):\n",
      "\n",
      "[Test 0]\n",
      "  âŒ Ms. Dawsonì€ ì‚¬ë‚´ ë©”ëª¨ë¥¼ ì‘ì„±í•˜ê³  ë°°í¬í•˜ê¸° ìœ„í•´ ë¶€ì„œì¥ì—ê²Œ ìš”ì²­í•©ë‹ˆë‹¤....\n",
      "\n",
      "[Test 1]\n",
      "  âœ… #Person1# ê³¼ êµí†µ ì²´ì¦ìœ¼ë¡œ ì¸í•´ ëŒ€ì¤‘êµí†µìœ¼ë¡œ ì¶œí‡´ê·¼í•˜ëŠ” ê²ƒì´ ë” ë‚˜ì€ ì„ íƒì´ ë  ìˆ˜ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ë˜í•œ, ì¶œê·¼í•  ë•Œ ì°¨ë¥¼ íƒ€ì§€ ì•Šê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤....\n",
      "\n",
      "[Test 2]\n",
      "  âŒ KateëŠ” Mashaì™€ Heroê°€ ë‘ ë‹¬ ë™ì•ˆ ë³„ê±°í•˜ë‹¤ê°€ ê²°êµ­ ì´í˜¼ì„ í•˜ê²Œ ë˜ì—ˆë‹¤ê³  Kateì—ê²Œ ì„¤ëª…í•œë‹¤....\n",
      "\n",
      "[Test 3]\n",
      "  âœ… #Person1# ì€ Brianì˜ ìƒì¼ì„ ì¶•í•˜í•˜ê¸° ìœ„í•´ ì„ ë¬¼ì„ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤....\n",
      "\n",
      "[Test 4]\n",
      "  âœ… #Person1# ê³¼ ì˜¬ë¦¼í”½ ìŠ¤íƒ€ë””ì›€ì˜ ì „ì²´ ìŠ¤íƒ€ë””ì›€ì€ 6ì›”ì— ì™„ê³µë  ì˜ˆì •ì´ë©°, ì´ 5000ì„ì´ ìˆë‹¤....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“¤ Phase 2-A ì œì¶œ: Grid Search ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš© ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“¤ Phase 2-A: Grid Search ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš© ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def generate_submission_with_best_config(model, tokenizer, test_df, gen_config, output_path):\n",
    "    \"\"\"\n",
    "    Grid Searchì—ì„œ ì°¾ì€ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    \n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"ì¶”ë¡ \"):\n",
    "        inputs = tokenizer(\n",
    "            row['dialogue'],\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=loaded_config['tokenizer']['encoder_max_len'],\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                **gen_config\n",
    "            )\n",
    "        \n",
    "        pred = decode_with_speaker_tags(tokenizer, outputs[0])\n",
    "        pred = postprocess_summary(pred)\n",
    "        summaries.append(pred)\n",
    "    \n",
    "    # ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "    submission = pd.DataFrame({\n",
    "        'fname': test_df['fname'],\n",
    "        'summary': summaries\n",
    "    })\n",
    "    \n",
    "    submission.to_csv(output_path, index=False)\n",
    "    \n",
    "    # í†µê³„ ì¶œë ¥\n",
    "    has_speaker = sum(1 for s in summaries if '#Person' in s)\n",
    "    avg_len = np.mean([len(s) for s in summaries])\n",
    "    print(f\"âœ… ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "    print(f\"   í‰ê·  ê¸¸ì´: {avg_len:.1f}ì\")\n",
    "    print(f\"   í™”ì íƒœê·¸ í¬í•¨: {has_speaker}/{len(summaries)}ê°œ ({has_speaker/len(summaries)*100:.1f}%)\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# Grid Search ìµœì  ì„¤ì • ê°€ì ¸ì˜¤ê¸°\n",
    "try:\n",
    "    # grid_results_sortedì—ì„œ ìµœì  ì„¤ì • ê°€ì ¸ì˜¤ê¸°\n",
    "    BEST_GRID_CONFIG = {\n",
    "        \"num_beams\": best_config['config']['num_beams'],\n",
    "        \"length_penalty\": best_config['config'].get('length_penalty', 1.0),\n",
    "        \"repetition_penalty\": best_config['config'].get('repetition_penalty', 1.0),\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"max_length\": 100,\n",
    "        \"early_stopping\": True,\n",
    "    }\n",
    "    print(f\"ğŸ“Œ Grid Search ìµœì  ì„¤ì •: {best_config['name']}\")\n",
    "    print(f\"   - num_beams: {BEST_GRID_CONFIG['num_beams']}\")\n",
    "    print(f\"   - length_penalty: {BEST_GRID_CONFIG['length_penalty']}\")\n",
    "    print(f\"   - repetition_penalty: {BEST_GRID_CONFIG['repetition_penalty']}\")\n",
    "    print(f\"   - Dev ROUGE-L: {best_config['rouge-l']:.2f}\")\n",
    "    print(f\"   - ì˜ˆìƒ LB: {best_config['estimated_lb']:.2f}\")\n",
    "except:\n",
    "    # ê¸°ë³¸ ì„¤ì • ì‚¬ìš©\n",
    "    BEST_GRID_CONFIG = {\n",
    "        \"num_beams\": 6,\n",
    "        \"length_penalty\": 1.0,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"max_length\": 100,\n",
    "        \"early_stopping\": True,\n",
    "    }\n",
    "    print(\"âš ï¸ Grid Search ê²°ê³¼ ì—†ìŒ, ê¸°ë³¸ ì„¤ì • ì‚¬ìš©\")\n",
    "    print(f\"   ì„¤ì •: {BEST_GRID_CONFIG}\")\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "print(f\"\\nğŸš€ Grid Search ìµœì  ì„¤ì •ìœ¼ë¡œ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\")\n",
    "\n",
    "submission_grid = generate_submission_with_best_config(\n",
    "    eval_model, eval_tokenizer, test_df,\n",
    "    BEST_GRID_CONFIG,\n",
    "    output_path=\"./prediction/submit_phase2a_gridsearch.csv\"\n",
    ")\n",
    "\n",
    "# ìƒ˜í”Œ í™•ì¸\n",
    "print(\"\\nğŸ“‹ ì œì¶œ ìƒ˜í”Œ (Grid Search ìµœì í™”):\")\n",
    "for i in range(5):\n",
    "    print(f\"\\n[Test {i}]\")\n",
    "    summary = submission_grid['summary'].iloc[i]\n",
    "    has_tag = \"âœ…\" if \"#Person\" in summary else \"âŒ\"\n",
    "    print(f\"  {has_tag} {summary[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¯ Phase 2-B: N-Best Reranking\n",
      "================================================================================\n",
      "\n",
      "ğŸ”¬ N-Best Reranking íš¨ê³¼ ì¸¡ì • (50ê°œ ìƒ˜í”Œ)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N-Best: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:10<00:00,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š N-Best Reranking ê²°ê³¼:\n",
      "   ë‹¨ì¼ Beam:        34.71\n",
      "   N-Best (Oracle):  39.69\n",
      "   í–¥ìƒ:             +4.98\n",
      "\n",
      "   ì˜ˆìƒ ë¦¬ë”ë³´ë“œ:\n",
      "   ë‹¨ì¼ Beam:        45.82\n",
      "   N-Best (Oracle):  52.40\n",
      "\n",
      "ğŸ“‹ í™”ì íƒœê·¸ í™•ì¸ (N-Best ìƒ˜í”Œ):\n",
      "  í›„ë³´ 0: #Person1# ì€ ë…ì¼ ì—”ì§€ë‹ˆì–´ê°€ êµí†µ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë§Œë“  ìŠ¤ì¿ í„°ë¥¼ ì†Œê°œí•˜ë©°, ë””ìì¸ì´ ì˜ˆì˜ê³  ë‚ ë µí•˜ë‹¤ê³  ì„¤ëª…í•©ë‹ˆë‹¤. ë˜í•œ, ì–‘ ì†ìœ¼...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ¯ Phase 2-B: N-Best Reranking êµ¬í˜„\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ¯ Phase 2-B: N-Best Reranking\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def generate_nbest_candidates(model, tokenizer, input_text, config, n_candidates=8):\n",
    "    \"\"\"N-Best í›„ë³´ ìƒì„± (í™”ì íƒœê·¸ ë³´ì¡´)\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=loaded_config['tokenizer']['encoder_max_len'],\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            num_beams=config.get(\"num_beams\", 12),\n",
    "            num_return_sequences=n_candidates,\n",
    "            length_penalty=config.get(\"length_penalty\", 1.0),\n",
    "            repetition_penalty=config.get(\"repetition_penalty\", 1.2),\n",
    "            no_repeat_ngram_size=config.get(\"no_repeat_ngram_size\", 3),\n",
    "            max_length=config.get(\"max_length\", 100),\n",
    "            early_stopping=True,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    \n",
    "    candidates = []\n",
    "    for output in outputs:\n",
    "        # âš ï¸ ìˆ˜ì •: í™”ì íƒœê·¸ ë³´ì¡´í•˜ë©´ì„œ ë””ì½”ë”©\n",
    "        text = decode_with_speaker_tags(tokenizer, output)\n",
    "        text = postprocess_summary(text)\n",
    "        candidates.append(text)\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def rerank_by_reference(candidates, reference):\n",
    "    \"\"\"Referenceì™€ ë¹„êµí•˜ì—¬ ìµœì  í›„ë³´ ì„ íƒ (Oracle)\"\"\"\n",
    "    best_score = -1\n",
    "    best_candidate = candidates[0]\n",
    "    \n",
    "    for cand in candidates:\n",
    "        scores = compute_morpheme_rouge(cand, reference)\n",
    "        if scores['rouge-l'] > best_score:\n",
    "            best_score = scores['rouge-l']\n",
    "            best_candidate = cand\n",
    "    \n",
    "    return best_candidate, best_score\n",
    "\n",
    "def rerank_by_heuristic(candidates, target_length=85):\n",
    "    \"\"\"Heuristicìœ¼ë¡œ ìµœì  í›„ë³´ ì„ íƒ (Testìš©)\"\"\"\n",
    "    # ê¸¸ì´ ê¸°ë°˜ ì„ íƒ (Dev ìµœì  ê¸¸ì´ì™€ ê°€ì¥ ê°€ê¹Œìš´ í›„ë³´)\n",
    "    top_candidates = candidates[:3]  # ìƒìœ„ 3ê°œ ì¤‘ ì„ íƒ\n",
    "    best_candidate = min(top_candidates, key=lambda x: abs(len(x) - target_length))\n",
    "    return best_candidate\n",
    "\n",
    "# N-Best Reranking ì„¤ì •\n",
    "NBEST_CONFIG = {\n",
    "    \"num_beams\": 12,\n",
    "    \"length_penalty\": 1.0,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"max_length\": 100,\n",
    "}\n",
    "\n",
    "# Devì—ì„œ N-Best Reranking íš¨ê³¼ ì¸¡ì •\n",
    "print(\"\\nğŸ”¬ N-Best Reranking íš¨ê³¼ ì¸¡ì • (50ê°œ ìƒ˜í”Œ)...\")\n",
    "\n",
    "sample_df = dev_df.sample(n=50, random_state=42)\n",
    "single_beam_scores = []\n",
    "nbest_scores = []\n",
    "\n",
    "for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"N-Best\"):\n",
    "    candidates = generate_nbest_candidates(\n",
    "        eval_model, eval_tokenizer, \n",
    "        row['dialogue'], NBEST_CONFIG, n_candidates=8\n",
    "    )\n",
    "    \n",
    "    # ë‹¨ì¼ beam (ì²« ë²ˆì§¸ í›„ë³´)\n",
    "    single_score = compute_morpheme_rouge(candidates[0], row['summary'])['rouge-l']\n",
    "    single_beam_scores.append(single_score)\n",
    "    \n",
    "    # N-Best Reranking (Oracle)\n",
    "    _, best_score = rerank_by_reference(candidates, row['summary'])\n",
    "    nbest_scores.append(best_score)\n",
    "\n",
    "avg_single = np.mean(single_beam_scores) * 100\n",
    "avg_nbest = np.mean(nbest_scores) * 100\n",
    "\n",
    "print(f\"\\nğŸ“Š N-Best Reranking ê²°ê³¼:\")\n",
    "print(f\"   ë‹¨ì¼ Beam:        {avg_single:.2f}\")\n",
    "print(f\"   N-Best (Oracle):  {avg_nbest:.2f}\")\n",
    "print(f\"   í–¥ìƒ:             +{avg_nbest - avg_single:.2f}\")\n",
    "print(f\"\\n   ì˜ˆìƒ ë¦¬ë”ë³´ë“œ:\")\n",
    "print(f\"   ë‹¨ì¼ Beam:        {avg_single * 1.32:.2f}\")\n",
    "print(f\"   N-Best (Oracle):  {avg_nbest * 1.32:.2f}\")\n",
    "\n",
    "# í™”ì íƒœê·¸ í™•ì¸\n",
    "print(\"\\nğŸ“‹ í™”ì íƒœê·¸ í™•ì¸ (N-Best ìƒ˜í”Œ):\")\n",
    "print(f\"  í›„ë³´ 0: {candidates[0][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“¤ Phase 2-B: N-Best Reranking ì ìš© ì œì¶œ íŒŒì¼ ìƒì„±\n",
      "================================================================================\n",
      "ğŸ“Œ N-Best ì„¤ì •: n_candidates=5, target_length=73\n",
      "ğŸ“Œ ìƒì„± ì„¤ì •: {'num_beams': 12, 'length_penalty': 1.0, 'repetition_penalty': 1.2, 'no_repeat_ngram_size': 3, 'max_length': 100}\n",
      "\n",
      "ğŸš€ N-Best Reranking ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N-Best ì¶”ë¡ : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [01:41<00:00,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: ./prediction/submit_phase2b_nbest.csv\n",
      "   í‰ê·  ê¸¸ì´: 74.9ì\n",
      "   í™”ì íƒœê·¸ í¬í•¨: 359/499ê°œ (71.9%)\n",
      "\n",
      "ğŸ“‹ ì œì¶œ ìƒ˜í”Œ (N-Best Reranking):\n",
      "\n",
      "[Test 0]\n",
      "  âœ… #Person1# ì€ Ms. Dawsonì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¥¼ ì‘ì„±í•˜ê³  ë°°í¬í•´ ë‹¬ë¼ê³  ìš”ì²­í•©ë‹ˆë‹¤....\n",
      "\n",
      "[Test 1]\n",
      "  âœ… #Person1# ê³¼ êµí†µ ì²´ì¦ìœ¼ë¡œ ì¸í•´ ëŒ€ì¤‘êµí†µìœ¼ë¡œ ì¶œí‡´ê·¼í•˜ëŠ” ê²ƒì´ ë” ë‚˜ì€ ì„ íƒì´ ë  ìˆ˜ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ë˜í•œ ì¶œê·¼í•  ë•Œ ì°¨ë¥¼ íƒ€ì§€ ì•Šê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤....\n",
      "\n",
      "[Test 2]\n",
      "  âŒ KateëŠ” Mashaì™€ Heroê°€ ë‘ ë‹¬ ë™ì•ˆ ë³„ê±°í•˜ë‹¤ê°€ ê²°êµ­ ì´í˜¼ì„ í•˜ê²Œ ë˜ì—ˆë‹¤ê³  Kateì—ê²Œ ì´ì•¼ê¸°í•œë‹¤....\n",
      "\n",
      "[Test 3]\n",
      "  âŒ Brianì€ íŒŒí‹°ì—ì„œ ìì‹ ì˜ ìƒì¼ì„ ì¶•í•˜í•˜ê¸° ìœ„í•´ ì¤€ë¹„í•œ ì„ ë¬¼ë¡œ ëª©ê±¸ì´ë¥¼ ì„ ë¬¼ë¡œ ë°›ìŠµë‹ˆë‹¤....\n",
      "\n",
      "[Test 4]\n",
      "  âœ… #Person1# ê³¼ ì˜¬ë¦¼í”½ ìŠ¤íƒ€ë””ì›€ì˜ ì „ì²´ ìŠ¤íƒ€ë””ì›€ì´ ì™„ê³µë  ì˜ˆì •ì´ë©°, ì´ 5000ì„ì´ ìˆë‹¤ê³  ì„¤ëª…í•©ë‹ˆë‹¤....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“¤ Phase 2-B ì œì¶œ: N-Best Reranking ì ìš© ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“¤ Phase 2-B: N-Best Reranking ì ìš© ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def generate_submission_with_nbest(model, tokenizer, test_df, nbest_config, output_path, \n",
    "                                   n_candidates=5, target_length=85):\n",
    "    \"\"\"\n",
    "    N-Best í›„ë³´ ìƒì„± í›„ Heuristicìœ¼ë¡œ ìµœì  í›„ë³´ ì„ íƒí•˜ì—¬ ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "    (Test ë°ì´í„°ëŠ” referenceê°€ ì—†ìœ¼ë¯€ë¡œ ê¸¸ì´ ê¸°ë°˜ heuristic ì‚¬ìš©)\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    \n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"N-Best ì¶”ë¡ \"):\n",
    "        # N-Best í›„ë³´ ìƒì„±\n",
    "        candidates = generate_nbest_candidates(\n",
    "            model, tokenizer, \n",
    "            row['dialogue'], \n",
    "            nbest_config, \n",
    "            n_candidates=n_candidates\n",
    "        )\n",
    "        \n",
    "        # Heuristic ê¸°ë°˜ ìµœì  í›„ë³´ ì„ íƒ (Testìš©)\n",
    "        # ì „ëµ: ìƒìœ„ 3ê°œ ì¤‘ target_lengthì— ê°€ì¥ ê°€ê¹Œìš´ í›„ë³´ ì„ íƒ\n",
    "        top_candidates = candidates[:min(3, len(candidates))]\n",
    "        best_candidate = min(top_candidates, key=lambda x: abs(len(x) - target_length))\n",
    "        \n",
    "        summaries.append(best_candidate)\n",
    "    \n",
    "    # ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "    submission = pd.DataFrame({\n",
    "        'fname': test_df['fname'],\n",
    "        'summary': summaries\n",
    "    })\n",
    "    \n",
    "    submission.to_csv(output_path, index=False)\n",
    "    \n",
    "    # í†µê³„ ì¶œë ¥\n",
    "    has_speaker = sum(1 for s in summaries if '#Person' in s)\n",
    "    avg_len = np.mean([len(s) for s in summaries])\n",
    "    print(f\"âœ… ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "    print(f\"   í‰ê·  ê¸¸ì´: {avg_len:.1f}ì\")\n",
    "    print(f\"   í™”ì íƒœê·¸ í¬í•¨: {has_speaker}/{len(summaries)}ê°œ ({has_speaker/len(summaries)*100:.1f}%)\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# N-Best ì„¤ì • (Dev ì‹¤í—˜ ê²°ê³¼ ê¸°ë°˜)\n",
    "NBEST_SUBMISSION_CONFIG = {\n",
    "    \"num_beams\": 12,\n",
    "    \"length_penalty\": 1.0,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"max_length\": 100,\n",
    "}\n",
    "\n",
    "# Devì—ì„œ ê´€ì°°ëœ ìµœì  ìš”ì•½ ê¸¸ì´ (í‰ê· )\n",
    "# postproc_resultsë‚˜ base_resultsì—ì„œ ê°€ì ¸ì˜¤ê¸°\n",
    "try:\n",
    "    optimal_target_length = int(postproc_results['avg_len'])\n",
    "except:\n",
    "    optimal_target_length = 80  # ê¸°ë³¸ê°’\n",
    "\n",
    "print(f\"ğŸ“Œ N-Best ì„¤ì •: n_candidates=5, target_length={optimal_target_length}\")\n",
    "print(f\"ğŸ“Œ ìƒì„± ì„¤ì •: {NBEST_SUBMISSION_CONFIG}\")\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "print(f\"\\nğŸš€ N-Best Reranking ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\")\n",
    "\n",
    "submission_nbest = generate_submission_with_nbest(\n",
    "    eval_model, eval_tokenizer, test_df,\n",
    "    NBEST_SUBMISSION_CONFIG,\n",
    "    output_path=\"./prediction/submit_phase2b_nbest.csv\",\n",
    "    n_candidates=5,\n",
    "    target_length=optimal_target_length\n",
    ")\n",
    "\n",
    "# ìƒ˜í”Œ í™•ì¸\n",
    "print(\"\\nğŸ“‹ ì œì¶œ ìƒ˜í”Œ (N-Best Reranking):\")\n",
    "for i in range(5):\n",
    "    print(f\"\\n[Test {i}]\")\n",
    "    summary = submission_nbest['summary'].iloc[i]\n",
    "    has_tag = \"âœ…\" if \"#Person\" in summary else \"âŒ\"\n",
    "    print(f\"  {has_tag} {summary[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¤ Phase 3: ëª¨ë¸ ì•™ìƒë¸” ì „ëµ\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ ì•™ìƒë¸” ì „ëµ:\n",
      "1. digit82/kobart-summarization - ë¦¬ë”ë³´ë“œ ìµœê³  ì„±ëŠ¥ ëª¨ë¸\n",
      "2. gogamza/kobart-base-v2 - ë³´ì™„ì  íŠ¹ì„±\n",
      "\n",
      "ì•™ìƒë¸” ë°©ë²•:\n",
      "- ê° ëª¨ë¸ì—ì„œ N-best í›„ë³´ ìƒì„±\n",
      "- Dev ë°ì´í„° ê¸°ì¤€ ROUGEë¡œ ìµœì  í›„ë³´ ì„ íƒ\n",
      "- Testì—ì„œëŠ” ê¸¸ì´/ë‹¤ì–‘ì„± ê¸°ë°˜ heuristic ì ìš©\n",
      "\n",
      "âš ï¸ í™”ì íƒœê·¸(#Person1#, #Person2#)ëŠ” ë³´ì¡´ë©ë‹ˆë‹¤!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ¤ Phase 3: ëª¨ë¸ ì•™ìƒë¸” ì „ëµ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ¤ Phase 3: ëª¨ë¸ ì•™ìƒë¸” ì „ëµ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def load_secondary_model(model_name_or_path, device):\n",
    "    \"\"\"ë³´ì¡° ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    tokenizer.add_special_tokens({\n",
    "        'additional_special_tokens': loaded_config['tokenizer']['special_tokens']\n",
    "    })\n",
    "    \n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name_or_path)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# ì•™ìƒë¸” ì¶”ë¡  í•¨ìˆ˜\n",
    "def ensemble_inference(models_info, dialogue, gen_config, reference=None):\n",
    "    \"\"\"\n",
    "    ì—¬ëŸ¬ ëª¨ë¸ì˜ ì¶œë ¥ ì¤‘ ìµœì  ì„ íƒ (í™”ì íƒœê·¸ ë³´ì¡´)\n",
    "    models_info: [(model, tokenizer, name), ...]\n",
    "    \"\"\"\n",
    "    all_candidates = []\n",
    "    \n",
    "    for model, tokenizer, name in models_info:\n",
    "        inputs = tokenizer(\n",
    "            dialogue,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=loaded_config['tokenizer']['encoder_max_len'],\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                num_beams=gen_config.get(\"num_beams\", 6),\n",
    "                num_return_sequences=gen_config.get(\"num_return_sequences\", 3),\n",
    "                length_penalty=gen_config.get(\"length_penalty\", 1.0),\n",
    "                repetition_penalty=gen_config.get(\"repetition_penalty\", 1.2),\n",
    "                no_repeat_ngram_size=gen_config.get(\"no_repeat_ngram_size\", 3),\n",
    "                max_length=gen_config.get(\"max_length\", 100),\n",
    "                early_stopping=True,\n",
    "            )\n",
    "        \n",
    "        for output in outputs:\n",
    "            # âš ï¸ ìˆ˜ì •: í™”ì íƒœê·¸ ë³´ì¡´í•˜ë©´ì„œ ë””ì½”ë”©\n",
    "            text = decode_with_speaker_tags(tokenizer, output)\n",
    "            text = postprocess_summary(text)\n",
    "            all_candidates.append((text, name))\n",
    "    \n",
    "    # Referenceê°€ ìˆìœ¼ë©´ ROUGEë¡œ ìµœì  ì„ íƒ, ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ë°˜í™˜\n",
    "    if reference:\n",
    "        best_candidate = all_candidates[0][0]\n",
    "        best_score = -1\n",
    "        for cand, name in all_candidates:\n",
    "            score = compute_morpheme_rouge(cand, reference)['rouge-l']\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_candidate = cand\n",
    "        return best_candidate, best_score\n",
    "    else:\n",
    "        # Heuristic: ê¸¸ì´ê°€ ì¤‘ê°„ì¸ í›„ë³´ ì„ íƒ\n",
    "        lengths = [len(c[0]) for c in all_candidates]\n",
    "        median_len = np.median(lengths)\n",
    "        best_candidate = min(all_candidates, key=lambda x: abs(len(x[0]) - median_len))\n",
    "        return best_candidate[0], None\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“‹ ì•™ìƒë¸” ì „ëµ:\n",
    "1. digit82/kobart-summarization - ë¦¬ë”ë³´ë“œ ìµœê³  ì„±ëŠ¥ ëª¨ë¸\n",
    "2. gogamza/kobart-base-v2 - ë³´ì™„ì  íŠ¹ì„±\n",
    "\n",
    "ì•™ìƒë¸” ë°©ë²•:\n",
    "- ê° ëª¨ë¸ì—ì„œ N-best í›„ë³´ ìƒì„±\n",
    "- Dev ë°ì´í„° ê¸°ì¤€ ROUGEë¡œ ìµœì  í›„ë³´ ì„ íƒ\n",
    "- Testì—ì„œëŠ” ê¸¸ì´/ë‹¤ì–‘ì„± ê¸°ë°˜ heuristic ì ìš©\n",
    "\n",
    "âš ï¸ í™”ì íƒœê·¸(#Person1#, #Person2#)ëŠ” ë³´ì¡´ë©ë‹ˆë‹¤!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¤ Phase 3: ëª¨ë¸ ì•™ìƒë¸” ì œì¶œ íŒŒì¼ ìƒì„±\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Phase 3 ì•™ìƒë¸” ì œì¶œ íŒŒì¼ ìƒì„± ì‹œì‘\n",
      "============================================================\n",
      "\n",
      "[1] Primary ëª¨ë¸ (digit82 fine-tuned) ë‹¨ë… ì œì¶œ íŒŒì¼...\n",
      "\n",
      "ğŸ“Š ë‹¨ì¼ ëª¨ë¸ (digit82-finetuned) ì œì¶œ íŒŒì¼ ìƒì„±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "digit82-finetuned ì¶”ë¡  ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [01:28<00:00,  5.64it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: ./prediction/submit_phase3_primary.csv\n",
      "\n",
      "[2] Secondary ëª¨ë¸ ë¡œë“œ ì‹œë„ (gogamza/kobart-base-v2)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Secondary ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\n",
      "\n",
      "[3] ì•™ìƒë¸” ì œì¶œ íŒŒì¼ ìƒì„± (median_length ì„ íƒ)...\n",
      "\n",
      "ğŸ“Š ì•™ìƒë¸” ëª¨ë¸ ì œì¶œ íŒŒì¼ ìƒì„± ì‹œì‘...\n",
      "  - ëª¨ë¸ ìˆ˜: 2\n",
      "  - ì„ íƒ ë°©ë²•: median_length\n",
      "  - í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: 499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì•™ìƒë¸” ì¶”ë¡  ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [05:44<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: ./prediction/submit_phase3_ensemble.csv\n",
      "  - ì´ ìƒ˜í”Œ ìˆ˜: 499\n",
      "  - ìš”ì•½ í‰ê·  ê¸¸ì´: 82.1ì\n",
      "  - ìš”ì•½ ìµœì†Œ ê¸¸ì´: 28ì\n",
      "  - ìš”ì•½ ìµœëŒ€ ê¸¸ì´: 222ì\n",
      "\n",
      "ğŸ§¹ Secondary ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ Phase 3 ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ\n",
      "============================================================\n",
      "\n",
      "ìƒì„±ëœ ì œì¶œ íŒŒì¼:\n",
      "1. ./prediction/submit_phase3_primary.csv\n",
      "   - Primary ëª¨ë¸ (digit82 fine-tuned) ë‹¨ë…\n",
      "   - ì•™ìƒë¸” baselineìœ¼ë¡œ ì‚¬ìš©\n",
      "\n",
      "2. ./prediction/submit_phase3_ensemble.csv (Secondary ëª¨ë¸ ë¡œë“œ ì„±ê³µ ì‹œ)\n",
      "   - digit82-finetuned + gogamza-base-v2 ì•™ìƒë¸”\n",
      "   - median_length ë°©ì‹ìœ¼ë¡œ ìµœì  í›„ë³´ ì„ íƒ\n",
      "\n",
      "ğŸ“Œ ë¦¬ë”ë³´ë“œ ì œì¶œ ìˆœì„œ ê¶Œì¥:\n",
      "   1) submit_phase3_primary.csv â†’ ë‹¨ì¼ ëª¨ë¸ baseline\n",
      "   2) submit_phase3_ensemble.csv â†’ ì•™ìƒë¸” íš¨ê³¼ í™•ì¸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ¤ Phase 3: ëª¨ë¸ ì•™ìƒë¸” ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ¤ Phase 3: ëª¨ë¸ ì•™ìƒë¸” ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ì•™ìƒë¸” ì„¤ì •\n",
    "ENSEMBLE_CONFIG = {\n",
    "    \"primary_model\": \"./prediction_kobart_v1/checkpoint-4674\",  # digit82 ê¸°ë°˜ fine-tuned\n",
    "    \"secondary_model\": \"gogamza/kobart-base-v2\",                # ë³´ì¡° ëª¨ë¸\n",
    "    \"gen_config\": {\n",
    "        \"num_beams\": 6,\n",
    "        \"num_return_sequences\": 3,\n",
    "        \"length_penalty\": 1.0,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"max_length\": 100,\n",
    "    },\n",
    "    \"selection_method\": \"median_length\"  # or \"first_primary\" or \"longest\"\n",
    "}\n",
    "\n",
    "def generate_ensemble_submission(test_df, models_info, gen_config, output_path, selection_method=\"median_length\"):\n",
    "    \"\"\"ì•™ìƒë¸” ëª¨ë¸ë¡œ ì œì¶œ íŒŒì¼ ìƒì„±\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ì•™ìƒë¸” ëª¨ë¸ ì œì¶œ íŒŒì¼ ìƒì„± ì‹œì‘...\")\n",
    "    print(f\"  - ëª¨ë¸ ìˆ˜: {len(models_info)}\")\n",
    "    print(f\"  - ì„ íƒ ë°©ë²•: {selection_method}\")\n",
    "    print(f\"  - í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(test_df)}\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"ì•™ìƒë¸” ì¶”ë¡  ì¤‘\"):\n",
    "        dialogue = row['dialogue']\n",
    "        all_candidates = []\n",
    "        \n",
    "        # ê° ëª¨ë¸ì—ì„œ í›„ë³´ ìƒì„±\n",
    "        for model, tokenizer, name in models_info:\n",
    "            inputs = tokenizer(\n",
    "                dialogue,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=loaded_config['tokenizer']['encoder_max_len'],\n",
    "                truncation=True,\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    input_ids=inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    num_beams=gen_config.get(\"num_beams\", 6),\n",
    "                    num_return_sequences=gen_config.get(\"num_return_sequences\", 3),\n",
    "                    length_penalty=gen_config.get(\"length_penalty\", 1.0),\n",
    "                    repetition_penalty=gen_config.get(\"repetition_penalty\", 1.2),\n",
    "                    no_repeat_ngram_size=gen_config.get(\"no_repeat_ngram_size\", 3),\n",
    "                    max_length=gen_config.get(\"max_length\", 100),\n",
    "                    early_stopping=True,\n",
    "                )\n",
    "            \n",
    "            # í™”ì íƒœê·¸ ë³´ì¡´í•˜ë©´ì„œ ë””ì½”ë”©\n",
    "            for output in outputs:\n",
    "                text = decode_with_speaker_tags(tokenizer, output)\n",
    "                text = postprocess_summary(text)\n",
    "                all_candidates.append({\n",
    "                    \"text\": text,\n",
    "                    \"model\": name,\n",
    "                    \"length\": len(text)\n",
    "                })\n",
    "        \n",
    "        # ì„ íƒ ë°©ë²•ì— ë”°ë¼ ìµœì  í›„ë³´ ì„ íƒ\n",
    "        if selection_method == \"median_length\":\n",
    "            # ì¤‘ê°„ ê¸¸ì´ í›„ë³´ ì„ íƒ\n",
    "            lengths = [c[\"length\"] for c in all_candidates]\n",
    "            median_len = np.median(lengths)\n",
    "            best = min(all_candidates, key=lambda x: abs(x[\"length\"] - median_len))\n",
    "        elif selection_method == \"first_primary\":\n",
    "            # Primary ëª¨ë¸ì˜ ì²« ë²ˆì§¸ í›„ë³´\n",
    "            best = all_candidates[0]\n",
    "        elif selection_method == \"longest\":\n",
    "            # ê°€ì¥ ê¸´ í›„ë³´ ì„ íƒ\n",
    "            best = max(all_candidates, key=lambda x: x[\"length\"])\n",
    "        elif selection_method == \"shortest\":\n",
    "            # ê°€ì¥ ì§§ì€ í›„ë³´ ì„ íƒ (ê³¼ì í•© ë°©ì§€)\n",
    "            best = min(all_candidates, key=lambda x: x[\"length\"])\n",
    "        else:\n",
    "            best = all_candidates[0]\n",
    "        \n",
    "        results.append({\n",
    "            \"fname\": row['fname'],\n",
    "            \"summary\": best[\"text\"]\n",
    "        })\n",
    "    \n",
    "    # ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "    submission_df = pd.DataFrame(results)\n",
    "    submission_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    # í†µê³„ ì¶œë ¥\n",
    "    summary_lengths = [len(r[\"summary\"]) for r in results]\n",
    "    print(f\"\\nâœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "    print(f\"  - ì´ ìƒ˜í”Œ ìˆ˜: {len(results)}\")\n",
    "    print(f\"  - ìš”ì•½ í‰ê·  ê¸¸ì´: {np.mean(summary_lengths):.1f}ì\")\n",
    "    print(f\"  - ìš”ì•½ ìµœì†Œ ê¸¸ì´: {min(summary_lengths)}ì\")\n",
    "    print(f\"  - ìš”ì•½ ìµœëŒ€ ê¸¸ì´: {max(summary_lengths)}ì\")\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "def generate_single_model_submission(test_df, model, tokenizer, model_name, gen_config, output_path):\n",
    "    \"\"\"ë‹¨ì¼ ëª¨ë¸ë¡œ ì œì¶œ íŒŒì¼ ìƒì„± (ì•™ìƒë¸” ë¹„êµìš©)\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ë‹¨ì¼ ëª¨ë¸ ({model_name}) ì œì¶œ íŒŒì¼ ìƒì„±...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=f\"{model_name} ì¶”ë¡  ì¤‘\"):\n",
    "        dialogue = row['dialogue']\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            dialogue,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=loaded_config['tokenizer']['encoder_max_len'],\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                num_beams=gen_config.get(\"num_beams\", 6),\n",
    "                length_penalty=gen_config.get(\"length_penalty\", 1.0),\n",
    "                repetition_penalty=gen_config.get(\"repetition_penalty\", 1.2),\n",
    "                no_repeat_ngram_size=gen_config.get(\"no_repeat_ngram_size\", 3),\n",
    "                max_length=gen_config.get(\"max_length\", 100),\n",
    "                early_stopping=True,\n",
    "            )\n",
    "        \n",
    "        summary = decode_with_speaker_tags(tokenizer, outputs[0])\n",
    "        summary = postprocess_summary(summary)\n",
    "        \n",
    "        results.append({\n",
    "            \"fname\": row['fname'],\n",
    "            \"summary\": summary\n",
    "        })\n",
    "    \n",
    "    submission_df = pd.DataFrame(results)\n",
    "    submission_df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "# =============================================================================\n",
    "# ì œì¶œ íŒŒì¼ ìƒì„± ì‹¤í–‰\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸš€ Phase 3 ì•™ìƒë¸” ì œì¶œ íŒŒì¼ ìƒì„± ì‹œì‘\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ë°©ë²• 1: Primary ëª¨ë¸ë§Œ ì‚¬ìš© (baseline - í˜„ì¬ ëª¨ë¸)\n",
    "print(\"\\n[1] Primary ëª¨ë¸ (digit82 fine-tuned) ë‹¨ë… ì œì¶œ íŒŒì¼...\")\n",
    "submission_primary = generate_single_model_submission(\n",
    "    test_df=test_df,\n",
    "    model=eval_model,\n",
    "    tokenizer=eval_tokenizer,\n",
    "    model_name=\"digit82-finetuned\",\n",
    "    gen_config=ENSEMBLE_CONFIG[\"gen_config\"],\n",
    "    output_path=\"./prediction/submit_phase3_primary.csv\"\n",
    ")\n",
    "\n",
    "# ë°©ë²• 2: ì•™ìƒë¸” ì‹œë„ (secondary ëª¨ë¸ ë¡œë“œ ê°€ëŠ¥ ì‹œ)\n",
    "try:\n",
    "    print(\"\\n[2] Secondary ëª¨ë¸ ë¡œë“œ ì‹œë„ (gogamza/kobart-base-v2)...\")\n",
    "    secondary_model, secondary_tokenizer = load_secondary_model(\n",
    "        ENSEMBLE_CONFIG[\"secondary_model\"], device\n",
    "    )\n",
    "    print(\"âœ… Secondary ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\")\n",
    "    \n",
    "    # ì•™ìƒë¸” ëª¨ë¸ ì •ë³´\n",
    "    models_info = [\n",
    "        (eval_model, eval_tokenizer, \"digit82-finetuned\"),\n",
    "        (secondary_model, secondary_tokenizer, \"gogamza-base-v2\"),\n",
    "    ]\n",
    "    \n",
    "    # ì•™ìƒë¸” ì œì¶œ íŒŒì¼ ìƒì„± (median_length ë°©ì‹)\n",
    "    print(\"\\n[3] ì•™ìƒë¸” ì œì¶œ íŒŒì¼ ìƒì„± (median_length ì„ íƒ)...\")\n",
    "    submission_ensemble = generate_ensemble_submission(\n",
    "        test_df=test_df,\n",
    "        models_info=models_info,\n",
    "        gen_config=ENSEMBLE_CONFIG[\"gen_config\"],\n",
    "        output_path=\"./prediction/submit_phase3_ensemble.csv\",\n",
    "        selection_method=\"median_length\"\n",
    "    )\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    del secondary_model, secondary_tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\nğŸ§¹ Secondary ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâš ï¸ Secondary ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"   Primary ëª¨ë¸ë§Œ ì‚¬ìš©í•œ ì œì¶œ íŒŒì¼ì„ ì‚¬ìš©í•˜ì„¸ìš”.\")\n",
    "    submission_ensemble = None\n",
    "\n",
    "# =============================================================================\n",
    "# ê²°ê³¼ ìš”ì•½\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“‹ Phase 3 ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "ìƒì„±ëœ ì œì¶œ íŒŒì¼:\n",
    "1. ./prediction/submit_phase3_primary.csv\n",
    "   - Primary ëª¨ë¸ (digit82 fine-tuned) ë‹¨ë…\n",
    "   - ì•™ìƒë¸” baselineìœ¼ë¡œ ì‚¬ìš©\n",
    "\n",
    "2. ./prediction/submit_phase3_ensemble.csv (Secondary ëª¨ë¸ ë¡œë“œ ì„±ê³µ ì‹œ)\n",
    "   - digit82-finetuned + gogamza-base-v2 ì•™ìƒë¸”\n",
    "   - median_length ë°©ì‹ìœ¼ë¡œ ìµœì  í›„ë³´ ì„ íƒ\n",
    "\n",
    "ğŸ“Œ ë¦¬ë”ë³´ë“œ ì œì¶œ ìˆœì„œ ê¶Œì¥:\n",
    "   1) submit_phase3_primary.csv â†’ ë‹¨ì¼ ëª¨ë¸ baseline\n",
    "   2) submit_phase3_ensemble.csv â†’ ì•™ìƒë¸” íš¨ê³¼ í™•ì¸\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“ Phase 4: Knowledge Distillation ì„¤ì •\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ Knowledge Distillation íŒŒì´í”„ë¼ì¸:\n",
      "\n",
      "Step 1: Teacher ëª¨ë¸ (digit82)ë¡œ Train ë°ì´í„° pseudo-label ìƒì„±\n",
      "Step 2: Gold label 50% + Teacher pseudo-label 50% í˜¼í•© ë°ì´í„°ì…‹ êµ¬ì¶•\n",
      "Step 3: Student ëª¨ë¸ (gogamza) í•™ìŠµ\n",
      "Step 4: í•™ìŠµëœ Studentë¡œ ì¶”ë¡  ë° í‰ê°€\n",
      "\n",
      "ğŸ’¡ í•µì‹¬ ì•„ì´ë””ì–´:\n",
      "- Teacherì˜ \"ìŠ¤íƒ€ì¼\"ê³¼ \"ì„ íƒ\"ì„ Studentê°€ í•™ìŠµ\n",
      "- Gold labelë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ ë‹¤ì–‘í•œ í‘œí˜„ í•™ìŠµ ê°€ëŠ¥\n",
      "\n",
      "âš ï¸ í™”ì íƒœê·¸(#Person1#, #Person2#)ëŠ” ë³´ì¡´ë©ë‹ˆë‹¤!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“ Phase 4: Knowledge Distillation ì„¤ì •\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“ Phase 4: Knowledge Distillation ì„¤ì •\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "DISTILL_CONFIG = {\n",
    "    \"teacher_model\": \"digit82/kobart-summarization\",  # Teacher (ë” í°/ì¢‹ì€ ëª¨ë¸)\n",
    "    \"student_model\": \"gogamza/kobart-base-v2\",        # Student (í•™ìŠµí•  ëª¨ë¸)\n",
    "    \"output_dir\": \"./results_distillation\",\n",
    "    \"gold_ratio\": 0.5,  # Gold label 50%, Teacher pseudo-label 50%\n",
    "    \"training\": {\n",
    "        \"num_train_epochs\": 10,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"per_device_eval_batch_size\": 32,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"fp16\": True,\n",
    "        \"label_smoothing_factor\": 0.1,\n",
    "    }\n",
    "}\n",
    "\n",
    "class DistillationDataset(Dataset):\n",
    "    \"\"\"Gold labelê³¼ Teacher pseudo-labelì„ í˜¼í•©í•œ ë°ì´í„°ì…‹\"\"\"\n",
    "    \n",
    "    def __init__(self, dialogues, gold_labels, teacher_labels, tokenizer, max_enc_len, max_dec_len, gold_ratio=0.5):\n",
    "        self.dialogues = dialogues\n",
    "        self.gold_labels = gold_labels\n",
    "        self.teacher_labels = teacher_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_enc_len = max_enc_len\n",
    "        self.max_dec_len = max_dec_len\n",
    "        \n",
    "        # ê° ìƒ˜í”Œì— ëŒ€í•´ gold/teacher ì¤‘ í•˜ë‚˜ ì„ íƒ\n",
    "        self.selected_labels = []\n",
    "        for i in range(len(dialogues)):\n",
    "            if random.random() < gold_ratio:\n",
    "                self.selected_labels.append(gold_labels[i])\n",
    "            else:\n",
    "                self.selected_labels.append(teacher_labels[i])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dialogue = self.dialogues[idx]\n",
    "        summary = self.selected_labels[idx]\n",
    "        \n",
    "        # ì¸ì½”ë” ì…ë ¥\n",
    "        encoder_inputs = self.tokenizer(\n",
    "            dialogue,\n",
    "            max_length=self.max_enc_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # ë””ì½”ë” ë ˆì´ë¸”\n",
    "        decoder_inputs = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=self.max_dec_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        labels = decoder_inputs['input_ids'].squeeze()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoder_inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': encoder_inputs['attention_mask'].squeeze(),\n",
    "            'labels': labels,\n",
    "        }\n",
    "\n",
    "def generate_teacher_pseudo_labels(teacher_model, teacher_tokenizer, dialogues, gen_config, batch_size=16):\n",
    "    \"\"\"Teacher ëª¨ë¸ë¡œ pseudo-label ìƒì„± (í™”ì íƒœê·¸ ë³´ì¡´)\"\"\"\n",
    "    pseudo_labels = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(dialogues), batch_size), desc=\"Teacher ì¶”ë¡ \"):\n",
    "        batch = dialogues[i:i+batch_size]\n",
    "        \n",
    "        inputs = teacher_tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=loaded_config['tokenizer']['encoder_max_len'],\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = teacher_model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                **gen_config\n",
    "            )\n",
    "        \n",
    "        for output in outputs:\n",
    "            # âš ï¸ ìˆ˜ì •: í™”ì íƒœê·¸ ë³´ì¡´í•˜ë©´ì„œ ë””ì½”ë”©\n",
    "            text = decode_with_speaker_tags(teacher_tokenizer, output)\n",
    "            text = postprocess_summary(text)\n",
    "            pseudo_labels.append(text)\n",
    "    \n",
    "    return pseudo_labels\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“‹ Knowledge Distillation íŒŒì´í”„ë¼ì¸:\n",
    "\n",
    "Step 1: Teacher ëª¨ë¸ (digit82)ë¡œ Train ë°ì´í„° pseudo-label ìƒì„±\n",
    "Step 2: Gold label 50% + Teacher pseudo-label 50% í˜¼í•© ë°ì´í„°ì…‹ êµ¬ì¶•\n",
    "Step 3: Student ëª¨ë¸ (gogamza) í•™ìŠµ\n",
    "Step 4: í•™ìŠµëœ Studentë¡œ ì¶”ë¡  ë° í‰ê°€\n",
    "\n",
    "ğŸ’¡ í•µì‹¬ ì•„ì´ë””ì–´:\n",
    "- Teacherì˜ \"ìŠ¤íƒ€ì¼\"ê³¼ \"ì„ íƒ\"ì„ Studentê°€ í•™ìŠµ\n",
    "- Gold labelë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ ë‹¤ì–‘í•œ í‘œí˜„ í•™ìŠµ ê°€ëŠ¥\n",
    "\n",
    "âš ï¸ í™”ì íƒœê·¸(#Person1#, #Person2#)ëŠ” ë³´ì¡´ë©ë‹ˆë‹¤!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“¤ ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± (í™”ì íƒœê·¸ ë³´ì¡´)\n",
      "================================================================================\n",
      "ğŸ“Š Test ë°ì´í„°: 499ê°œ\n",
      "\n",
      "ğŸš€ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì¶”ë¡ : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [01:31<00:00,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: ./prediction/submit_optimized_v2.csv\n",
      "   í‰ê·  ê¸¸ì´: 75.0ì\n",
      "   í™”ì íƒœê·¸ í¬í•¨: 359/499ê°œ (71.9%)\n",
      "\n",
      "ğŸ“‹ ì œì¶œ ìƒ˜í”Œ (í™”ì íƒœê·¸ í™•ì¸):\n",
      "\n",
      "[Test 0]\n",
      "  âŒ Ms. Dawsonì€ ì‚¬ë‚´ ë©”ëª¨ë¥¼ ì‘ì„±í•˜ê³  ë°°í¬í•˜ê¸° ìœ„í•´ ë¶€ì„œì¥ì—ê²Œ ìš”ì²­í•©ë‹ˆë‹¤....\n",
      "\n",
      "[Test 1]\n",
      "  âœ… #Person1# ê³¼ êµí†µ ì²´ì¦ìœ¼ë¡œ ì¸í•´ ëŒ€ì¤‘êµí†µìœ¼ë¡œ ì¶œí‡´ê·¼í•˜ëŠ” ê²ƒì´ ë” ë‚˜ì€ ì„ íƒì´ ë  ìˆ˜ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ë˜í•œ, ì¶œê·¼í•  ë•Œ ì°¨ë¥¼ íƒ€ì§€ ì•Šê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤....\n",
      "\n",
      "[Test 2]\n",
      "  âŒ KateëŠ” Mashaì™€ Heroê°€ ë‘ ë‹¬ ë™ì•ˆ ë³„ê±°í•˜ë‹¤ê°€ ê²°êµ­ ì´í˜¼ì„ í•˜ê²Œ ë˜ì—ˆë‹¤ê³  ì „í•œë‹¤....\n",
      "\n",
      "[Test 3]\n",
      "  âœ… #Person1# ì€ Brianì˜ ìƒì¼ì„ ì¶•í•˜í•˜ê¸° ìœ„í•´ ì„ ë¬¼ì„ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤....\n",
      "\n",
      "[Test 4]\n",
      "  âœ… #Person1# ê³¼ ì˜¬ë¦¼í”½ ìŠ¤íƒ€ë””ì›€ì˜ ì „ì²´ ìŠ¤íƒ€ë””ì›€ì€ 6ì›”ì— ì™„ê³µë  ì˜ˆì •ì´ë©°, ì´ 5000ì„ì´ ìˆë‹¤....\n",
      "\n",
      "ğŸ“‹ ê¸°ì¡´ ê²°ê³¼(output_k1.csv)ì™€ ë¹„êµ:\n",
      "\n",
      "[Test 0]\n",
      "  ê¸°ì¡´:  #Person1# ì€ Ms. Dawsonì—ê²Œ ì´ë©”ì¼ê³¼ ê³µì‹ ë©”ëª¨ë§Œ ì œí•œëœë‹¤ê³  ì•Œë¦½ë‹ˆë‹¤. ì´ëŠ” ì‚¬ë‚´ í†µì‹ ì—ë§Œ ì ìš©ë˜ë©°, ì™¸ë¶€ í†µì‹ ì—ë„ ì ìš©ë©ë‹ˆë‹¤...\n",
      "  ìƒˆë¡œ: Ms. Dawsonì€ ì‚¬ë‚´ ë©”ëª¨ë¥¼ ì‘ì„±í•˜ê³  ë°°í¬í•˜ê¸° ìœ„í•´ ë¶€ì„œì¥ì—ê²Œ ìš”ì²­í•©ë‹ˆë‹¤....\n",
      "\n",
      "[Test 1]\n",
      "  ê¸°ì¡´:  #Person2# ëŠ” #Person1# ì—ê²Œ ì¶œí‡´ê·¼ ì‹œê°„ì— êµí†µì²´ì¦ì„ í”¼í•˜ê¸° ìœ„í•´ ëŒ€ì¤‘êµí†µì„ ì´ìš©í•˜ëŠ” ê²ƒì´ ë” ë‚˜ì€ ì„ íƒì´ë¼ê³  ì¡°ì–¸í•©ë‹ˆë‹¤. #...\n",
      "  ìƒˆë¡œ: #Person1# ê³¼ êµí†µ ì²´ì¦ìœ¼ë¡œ ì¸í•´ ëŒ€ì¤‘êµí†µìœ¼ë¡œ ì¶œí‡´ê·¼í•˜ëŠ” ê²ƒì´ ë” ë‚˜ì€ ì„ íƒì´ ë  ìˆ˜ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ë˜í•œ, ì¶œê·¼í•  ë•Œ ì°¨ë¥¼ íƒ€ì§€ ì•Š...\n",
      "\n",
      "[Test 2]\n",
      "  ê¸°ì¡´:  #Person1# ì€ Kateì—ê²Œ Mashaì™€ Heroê°€ ë‘ ë‹¬ ë™ì•ˆ ë³„ê±°í•˜ë‹¤ê°€ ê²°êµ­ ì´í˜¼í–ˆë‹¤ê³  ì „í•©ë‹ˆë‹¤. ê·¸ë“¤ì€ ìë…€ ì–‘ìœ¡ì— ëŒ€í•´ ì´ì•¼ê¸°í•©ë‹ˆ...\n",
      "  ìƒˆë¡œ: KateëŠ” Mashaì™€ Heroê°€ ë‘ ë‹¬ ë™ì•ˆ ë³„ê±°í•˜ë‹¤ê°€ ê²°êµ­ ì´í˜¼ì„ í•˜ê²Œ ë˜ì—ˆë‹¤ê³  ì „í•œë‹¤....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“¤ ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± (ìµœì  ì„¤ì • ì ìš©) - í™”ì íƒœê·¸ ë³´ì¡´!\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“¤ ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± (í™”ì íƒœê·¸ ë³´ì¡´)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def generate_final_submission(model, tokenizer, test_df, gen_config, output_path):\n",
    "    \"\"\"ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± (í™”ì íƒœê·¸ ë³´ì¡´)\"\"\"\n",
    "    summaries = []\n",
    "    \n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"ì¶”ë¡ \"):\n",
    "        inputs = tokenizer(\n",
    "            row['dialogue'],\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=loaded_config['tokenizer']['encoder_max_len'],\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                **gen_config\n",
    "            )\n",
    "        \n",
    "        # âš ï¸ í•µì‹¬ ìˆ˜ì •: í™”ì íƒœê·¸ ë³´ì¡´í•˜ë©´ì„œ ë””ì½”ë”©!\n",
    "        pred = decode_with_speaker_tags(tokenizer, outputs[0])\n",
    "        pred = postprocess_summary(pred)\n",
    "        summaries.append(pred)\n",
    "    \n",
    "    # ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "    submission = pd.DataFrame({\n",
    "        'fname': test_df['fname'],\n",
    "        'summary': summaries\n",
    "    })\n",
    "    \n",
    "    submission.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "    print(f\"   í‰ê·  ê¸¸ì´: {np.mean([len(s) for s in summaries]):.1f}ì\")\n",
    "    \n",
    "    # í™”ì íƒœê·¸ í¬í•¨ ì—¬ë¶€ í™•ì¸\n",
    "    has_speaker_tags = sum(1 for s in summaries if '#Person' in s)\n",
    "    print(f\"   í™”ì íƒœê·¸ í¬í•¨: {has_speaker_tags}/{len(summaries)}ê°œ ({has_speaker_tags/len(summaries)*100:.1f}%)\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# Test ë°ì´í„° ë¡œë“œ\n",
    "test_df = pd.read_csv(\"./data/test.csv\")\n",
    "print(f\"ğŸ“Š Test ë°ì´í„°: {len(test_df)}ê°œ\")\n",
    "\n",
    "# ìµœì  ì„¤ì • (Grid Search ê²°ê³¼ ê¸°ë°˜)\n",
    "FINAL_GEN_CONFIG = {\n",
    "    \"num_beams\": 7,\n",
    "    \"length_penalty\": 1.0,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"max_length\": 100,\n",
    "    \"early_stopping\": True,\n",
    "}\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "print(\"\\nğŸš€ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\")\n",
    "submission = generate_final_submission(\n",
    "    eval_model, eval_tokenizer, test_df, \n",
    "    FINAL_GEN_CONFIG, \n",
    "    \"./prediction/submit_optimized_v2.csv\"\n",
    ")\n",
    "\n",
    "# ìƒ˜í”Œ ì¶œë ¥ (í™”ì íƒœê·¸ í™•ì¸)\n",
    "print(\"\\nğŸ“‹ ì œì¶œ ìƒ˜í”Œ (í™”ì íƒœê·¸ í™•ì¸):\")\n",
    "for i in range(5):\n",
    "    print(f\"\\n[Test {i}]\")\n",
    "    summary = submission['summary'].iloc[i]\n",
    "    has_tag = \"âœ…\" if \"#Person\" in summary else \"âŒ\"\n",
    "    print(f\"  {has_tag} {summary[:100]}...\")\n",
    "\n",
    "# ê¸°ì¡´ output_k1.csvì™€ ë¹„êµ\n",
    "print(\"\\nğŸ“‹ ê¸°ì¡´ ê²°ê³¼(output_k1.csv)ì™€ ë¹„êµ:\")\n",
    "old_output = pd.read_csv(\"./prediction/output_k1.csv\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n[Test {i}]\")\n",
    "    print(f\"  ê¸°ì¡´: {old_output['summary'].iloc[i][:80]}...\")\n",
    "    print(f\"  ìƒˆë¡œ: {submission['summary'].iloc[i][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“ ìš”ì•½ë¬¸ ê¸¸ì´ ìµœì í™” (Phase 7)\n",
    "\n",
    "### ë¬¸ì œì :\n",
    "- í˜„ì¬ ìƒì„± ìš”ì•½ë¬¸ í‰ê·  ê¸¸ì´: **75.0ì**\n",
    "- í•™ìŠµ ë°ì´í„° ì •ë‹µ ìš”ì•½ë¬¸ í‰ê·  ê¸¸ì´: **85.8ì**  \n",
    "- ì°¨ì´: **ì•½ 10.8ì ë¶€ì¡±**\n",
    "\n",
    "### í•´ê²° ë°©ì•ˆ:\n",
    "1. `max_new_tokens` ì¦ê°€ (60/80/100 â†’ 80/100/128)\n",
    "2. `min_length` íŒŒë¼ë¯¸í„° ì¶”ê°€ (ìµœì†Œ ê¸¸ì´ ë³´ì¥)\n",
    "3. `length_penalty` ì¦ê°€ (ê¸´ ìš”ì•½ ìœ ë„, 1.0 â†’ 1.2~1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“ ê¸¸ì´ ìµœì í™” ë²„ì „ ì¶”ë¡  ì„¤ì • (Phase 7)\n",
      "============================================================\n",
      "\n",
      "ê¸°ì¡´ vs ê°œì„  ë¹„êµ:\n",
      "ì¹´í…Œê³ ë¦¬       ê¸°ì¡´ max_tokens   ê°œì„  max_tokens   min_length   length_penalty\n",
      "------------------------------------------------------------------\n",
      "short      60              80              30           1.3           \n",
      "medium     80              100             40           1.4           \n",
      "long       100             128             50           1.5           \n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Phase 7: ìš”ì•½ë¬¸ ê¸¸ì´ ìµœì í™” ë²„ì „\n",
    "# ============================================================================\n",
    "\n",
    "# ê°œì„ ëœ ê¸¸ì´ ê¸°ë°˜ ì¶”ë¡  ì„¤ì • (ë” ê¸´ ìš”ì•½ ìƒì„±)\n",
    "LENGTH_OPTIMIZED_INFERENCE_CONFIG = {\n",
    "    # ì§§ì€ ëŒ€í™” (500ì ì´í•˜) - ê¸°ì¡´ 60 â†’ 80\n",
    "    \"short\": {\n",
    "        \"char_threshold\": 500,\n",
    "        \"max_new_tokens\": 80,\n",
    "        \"min_length\": 30,\n",
    "        \"num_beams\": 4,\n",
    "        \"length_penalty\": 1.3,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "    },\n",
    "    # ì¤‘ê°„ ëŒ€í™” (500~1500ì) - ê¸°ì¡´ 80 â†’ 100\n",
    "    \"medium\": {\n",
    "        \"char_threshold\": 1500,\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"min_length\": 40,\n",
    "        \"num_beams\": 5,\n",
    "        \"length_penalty\": 1.4,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "    },\n",
    "    # ê¸´ ëŒ€í™” (1500ì ì´ˆê³¼) - ê¸°ì¡´ 100 â†’ 128\n",
    "    \"long\": {\n",
    "        \"char_threshold\": float('inf'),\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"min_length\": 50,\n",
    "        \"num_beams\": 6,\n",
    "        \"length_penalty\": 1.5,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "    },\n",
    "}\n",
    "\n",
    "# ì£¼ì œë³„ ìµœì†Œ ê¸¸ì´ ì„¤ì • ì¶”ê°€\n",
    "TOPIC_LENGTH_CONFIG = {\n",
    "    \"medical\": {\"max_new_tokens\": 128, \"min_length\": 50, \"length_penalty\": 1.5},\n",
    "    \"insurance\": {\"max_new_tokens\": 128, \"min_length\": 50, \"length_penalty\": 1.5},\n",
    "    \"finance\": {\"max_new_tokens\": 110, \"min_length\": 45, \"length_penalty\": 1.4},\n",
    "    \"travel\": {\"max_new_tokens\": 100, \"min_length\": 40, \"length_penalty\": 1.3},\n",
    "    \"default\": {\"max_new_tokens\": 90, \"min_length\": 35, \"length_penalty\": 1.2},\n",
    "}\n",
    "\n",
    "def get_length_category_v2(dialogue_text):\n",
    "    \"\"\"ëŒ€í™” ê¸¸ì´ ì¹´í…Œê³ ë¦¬ ë°˜í™˜ (v2)\"\"\"\n",
    "    length = len(dialogue_text)\n",
    "    \n",
    "    if length <= LENGTH_OPTIMIZED_INFERENCE_CONFIG[\"short\"][\"char_threshold\"]:\n",
    "        return \"short\"\n",
    "    elif length <= LENGTH_OPTIMIZED_INFERENCE_CONFIG[\"medium\"][\"char_threshold\"]:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"long\"\n",
    "\n",
    "def get_dynamic_inference_config_v2(dialogue_text, use_topic=True, use_length=True):\n",
    "    \"\"\"\n",
    "    ê°œì„ ëœ ë™ì  ì¶”ë¡  ì„¤ì • ë°˜í™˜ (ê¸¸ì´ ìµœì í™” ë²„ì „)\n",
    "    - min_length ì¶”ê°€\n",
    "    - length_penalty ì¦ê°€\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"max_new_tokens\": 90,\n",
    "        \"min_length\": 35,\n",
    "        \"num_beams\": 5,\n",
    "        \"length_penalty\": 1.2,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "    }\n",
    "    \n",
    "    # 1. ê¸¸ì´ ê¸°ë°˜ ì„¤ì •\n",
    "    if use_length:\n",
    "        length_cat = get_length_category_v2(dialogue_text)\n",
    "        length_config = LENGTH_OPTIMIZED_INFERENCE_CONFIG[length_cat]\n",
    "        config.update({\n",
    "            \"max_new_tokens\": length_config[\"max_new_tokens\"],\n",
    "            \"min_length\": length_config[\"min_length\"],\n",
    "            \"num_beams\": length_config[\"num_beams\"],\n",
    "            \"length_penalty\": length_config[\"length_penalty\"],\n",
    "            \"no_repeat_ngram_size\": length_config[\"no_repeat_ngram_size\"],\n",
    "        })\n",
    "    \n",
    "    # 2. ì£¼ì œ ê¸°ë°˜ ì„¤ì • (ê¸¸ì´ë³´ë‹¤ ìš°ì„ )\n",
    "    if use_topic:\n",
    "        topic = detect_topic(dialogue_text)\n",
    "        if topic != \"default\":\n",
    "            topic_config = TOPIC_LENGTH_CONFIG[topic]\n",
    "            config[\"max_new_tokens\"] = max(config[\"max_new_tokens\"], topic_config[\"max_new_tokens\"])\n",
    "            config[\"min_length\"] = max(config[\"min_length\"], topic_config[\"min_length\"])\n",
    "            config[\"length_penalty\"] = max(config[\"length_penalty\"], topic_config[\"length_penalty\"])\n",
    "    \n",
    "    return config\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“ ê¸¸ì´ ìµœì í™” ë²„ì „ ì¶”ë¡  ì„¤ì • (Phase 7)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nê¸°ì¡´ vs ê°œì„  ë¹„êµ:\")\n",
    "print(f\"{'ì¹´í…Œê³ ë¦¬':<10} {'ê¸°ì¡´ max_tokens':<15} {'ê°œì„  max_tokens':<15} {'min_length':<12} {'length_penalty':<14}\")\n",
    "print(\"-\" * 66)\n",
    "\n",
    "old_config = {\n",
    "    \"short\": {\"max_new_tokens\": 60, \"length_penalty\": 1.0},\n",
    "    \"medium\": {\"max_new_tokens\": 80, \"length_penalty\": 1.1},\n",
    "    \"long\": {\"max_new_tokens\": 100, \"length_penalty\": 1.2},\n",
    "}\n",
    "\n",
    "for cat in [\"short\", \"medium\", \"long\"]:\n",
    "    new_cfg = LENGTH_OPTIMIZED_INFERENCE_CONFIG[cat]\n",
    "    old_cfg = old_config[cat]\n",
    "    print(f\"{cat:<10} {old_cfg['max_new_tokens']:<15} {new_cfg['max_new_tokens']:<15} {new_cfg['min_length']:<12} {new_cfg['length_penalty']:<14}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… inference_length_optimized() í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Phase 7: ê¸¸ì´ ìµœì í™” inference í•¨ìˆ˜\n",
    "# ============================================================================\n",
    "\n",
    "def inference_length_optimized(config):\n",
    "    \"\"\"\n",
    "    ê¸¸ì´ ìµœì í™” ë²„ì „ ì¶”ë¡  í•¨ìˆ˜\n",
    "    - min_length íŒŒë¼ë¯¸í„° ì¶”ê°€\n",
    "    - length_penalty ì¦ê°€\n",
    "    - max_new_tokens ì¦ê°€\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print('-' * 10, f'device : {device}', '-' * 10)\n",
    "\n",
    "    generate_model, tokenizer = load_tokenizer_and_model_for_test(config, device)\n",
    "\n",
    "    # ì„¤ì • ë¡œë“œ\n",
    "    prompt_config = config.get('prompt', {\n",
    "        \"use_prompt\": False,\n",
    "        \"prompt_style\": \"none\",\n",
    "        \"use_turn_separator\": False,\n",
    "        \"turn_separator\": \"[SEP]\",\n",
    "        \"tfidf_top_k\": 5,\n",
    "    })\n",
    "    \n",
    "    preprocess_config = config.get('preprocess', {\n",
    "        \"normalize_slang\": False,\n",
    "    })\n",
    "    \n",
    "    regex_pattern_config = config.get('regex_pattern', {\n",
    "        \"use_speaker_count\": True,\n",
    "        \"use_pii_instruction\": True,\n",
    "        \"show_pii_examples\": True,\n",
    "    })\n",
    "    \n",
    "    # 5-1. TF-IDF extractor ì‚¬ìš© ì—¬ë¶€ í™•ì¸\n",
    "    use_tfidf = prompt_config.get('prompt_style') in ['keyword', 'keyword_aware']\n",
    "    tfidf_ext = tfidf_extractor if use_tfidf else None\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ ì¶”ë¡  ì„¤ì •:\")\n",
    "    print(f\"   - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: {prompt_config.get('prompt_style', 'none')}\")\n",
    "    print(f\"   - ìŠ¬ë­ ì •ê·œí™”: {preprocess_config.get('normalize_slang', False)}\")\n",
    "    print(f\"   - ê¸¸ì´ ìµœì í™”: âœ… (min_length + length_penalty ì¦ê°€)\")\n",
    "    \n",
    "    preprocessor = Preprocess(\n",
    "        config['tokenizer']['bos_token'], \n",
    "        config['tokenizer']['eos_token'],\n",
    "        prompt_config=prompt_config,\n",
    "        preprocess_config=preprocess_config,\n",
    "        tfidf_extractor=tfidf_ext,\n",
    "        regex_pattern_config=regex_pattern_config\n",
    "    )\n",
    "\n",
    "    data_path = config['general']['data_path']\n",
    "    test_data, test_encoder_inputs_dataset = prepare_test_dataset(config, preprocessor, tokenizer)\n",
    "    \n",
    "    dataloader = DataLoader(test_encoder_inputs_dataset, batch_size=config['inference']['batch_size'])\n",
    "\n",
    "    summary = []\n",
    "    text_ids = []\n",
    "    \n",
    "    # ê¸¸ì´ ìµœì í™”: ë™ì  ì¶”ë¡  ì‚¬ìš©\n",
    "    print(\"\\nğŸš€ ê¸¸ì´ ìµœì í™” ëª¨ë“œë¡œ ìƒì„± ì‹œì‘...\")\n",
    "    sample_idx = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for item in tqdm(dataloader):\n",
    "            batch_size = len(item['ID'])\n",
    "            text_ids.extend(item['ID'])\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                # ê°œë³„ ìƒ˜í”Œì— ëŒ€í•œ ê¸¸ì´ ìµœì í™” ì„¤ì • ê³„ì‚°\n",
    "                dialogue_text = test_data.iloc[sample_idx]['dialogue']\n",
    "                dynamic_cfg = get_dynamic_inference_config_v2(dialogue_text, use_topic=True, use_length=True)\n",
    "                \n",
    "                # ë‹¨ì¼ ìƒ˜í”Œ ìƒì„± (min_length ì¶”ê°€!)\n",
    "                single_input_ids = item['input_ids'][i:i+1].to(device)\n",
    "                generated_ids = generate_model.generate(\n",
    "                    input_ids=single_input_ids,\n",
    "                    no_repeat_ngram_size=dynamic_cfg['no_repeat_ngram_size'],\n",
    "                    early_stopping=config['inference']['early_stopping'],\n",
    "                    max_length=dynamic_cfg['max_new_tokens'],\n",
    "                    min_length=dynamic_cfg['min_length'],  # ìµœì†Œ ê¸¸ì´ ì¶”ê°€!\n",
    "                    num_beams=dynamic_cfg['num_beams'],\n",
    "                    length_penalty=dynamic_cfg['length_penalty'],  # ì¦ê°€ëœ length_penalty\n",
    "                )\n",
    "                \n",
    "                # í™”ì íƒœê·¸ ë³´ì¡´í•˜ë©´ì„œ ë””ì½”ë”©\n",
    "                result = decode_with_speaker_tags(tokenizer, generated_ids[0])\n",
    "                summary.append(result)\n",
    "                sample_idx += 1\n",
    "\n",
    "    # í›„ì²˜ë¦¬ ì ìš© (ë„ì–´ì“°ê¸° êµì •)\n",
    "    preprocessed_summary = [postprocess_summary(s) for s in summary]\n",
    "\n",
    "    output = pd.DataFrame({\n",
    "        \"fname\": test_data['fname'],\n",
    "        \"summary\": preprocessed_summary,\n",
    "    })\n",
    "    \n",
    "    result_path = config['inference']['result_path']\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    output.to_csv(os.path.join(result_path, \"output.csv\"), index=False)\n",
    "    \n",
    "    # ê¸¸ì´ í†µê³„ ì¶œë ¥\n",
    "    lengths = output['summary'].str.len()\n",
    "    print(f\"\\nğŸ“Š ìƒì„± ìš”ì•½ë¬¸ ê¸¸ì´ í†µê³„:\")\n",
    "    print(f\"   í‰ê· : {lengths.mean():.1f}ì\")\n",
    "    print(f\"   ìµœì†Œ: {lengths.min()}ì\")\n",
    "    print(f\"   ìµœëŒ€: {lengths.max()}ì\")\n",
    "    print(f\"   ì¤‘ì•™ê°’: {lengths.median():.1f}ì\")\n",
    "\n",
    "    return output\n",
    "\n",
    "print(\"âœ… inference_length_optimized() í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================================================\n",
    "ğŸ“¤ ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± (í™”ì íƒœê·¸ ë³´ì¡´)\n",
    "================================================================================\n",
    "ğŸ“Š Test ë°ì´í„°: 499ê°œ\n",
    "\n",
    "ğŸš€ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\n",
    "ì¶”ë¡ : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [01:32<00:00,  5.41it/s]\n",
    "âœ… ì €ì¥ ì™„ë£Œ: ./prediction/submit_optimized_v2.csv\n",
    "   í‰ê·  ê¸¸ì´: 81.7ì\n",
    "   í™”ì íƒœê·¸ í¬í•¨: 434/499ê°œ (87.0%)\n",
    "\n",
    "ğŸ“‹ ì œì¶œ ìƒ˜í”Œ (í™”ì íƒœê·¸ í™•ì¸):\n",
    "\n",
    "[Test 0]\n",
    "  âœ… #Person1# ì€ Ms. Dawsonì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¥¼ ì‘ì„±í•˜ê³  ë°°í¬í•´ ë‹¬ë¼ê³  ìš”ì²­í•©ë‹ˆë‹¤....\n",
    "\n",
    "[Test 1]\n",
    "  âœ… #Person2# ëŠ” #Person1#ì—ê²Œ ì¶œí‡´ê·¼ ì‹œê°„ì— êµí†µ ì²´ì¦ì„ í”¼í•˜ê¸° ìœ„í•´ ëŒ€ì¤‘êµí†µì„ ì´ìš©í•˜ë¼ê³  ì œì•ˆí•©ë‹ˆë‹¤. #Person1# ì€ ëŒ€ì¤‘êµí†µì„ ì´ìš©í•˜ëŠ” ê²ƒì´ í™˜ê²½ì—ë„ ë” ì¢‹ë‹¤...\n",
    "\n",
    "[Test 2]\n",
    "  âœ… #Person1# ì€ Kateì—ê²Œ Mashaì™€ Heroê°€ ë‘ ë‹¬ ë™ì•ˆ ë³„ê±°í•˜ë‹¤ê°€ ê²°êµ­ ì´í˜¼í–ˆë‹¤ê³  ì „í•œë‹¤....\n",
    "\n",
    "[Test 3]\n",
    "  âœ… #Person1# ì€ Brianì—ê²Œ ìƒì¼ ì„ ë¬¼ë¡œ ëª©ê±¸ì´ë¥¼ ì£¼ë©°, íŒŒí‹°ì—ì„œ í•¨ê»˜ ì¶¤ì„ ì¶”ìê³  ì œì•ˆí•©ë‹ˆë‹¤....\n",
    "\n",
    "[Test 4]\n",
    "  âœ… #Person1# ê³¼ #Person2# ëŠ” ì˜¬ë¦¼í”½ ê³µì›ì˜ í¬ê¸°ì™€ ì‹œì„¤ì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ê³  ìˆìŠµë‹ˆë‹¤....\n",
    "\n",
    "ğŸ“‹ ê¸°ì¡´ ê²°ê³¼(output_k1.csv)ì™€ ë¹„êµ:\n",
    "\n",
    "[Test 0]\n",
    "  ê¸°ì¡´:  #Person1# ì€ Ms. Dawsonì—ê²Œ ì´ë©”ì¼ê³¼ ê³µì‹ ë©”ëª¨ë§Œ ì œí•œëœë‹¤ê³  ì•Œë¦½ë‹ˆë‹¤. ì´ëŠ” ì‚¬ë‚´ í†µì‹ ì—ë§Œ ì ìš©ë˜ë©°, ì™¸ë¶€ í†µì‹ ì—ë„ ì ìš©ë©ë‹ˆë‹¤...\n",
    "...\n",
    "\n",
    "[Test 2]\n",
    "  ê¸°ì¡´:  #Person1# ì€ Kateì—ê²Œ Mashaì™€ Heroê°€ ë‘ ë‹¬ ë™ì•ˆ ë³„ê±°í•˜ë‹¤ê°€ ê²°êµ­ ì´í˜¼í–ˆë‹¤ê³  ì „í•©ë‹ˆë‹¤. ê·¸ë“¤ì€ ìë…€ ì–‘ìœ¡ì— ëŒ€í•´ ì´ì•¼ê¸°í•©ë‹ˆ...\n",
    "  ìƒˆë¡œ: #Person1# ì€ Kateì—ê²Œ Mashaì™€ Heroê°€ ë‘ ë‹¬ ë™ì•ˆ ë³„ê±°í•˜ë‹¤ê°€ ê²°êµ­ ì´í˜¼í–ˆë‹¤ê³  ì „í•œë‹¤....\n",
    "Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ Phase 1-B ì œì¶œ íŒŒì¼ ìƒì„±: BASE_GEN_CONFIG ì ìš©, í›„ì²˜ë¦¬ ì ìš©\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì¶”ë¡ : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [01:23<00:00,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: ./prediction/submit_phase1b.csv\n",
      "   í‰ê·  ê¸¸ì´: 73.4ì\n",
      "   í™”ì íƒœê·¸ í¬í•¨: 360/499ê°œ (72.1%)\n",
      "â–¶ ë‘ íŒŒì¼ ìƒì„±: ./prediction/submit_phase1b.csv, ./prediction/submit_optimized_v2_.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# â–¶ Phase 1-B ì œì¶œ íŒŒì¼ ìƒì„± (BASE_GEN_CONFIG + í›„ì²˜ë¦¬)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"â–¶ Phase 1-B ì œì¶œ íŒŒì¼ ìƒì„±: BASE_GEN_CONFIG ì ìš©, í›„ì²˜ë¦¬ ì ìš©\")\n",
    "\n",
    "import shutil\n",
    "\n",
    "# BASE_GEN_CONFIG ëŠ” Phase 1-Aì—ì„œ ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì—†ìœ¼ë©´ inference ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "try:\n",
    "    gen_cfg = BASE_GEN_CONFIG\n",
    "except NameError:\n",
    "    gen_cfg = {\n",
    "        \"num_beams\": loaded_config['inference']['num_beams'],\n",
    "        \"no_repeat_ngram_size\": loaded_config['inference']['no_repeat_ngram_size'],\n",
    "        \"max_length\": loaded_config['inference']['generate_max_length'],\n",
    "        \"early_stopping\": loaded_config['inference']['early_stopping'],\n",
    "    }\n",
    "\n",
    "# ìƒì„± í•¨ìˆ˜ê°€ ì´ë¯¸ ë…¸íŠ¸ë¶ì— ìˆìœ¼ë¯€ë¡œ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "# íŒŒì¼ëª…: Phase1-B ì „ìš©\n",
    "out_path = \"./prediction/submit_phase1b.csv\"\n",
    "\n",
    "submission_phase1b = generate_final_submission(eval_model, eval_tokenizer, test_df, gen_cfg, out_path)\n",
    "\n",
    "# ì¶”ê°€ë¡œ ìš”ì²­ì í˜¼ë™ì„ ì¤„ì´ê¸° ìœ„í•´ ê°™ì€ ë‚´ìš©ì„ '_v2_.csv' ë¼ëŠ” ì´ë¦„ìœ¼ë¡œë„ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "shutil.copy(out_path, \"./prediction/submit_optimized_v2_.csv\")\n",
    "print(f\"â–¶ ë‘ íŒŒì¼ ìƒì„±: {out_path}, ./prediction/submit_optimized_v2_.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ”¬ Phase 5: TF-IDF í‚¤ì›Œë“œ í”„ë¡¬í”„íŠ¸ + ë™ì  ì¶”ë¡  ì‹¤í—˜\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Step 1: TF-IDF Vectorizer í•™ìŠµ ì¤‘...\n",
      "   - í•™ìŠµ ë°ì´í„° ìˆ˜: 12457ê°œ\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TFIDF_CONFIG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# TF-IDF Vectorizer í•™ìŠµ\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   - í•™ìŠµ ë°ì´í„° ìˆ˜: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dialogues)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mê°œ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   - TF-IDF ì„¤ì •: min_df=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTFIDF_CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_df\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, max_df=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTFIDF_CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_df\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Kiwi ê¸°ë°˜ í† í¬ë‚˜ì´ì €ë¡œ í•™ìŠµ\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TFIDF_CONFIG' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ”¬ Phase 5: TF-IDF í‚¤ì›Œë“œ í”„ë¡¬í”„íŠ¸ + ë™ì  ì¶”ë¡  ì‹¤í—˜\n",
    "# ============================================================================\n",
    "# ëª©í‘œ: TF-IDF ê¸°ë°˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€í•˜ê³ , ëŒ€í™” ê¸¸ì´ì— ë”°ë¼ \n",
    "#       ì¶”ë¡  ì„¤ì •ì„ ë™ì ìœ¼ë¡œ ì¡°ì ˆí•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ”¬ Phase 5: TF-IDF í‚¤ì›Œë“œ í”„ë¡¬í”„íŠ¸ + ë™ì  ì¶”ë¡  ì‹¤í—˜\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# Step 1: TF-IDF Vectorizer í•™ìŠµ (í•™ìŠµ ë°ì´í„° ê¸°ë°˜)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nğŸ“Š Step 1: TF-IDF Vectorizer í•™ìŠµ ì¤‘...\")\n",
    "\n",
    "# Train ë°ì´í„° ë¡œë“œ ë° ëŒ€í™”ë¬¸ ì¶”ì¶œ\n",
    "train_df_tfidf = pd.read_csv(\"./data/train.csv\")\n",
    "train_dialogues = train_df_tfidf['dialogue'].tolist()\n",
    "\n",
    "# TF-IDF Vectorizer í•™ìŠµ\n",
    "print(f\"   - í•™ìŠµ ë°ì´í„° ìˆ˜: {len(train_dialogues)}ê°œ\")\n",
    "print(f\"   - TF-IDF ì„¤ì •: min_df={TFIDF_CONFIG['min_df']}, max_df={TFIDF_CONFIG['max_df']}\")\n",
    "\n",
    "# Kiwi ê¸°ë°˜ í† í¬ë‚˜ì´ì €ë¡œ í•™ìŠµ\n",
    "try:\n",
    "    tfidf_extractor.fit(train_dialogues)\n",
    "    print(f\"   âœ… TF-IDF Vectorizer í•™ìŠµ ì™„ë£Œ\")\n",
    "    print(f\"   - Vocabulary í¬ê¸°: {len(tfidf_extractor.vectorizer.vocabulary_)}ê°œ\")\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ TF-IDF í•™ìŠµ ì‹¤íŒ¨: {e}\")\n",
    "    tfidf_extractor = None\n",
    "\n",
    "# =============================================================================\n",
    "# Step 2: ìƒ˜í”Œ ëŒ€í™”ë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nğŸ” Step 2: TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸\")\n",
    "\n",
    "if tfidf_extractor:\n",
    "    sample_dialogues = train_df_tfidf['dialogue'].head(5).tolist()\n",
    "    sample_summaries = train_df_tfidf['summary'].head(5).tolist()\n",
    "    \n",
    "    for i, (dialogue, summary) in enumerate(zip(sample_dialogues, sample_summaries)):\n",
    "        keywords = extract_tfidf_keywords(dialogue, tfidf_extractor, top_k=5)\n",
    "        print(f\"\\n[Sample {i+1}]\")\n",
    "        print(f\"  ëŒ€í™”ë¬¸(ì¼ë¶€): {dialogue[:80]}...\")\n",
    "        print(f\"  ìš”ì•½ë¬¸: {summary[:60]}...\")\n",
    "        print(f\"  TF-IDF í‚¤ì›Œë“œ: {keywords}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 3: ë™ì  ì¶”ë¡  ì„¤ì • í…ŒìŠ¤íŠ¸\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\\nâš™ï¸ Step 3: ë™ì  ì¶”ë¡  ì„¤ì • í…ŒìŠ¤íŠ¸\")\n",
    "\n",
    "# Test ë°ì´í„°ì˜ ëŒ€í™”ë¬¸ ê¸¸ì´ ë¶„í¬ í™•ì¸\n",
    "test_df_phase5 = pd.read_csv(\"./data/test.csv\")\n",
    "test_dialogue_lengths = [len(d) for d in test_df_phase5['dialogue'].tolist()]\n",
    "\n",
    "short_count = sum(1 for l in test_dialogue_lengths if l <= 500)\n",
    "medium_count = sum(1 for l in test_dialogue_lengths if 500 < l <= 1000)\n",
    "long_count = sum(1 for l in test_dialogue_lengths if l > 1000)\n",
    "\n",
    "print(f\"\\nğŸ“ Test ë°ì´í„° ëŒ€í™”ë¬¸ ê¸¸ì´ ë¶„í¬:\")\n",
    "print(f\"   - Short (â‰¤500ì): {short_count}ê°œ ({short_count/len(test_dialogue_lengths)*100:.1f}%)\")\n",
    "print(f\"   - Medium (501-1000ì): {medium_count}ê°œ ({medium_count/len(test_dialogue_lengths)*100:.1f}%)\")\n",
    "print(f\"   - Long (>1000ì): {long_count}ê°œ ({long_count/len(test_dialogue_lengths)*100:.1f}%)\")\n",
    "\n",
    "# ê° ê¸¸ì´ ë²”ì£¼ë³„ ë™ì  ì„¤ì • í™•ì¸\n",
    "print(\"\\nğŸ”§ ê¸¸ì´ë³„ ë™ì  ì¶”ë¡  ì„¤ì •:\")\n",
    "for test_len in [300, 700, 1500]:\n",
    "    config = get_dynamic_inference_config(test_len, DYNAMIC_INFERENCE_CONFIG)\n",
    "    print(f\"   - {test_len}ì â†’ max_tokens={config['max_new_tokens']}, num_beams={config['num_beams']}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 4: ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ ë¹„êµ (Dev ë°ì´í„° ê¸°ì¤€)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\\nğŸ“ Step 4: í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ ë³„ ì„±ëŠ¥ ë¹„êµ (Dev ë°ì´í„° 5ìƒ˜í”Œ)\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸í•  í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼\n",
    "prompt_styles = [\n",
    "    {\"name\": \"none\", \"use_prompt\": False, \"prompt_style\": \"none\"},\n",
    "    {\"name\": \"default\", \"use_prompt\": True, \"prompt_style\": \"default\"},\n",
    "    {\"name\": \"speaker_aware\", \"use_prompt\": True, \"prompt_style\": \"speaker_aware\"},\n",
    "    {\"name\": \"keyword (TF-IDF)\", \"use_prompt\": True, \"prompt_style\": \"keyword\", \"tfidf_top_k\": 5},\n",
    "    {\"name\": \"keyword_aware (Speaker+TF-IDF)\", \"use_prompt\": True, \"prompt_style\": \"keyword_aware\", \"tfidf_top_k\": 5},\n",
    "]\n",
    "\n",
    "dev_df_phase5 = pd.read_csv(\"./data/dev.csv\")\n",
    "sample_dev = dev_df_phase5.head(5)\n",
    "\n",
    "print(f\"\\ní…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(sample_dev)}ê°œ\")\n",
    "\n",
    "# ê° í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ë³„ ì „ì²˜ë¦¬ ê²°ê³¼ ë¹„êµ\n",
    "for style_config in prompt_styles:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ”¹ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: {style_config['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Preprocessor ìƒì„±\n",
    "    preprocessor = Preprocess(\n",
    "        loaded_config['tokenizer']['bos_token'],\n",
    "        loaded_config['tokenizer']['eos_token'],\n",
    "        prompt_config=style_config,\n",
    "        preprocess_config={\"normalize_slang\": True},  # ìŠ¬ë­ ì •ê·œí™” í™œì„±í™”\n",
    "        tfidf_extractor=tfidf_extractor if 'keyword' in style_config.get('prompt_style', '') else None\n",
    "    )\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ ìƒ˜í”Œë¡œ ì „ì²˜ë¦¬ ê²°ê³¼ í™•ì¸\n",
    "    sample_dialogue = sample_dev.iloc[0]['dialogue']\n",
    "    processed = preprocessor.full_preprocess_pipeline(sample_dialogue)\n",
    "    \n",
    "    print(f\"\\n[ì›ë³¸ ëŒ€í™”ë¬¸ (ì¼ë¶€)]\")\n",
    "    print(f\"  {sample_dialogue[:100]}...\")\n",
    "    print(f\"\\n[ì „ì²˜ë¦¬ ê²°ê³¼]\")\n",
    "    print(f\"  {processed[:150]}...\")\n",
    "    print(f\"\\n  ê¸¸ì´: ì›ë³¸={len(sample_dialogue)}ì â†’ ì „ì²˜ë¦¬={len(processed)}ì\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… Phase 5 ì„¤ì • ì™„ë£Œ!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "ë‹¤ìŒ ë‹¨ê³„:\n",
    "1. ìœ„ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ ì¤‘ ê°€ì¥ ì í•©í•œ ê²ƒ ì„ íƒ\n",
    "2. configì— ì ìš©í•˜ì—¬ ì¶”ë¡  ì‹¤í–‰\n",
    "3. ì œì¶œ íŒŒì¼ ìƒì„± ë° ë¦¬ë”ë³´ë“œ í‰ê°€\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“¤ Phase 5 ì œì¶œ íŒŒì¼ ìƒì„±: TF-IDF í‚¤ì›Œë“œ + ë™ì  ì¶”ë¡ \n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“¤ Phase 5 ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Phase 5 ì„¤ì •\n",
    "PHASE5_CONFIG = {\n",
    "    # í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "    \"prompt\": {\n",
    "        \"use_prompt\": True,\n",
    "        \"prompt_style\": \"keyword_aware\",  # Speaker + TF-IDF í‚¤ì›Œë“œ\n",
    "        \"use_turn_separator\": True,\n",
    "        \"turn_separator\": \" [SEP] \",\n",
    "        \"tfidf_top_k\": 5,\n",
    "    },\n",
    "    # êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •\n",
    "    \"preprocess\": {\n",
    "        \"normalize_slang\": True,  # ìŠ¬ë­ ì •ê·œí™” í™œì„±í™”\n",
    "    },\n",
    "    # ë™ì  ì¶”ë¡  ì„¤ì •\n",
    "    \"dynamic_inference\": {\n",
    "        \"use_dynamic_settings\": False,  # ë¨¼ì € ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸\n",
    "        \"length_thresholds\": {\n",
    "            \"short\": {\"max_chars\": 500, \"max_new_tokens\": 80, \"num_beams\": 4},\n",
    "            \"medium\": {\"max_chars\": 1000, \"max_new_tokens\": 100, \"num_beams\": 5},\n",
    "            \"long\": {\"max_chars\": float('inf'), \"max_new_tokens\": 120, \"num_beams\": 6},\n",
    "        }\n",
    "    },\n",
    "    # ìƒì„± ì„¤ì • (Grid Search ìµœì ê°’)\n",
    "    \"generation\": {\n",
    "        \"num_beams\": 7,\n",
    "        \"length_penalty\": 1.0,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"max_length\": 100,\n",
    "        \"early_stopping\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ“‹ Phase 5 ì„¤ì •:\")\n",
    "print(f\"   í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: {PHASE5_CONFIG['prompt']['prompt_style']}\")\n",
    "print(f\"   TF-IDF í‚¤ì›Œë“œ: {PHASE5_CONFIG['prompt']['tfidf_top_k']}ê°œ\")\n",
    "print(f\"   ìŠ¬ë­ ì •ê·œí™”: {PHASE5_CONFIG['preprocess']['normalize_slang']}\")\n",
    "print(f\"   ë™ì  ì¶”ë¡ : {PHASE5_CONFIG['dynamic_inference']['use_dynamic_settings']}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ì œì¶œ íŒŒì¼ ìƒì„± í•¨ìˆ˜\n",
    "# =============================================================================\n",
    "\n",
    "def generate_phase5_submission(model, tokenizer, test_df, config, output_path):\n",
    "    \"\"\"Phase 5 ì„¤ì •ìœ¼ë¡œ ì œì¶œ íŒŒì¼ ìƒì„±\"\"\"\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Preprocessor ìƒì„±\n",
    "    preprocessor = Preprocess(\n",
    "        loaded_config['tokenizer']['bos_token'],\n",
    "        loaded_config['tokenizer']['eos_token'],\n",
    "        prompt_config=config['prompt'],\n",
    "        preprocess_config=config['preprocess'],\n",
    "        tfidf_extractor=tfidf_extractor if 'keyword' in config['prompt'].get('prompt_style', '') else None\n",
    "    )\n",
    "    \n",
    "    gen_config = config['generation']\n",
    "    dynamic_config = config.get('dynamic_inference', {})\n",
    "    use_dynamic = dynamic_config.get('use_dynamic_settings', False)\n",
    "    \n",
    "    summaries = []\n",
    "    \n",
    "    # ëŒ€í™”ë¬¸ ê¸¸ì´ ê³„ì‚° (ë™ì  ì¶”ë¡ ìš©)\n",
    "    dialogue_lengths = [len(d) for d in test_df['dialogue'].tolist()]\n",
    "    \n",
    "    for idx in tqdm(range(len(test_df)), desc=\"ì¶”ë¡ \"):\n",
    "        dialogue = test_df.iloc[idx]['dialogue']\n",
    "        \n",
    "        # ì „ì²˜ë¦¬\n",
    "        processed_dialogue = preprocessor.full_preprocess_pipeline(dialogue)\n",
    "        \n",
    "        # í† í°í™”\n",
    "        inputs = tokenizer(\n",
    "            processed_dialogue,\n",
    "            max_length=loaded_config['tokenizer']['encoder_max_len'],\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # ë™ì  ì¶”ë¡  ì„¤ì •\n",
    "        if use_dynamic:\n",
    "            dyn_cfg = get_dynamic_inference_config(dialogue_lengths[idx], dynamic_config)\n",
    "            max_length = dyn_cfg['max_new_tokens']\n",
    "            num_beams = dyn_cfg['num_beams']\n",
    "        else:\n",
    "            max_length = gen_config['max_length']\n",
    "            num_beams = gen_config['num_beams']\n",
    "        \n",
    "        # ìƒì„±\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                num_beams=num_beams,\n",
    "                length_penalty=gen_config['length_penalty'],\n",
    "                repetition_penalty=gen_config['repetition_penalty'],\n",
    "                no_repeat_ngram_size=gen_config['no_repeat_ngram_size'],\n",
    "                max_length=max_length,\n",
    "                early_stopping=gen_config['early_stopping'],\n",
    "            )\n",
    "        \n",
    "        # ë””ì½”ë”© (í™”ì íƒœê·¸ ë³´ì¡´)\n",
    "        summary = decode_with_speaker_tags(tokenizer, generated_ids[0])\n",
    "        \n",
    "        # í›„ì²˜ë¦¬\n",
    "        summary = postprocess_summary(summary)\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    # ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "    submission = pd.DataFrame({\n",
    "        'fname': test_df['fname'],\n",
    "        'summary': summaries\n",
    "    })\n",
    "    \n",
    "    submission.to_csv(output_path, index=False)\n",
    "    print(f\"\\nâœ… ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "    print(f\"   í‰ê·  ê¸¸ì´: {np.mean([len(s) for s in summaries]):.1f}ì\")\n",
    "    \n",
    "    # í™”ì íƒœê·¸ í¬í•¨ ì—¬ë¶€ í™•ì¸\n",
    "    has_speaker_tags = sum(1 for s in summaries if '#Person' in s)\n",
    "    print(f\"   í™”ì íƒœê·¸ í¬í•¨: {has_speaker_tags}/{len(summaries)}ê°œ ({has_speaker_tags/len(summaries)*100:.1f}%)\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# =============================================================================\n",
    "# ì œì¶œ íŒŒì¼ ìƒì„± ì‹¤í–‰\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nğŸš€ Phase 5 ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\")\n",
    "\n",
    "# Test ë°ì´í„° ë¡œë“œ\n",
    "test_df_submit = pd.read_csv(\"./data/test.csv\")\n",
    "print(f\"ğŸ“Š Test ë°ì´í„°: {len(test_df_submit)}ê°œ\")\n",
    "\n",
    "# Phase 5 ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "submission_phase5 = generate_phase5_submission(\n",
    "    eval_model, eval_tokenizer, test_df_submit,\n",
    "    PHASE5_CONFIG,\n",
    "    \"./prediction/submit_phase5_tfidf.csv\"\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# ìƒ˜í”Œ ë¹„êµ ì¶œë ¥\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nğŸ“‹ Phase 5 ì œì¶œ ìƒ˜í”Œ:\")\n",
    "for i in range(5):\n",
    "    summary = submission_phase5['summary'].iloc[i]\n",
    "    has_tag = \"âœ…\" if \"#Person\" in summary else \"âŒ\"\n",
    "    print(f\"\\n[Test {i}]\")\n",
    "    print(f\"  {has_tag} {summary[:120]}...\")\n",
    "\n",
    "# ê¸°ì¡´ ìµœì í™” ì œì¶œê³¼ ë¹„êµ\n",
    "print(\"\\nğŸ“‹ ê¸°ì¡´ ìµœì í™” ì œì¶œ(submit_optimized_v2.csv)ê³¼ ë¹„êµ:\")\n",
    "try:\n",
    "    old_submission = pd.read_csv(\"./prediction/submit_optimized_v2.csv\")\n",
    "    for i in range(3):\n",
    "        print(f\"\\n[Test {i}]\")\n",
    "        print(f\"  ê¸°ì¡´: {old_submission['summary'].iloc[i][:80]}...\")\n",
    "        print(f\"  Phase5: {submission_phase5['summary'].iloc[i][:80]}...\")\n",
    "except:\n",
    "    print(\"   ê¸°ì¡´ ì œì¶œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… Phase 5 ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "ğŸ“¤ ì œì¶œ íŒŒì¼ ëª©ë¡:\n",
    "  - ./prediction/submit_phase5_tfidf.csv (TF-IDF í‚¤ì›Œë“œ + Speaker-aware í”„ë¡¬í”„íŠ¸)\n",
    "\n",
    "ğŸ” ë‹¤ìŒ ì‹¤í—˜ ì˜µì…˜:\n",
    "  1. ë™ì  ì¶”ë¡  í™œì„±í™” (use_dynamic_settings=True)\n",
    "  2. ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ í…ŒìŠ¤íŠ¸ (default, speaker_aware, keyword)\n",
    "  3. TF-IDF í‚¤ì›Œë“œ ìˆ˜ ì¡°ì ˆ (tfidf_top_k: 3, 5, 7)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š ì „ëµë³„ ê¸°ëŒ€ íš¨ê³¼ ìš”ì•½\n",
    "\n",
    "## ğŸ¯ ê° ì „ëµì˜ ì˜ˆìƒ ì ìˆ˜ í–¥ìƒ\n",
    "\n",
    "| ë‹¨ê³„ | ì „ëµ | ì˜ˆìƒ í–¥ìƒí­ | ëˆ„ì  ì˜ˆìƒ ì ìˆ˜ |\n",
    "|:---:|:---|:---:|:---:|\n",
    "| **Phase 1** | í˜•íƒœì†Œ ROUGE + í›„ì²˜ë¦¬ | +0.5~0.8 | 48.8~49.1 |\n",
    "| **Phase 2-A** | Grid Search ìµœì í™” | +0.3~0.5 | 49.1~49.6 |\n",
    "| **Phase 2-B** | N-Best Reranking | +0.3~0.8 | 49.4~50.4 |\n",
    "| **Phase 3** | ëª¨ë¸ ì•™ìƒë¸” | +0.5~1.0 | 49.9~51.4 |\n",
    "| **Phase 4** | Knowledge Distillation | +0.5~1.0 | 50.4~52.4 |\n",
    "\n",
    "## ğŸƒ ê¶Œì¥ ì‹¤í–‰ ìˆœì„œ\n",
    "\n",
    "1. **Phase 1**: ê¸°ë³¸ í‰ê°€ ë° í›„ì²˜ë¦¬ íš¨ê³¼ ì¸¡ì • (ë¹ ë¦„)\n",
    "2. **Phase 2-A**: Grid Searchë¡œ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰\n",
    "3. **Phase 2-B**: N-Best Reranking ì ìš© (ì¶”ë¡  ì‹œê°„ ì¦ê°€)\n",
    "4. **Phase 3**: ë‹¤ë¥¸ ëª¨ë¸ê³¼ ì•™ìƒë¸” (gogamza ëª¨ë¸ í•„ìš”)\n",
    "5. **Phase 4**: Distillation í•™ìŠµ (ì‹œê°„ ì†Œìš” ë†’ìŒ)\n",
    "\n",
    "## âš ï¸ ì£¼ì˜ì‚¬í•­\n",
    "\n",
    "- **GPU ë©”ëª¨ë¦¬**: N-Best, ì•™ìƒë¸”ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤\n",
    "- **í•™ìŠµ ì‹œê°„**: Distillationì€ ì¶”ê°€ í•™ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤\n",
    "- **ì¡°í•© íš¨ê³¼**: ëª¨ë“  ì „ëµì„ ì¡°í•©í•˜ë©´ ì‹œë„ˆì§€ íš¨ê³¼ê°€ ìˆìŠµë‹ˆë‹¤\n",
    "- **ê²€ì¦ í•„ìˆ˜**: ê° ë‹¨ê³„ë§ˆë‹¤ Dev ROUGEë¡œ íš¨ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”\n",
    "\n",
    "## ğŸ”§ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ\n",
    "\n",
    "```python\n",
    "# 1. ê°€ì¥ ë¹ ë¥¸ ê°œì„ : í›„ì²˜ë¦¬ë§Œ ì ìš©\n",
    "submission = postprocess_predictions(\"./prediction/output_k1.csv\")\n",
    "\n",
    "# 2. ì¤‘ê°„ ìˆ˜ì¤€ ê°œì„ : Grid Search + N-Best\n",
    "best_config = run_grid_search(model, tokenizer, dev_df)\n",
    "submission = generate_with_nbest(model, tokenizer, test_df, best_config)\n",
    "\n",
    "# 3. ìµœê³  ì„±ëŠ¥: ì•™ìƒë¸” + Distillation\n",
    "ensemble_submission = ensemble_inference(model1, model2, tokenizer, test_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ğŸ” Phase 6: ì •ê·œí‘œí˜„ì‹ ê¸°ë°˜ íŒ¨í„´ í™œìš© ì „ëµ ì‹¤í—˜\n",
    "# ============================================================================\n",
    "# ëª©í‘œ: í™”ì ìˆ˜ ëª…ì‹œ(6-1) + PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ(6-2)ë¡œ ROUGE ì ìˆ˜ í–¥ìƒ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ” Phase 6: ì •ê·œí‘œí˜„ì‹ ê¸°ë°˜ íŒ¨í„´ í™œìš© ì „ëµ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# Step 1: Train/Dev ë°ì´í„°ì—ì„œ í™”ì ìˆ˜ & PII ë§ˆìŠ¤í‚¹ ë¶„í¬ ë¶„ì„\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nğŸ“Š Step 1: ë°ì´í„° ë¶„ì„\")\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "train_df_phase6 = pd.read_csv(\"./data/train.csv\")\n",
    "dev_df_phase6 = pd.read_csv(\"./data/dev.csv\")\n",
    "test_df_phase6 = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "def analyze_dataset_patterns(df, name):\n",
    "    \"\"\"ë°ì´í„°ì…‹ì˜ í™”ì ìˆ˜ ë° PII ë§ˆìŠ¤í‚¹ íŒ¨í„´ ë¶„ì„\"\"\"\n",
    "    speaker_counts = []\n",
    "    pii_counts = []\n",
    "    pii_types = set()\n",
    "    \n",
    "    for dialogue in df['dialogue']:\n",
    "        analysis = analyze_dialogue_structure(dialogue)\n",
    "        speaker_counts.append(analysis['num_speakers'])\n",
    "        pii_counts.append(len(analysis['pii_masks']))\n",
    "        pii_types.update(analysis['pii_masks'])\n",
    "    \n",
    "    print(f\"\\nğŸ“Š {name} ë°ì´í„°ì…‹ ({len(df)}ê°œ):\")\n",
    "    print(f\"   í™”ì ìˆ˜ ë¶„í¬:\")\n",
    "    for num, count in sorted(pd.Series(speaker_counts).value_counts().items()):\n",
    "        print(f\"      {num}ëª…: {count}ê°œ ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    pii_exists = sum(1 for c in pii_counts if c > 0)\n",
    "    print(f\"   PII ë§ˆìŠ¤í‚¹ ì¡´ì¬: {pii_exists}/{len(df)}ê°œ ({pii_exists/len(df)*100:.1f}%)\")\n",
    "    print(f\"   PII ë§ˆìŠ¤í‚¹ ì¢…ë¥˜: {sorted(pii_types)}\")\n",
    "    \n",
    "    return speaker_counts, pii_counts, pii_types\n",
    "\n",
    "train_speakers, train_pii, train_pii_types = analyze_dataset_patterns(train_df_phase6, \"Train\")\n",
    "dev_speakers, dev_pii, dev_pii_types = analyze_dataset_patterns(dev_df_phase6, \"Dev\")\n",
    "test_speakers, test_pii, test_pii_types = analyze_dataset_patterns(test_df_phase6, \"Test\")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 2: 6-1, 6-2 í”„ë¡¬í”„íŠ¸ ìƒì„± ì˜ˆì‹œ\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“ Step 2: 6-1, 6-2 í”„ë¡¬í”„íŠ¸ ìƒì„± ì˜ˆì‹œ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# PII ë§ˆìŠ¤í‚¹ì´ ìˆëŠ” ìƒ˜í”Œ ì°¾ê¸°\n",
    "sample_with_pii = None\n",
    "for i, dialogue in enumerate(train_df_phase6['dialogue']):\n",
    "    analysis = analyze_dialogue_structure(dialogue)\n",
    "    if analysis['has_pii_masks']:\n",
    "        sample_with_pii = (i, dialogue, train_df_phase6['summary'].iloc[i])\n",
    "        break\n",
    "\n",
    "if sample_with_pii:\n",
    "    idx, dialogue, summary = sample_with_pii\n",
    "    analysis = analyze_dialogue_structure(dialogue)\n",
    "    \n",
    "    print(f\"\\n[ìƒ˜í”Œ {idx}] PII ë§ˆìŠ¤í‚¹ í¬í•¨ ëŒ€í™”:\")\n",
    "    print(f\"  ëŒ€í™”ë¬¸(ì• 200ì): {dialogue[:200]}...\")\n",
    "    print(f\"\\n  ğŸ” ëŒ€í™” êµ¬ì¡° ë¶„ì„:\")\n",
    "    print(f\"     - í™”ì ìˆ˜: {analysis['num_speakers']}ëª…\")\n",
    "    print(f\"     - PII ë§ˆìŠ¤í‚¹: {list(analysis['pii_masks'])}\")\n",
    "    \n",
    "    print(f\"\\n  ğŸ“ 6-1 í™”ì ìˆ˜ í”„ë¡¬í”„íŠ¸:\")\n",
    "    speaker_prompt = build_speaker_aware_prompt(analysis['num_speakers'])\n",
    "    print(f\"     \\\"{speaker_prompt}\\\"\")\n",
    "    \n",
    "    print(f\"\\n  ğŸ“ 6-2 PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ:\")\n",
    "    pii_prompt = build_pii_mask_instruction(analysis['pii_masks'], show_examples=True)\n",
    "    print(f\"     \\\"{pii_prompt}\\\"\")\n",
    "    \n",
    "    print(f\"\\n  ğŸ“ í†µí•© í”„ë¡¬í”„íŠ¸ (balanced ìŠ¤íƒ€ì¼):\")\n",
    "    full_prompt = build_summary_prompt(\n",
    "        dialogue,\n",
    "        prompt_style=\"balanced\",\n",
    "        use_speaker_count=True,\n",
    "        use_pii_instruction=True\n",
    "    )\n",
    "    print(f\"     {full_prompt[:300]}...\")\n",
    "    \n",
    "    print(f\"\\n  âœ… ì •ë‹µ ìš”ì•½ë¬¸:\")\n",
    "    print(f\"     {summary}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 3: í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ë³„ ë¹„êµ ì‹¤í—˜ (Dev 5ìƒ˜í”Œ)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ§ª Step 3: í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ë³„ ë¹„êµ (Dev 5ìƒ˜í”Œ)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ ì„¤ì •\n",
    "PROMPT_STYLES_PHASE6 = [\n",
    "    {\n",
    "        \"name\": \"baseline (none)\",\n",
    "        \"prompt_style\": \"none\",\n",
    "        \"use_speaker_count\": False,\n",
    "        \"use_pii_instruction\": False,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"6-1ë§Œ (í™”ì ìˆ˜)\",\n",
    "        \"prompt_style\": \"speaker_aware\",\n",
    "        \"use_speaker_count\": True,\n",
    "        \"use_pii_instruction\": False,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"6-2ë§Œ (PII ìœ ì§€ ì§€ì‹œ)\",\n",
    "        \"prompt_style\": \"default\",\n",
    "        \"use_speaker_count\": False,\n",
    "        \"use_pii_instruction\": True,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"6-1 + 6-2 (balanced)\",\n",
    "        \"prompt_style\": \"balanced\",\n",
    "        \"use_speaker_count\": True,\n",
    "        \"use_pii_instruction\": True,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"TF-IDF + 6-1 + 6-2 (keyword_aware)\",\n",
    "        \"prompt_style\": \"keyword_aware\",\n",
    "        \"use_speaker_count\": True,\n",
    "        \"use_pii_instruction\": True,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Dev ìƒ˜í”Œì—ì„œ PII ë§ˆìŠ¤í‚¹ ìˆëŠ” ê²ƒ ìš°ì„  ì„ íƒ\n",
    "dev_samples_phase6 = []\n",
    "for i, dialogue in enumerate(dev_df_phase6['dialogue']):\n",
    "    analysis = analyze_dialogue_structure(dialogue)\n",
    "    if analysis['has_pii_masks'] and len(dev_samples_phase6) < 3:\n",
    "        dev_samples_phase6.append(i)\n",
    "# ë‚˜ë¨¸ì§€ëŠ” ìˆœì„œëŒ€ë¡œ\n",
    "for i in range(len(dev_df_phase6)):\n",
    "    if i not in dev_samples_phase6 and len(dev_samples_phase6) < 5:\n",
    "        dev_samples_phase6.append(i)\n",
    "\n",
    "print(f\"\\nì„ íƒëœ Dev ìƒ˜í”Œ: {dev_samples_phase6}\")\n",
    "\n",
    "for sample_idx in dev_samples_phase6[:2]:  # ì²˜ìŒ 2ê°œë§Œ ìƒì„¸ ì¶œë ¥\n",
    "    dialogue = dev_df_phase6.iloc[sample_idx]['dialogue']\n",
    "    summary = dev_df_phase6.iloc[sample_idx]['summary']\n",
    "    analysis = analyze_dialogue_structure(dialogue)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[Dev ìƒ˜í”Œ {sample_idx}]\")\n",
    "    print(f\"  í™”ì: {analysis['num_speakers']}ëª…, PII: {list(analysis['pii_masks'])[:3]}\")\n",
    "    print(f\"  ì •ë‹µ ìš”ì•½: {summary[:80]}...\")\n",
    "    \n",
    "    for style in PROMPT_STYLES_PHASE6[:3]:  # ì²˜ìŒ 3ê°œ ìŠ¤íƒ€ì¼ë§Œ ë¹„êµ\n",
    "        print(f\"\\n  ğŸ”¹ {style['name']}:\")\n",
    "        \n",
    "        if style['prompt_style'] == \"keyword_aware\":\n",
    "            prompted = build_summary_prompt(\n",
    "                dialogue,\n",
    "                prompt_style=style['prompt_style'],\n",
    "                tfidf_extractor=tfidf_extractor,\n",
    "                top_k=5,\n",
    "                use_speaker_count=style['use_speaker_count'],\n",
    "                use_pii_instruction=style['use_pii_instruction']\n",
    "            )\n",
    "        else:\n",
    "            prompted = build_summary_prompt(\n",
    "                dialogue,\n",
    "                prompt_style=style['prompt_style'],\n",
    "                use_speaker_count=style['use_speaker_count'],\n",
    "                use_pii_instruction=style['use_pii_instruction']\n",
    "            )\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ë§Œ ì¶œë ¥ (ëŒ€í™”ë¬¸ ì œì™¸)\n",
    "        prompt_only = prompted.split('\\n\\n')[0] if '\\n\\n' in prompted else prompted[:200]\n",
    "        print(f\"     í”„ë¡¬í”„íŠ¸: {prompt_only[:150]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… Phase 6 ë¶„ì„ ì™„ë£Œ!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“¤ Phase 6 ì œì¶œ íŒŒì¼ ìƒì„±: 6-1 + 6-2 ì „ëµ ì ìš©\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“¤ Phase 6 ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Phase 6 ì„¤ì • (6-1, 6-2 ì „ëµ ì ìš©)\n",
    "PHASE6_CONFIG = {\n",
    "    # í”„ë¡¬í”„íŠ¸ ì„¤ì • (balanced = 6-1 + 6-2)\n",
    "    \"prompt\": {\n",
    "        \"use_prompt\": True,\n",
    "        \"prompt_style\": \"balanced\",  # 6-1 + 6-2 í†µí•© ìŠ¤íƒ€ì¼\n",
    "        \"use_turn_separator\": False,\n",
    "        \"turn_separator\": \" [SEP] \",\n",
    "        \"tfidf_top_k\": 5,\n",
    "    },\n",
    "    # 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš© ì„¤ì •\n",
    "    \"regex_pattern\": {\n",
    "        \"use_speaker_count\": True,       # 6-1. í™”ì ìˆ˜ ëª…ì‹œ\n",
    "        \"use_pii_instruction\": True,     # 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ\n",
    "        \"show_pii_examples\": True,       # ë§ˆìŠ¤í‚¹ ì˜ˆì‹œ í‘œì‹œ\n",
    "    },\n",
    "    # êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •\n",
    "    \"preprocess\": {\n",
    "        \"normalize_slang\": True,\n",
    "    },\n",
    "    # ë™ì  ì¶”ë¡  ì„¤ì •\n",
    "    \"dynamic_inference\": {\n",
    "        \"use_length_based\": False,\n",
    "        \"use_topic_based\": False,\n",
    "    },\n",
    "    # ìƒì„± ì„¤ì • (Grid Search ìµœì ê°’)\n",
    "    \"generation\": {\n",
    "        \"num_beams\": 7,\n",
    "        \"length_penalty\": 1.0,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"max_length\": 100,\n",
    "        \"early_stopping\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ“‹ Phase 6 ì„¤ì •:\")\n",
    "print(f\"   í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: {PHASE6_CONFIG['prompt']['prompt_style']}\")\n",
    "print(f\"   6-1 í™”ì ìˆ˜ ëª…ì‹œ: {PHASE6_CONFIG['regex_pattern']['use_speaker_count']}\")\n",
    "print(f\"   6-2 PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: {PHASE6_CONFIG['regex_pattern']['use_pii_instruction']}\")\n",
    "print(f\"   ìŠ¬ë­ ì •ê·œí™”: {PHASE6_CONFIG['preprocess']['normalize_slang']}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ì œì¶œ íŒŒì¼ ìƒì„± í•¨ìˆ˜\n",
    "# =============================================================================\n",
    "\n",
    "def generate_phase6_submission(model, tokenizer, test_df, config, output_path):\n",
    "    \"\"\"Phase 6 ì„¤ì •ìœ¼ë¡œ ì œì¶œ íŒŒì¼ ìƒì„± (6-1 + 6-2 ì „ëµ)\"\"\"\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ ì„¤ì •\n",
    "    regex_pattern_config = config.get('regex_pattern', {\n",
    "        \"use_speaker_count\": True,\n",
    "        \"use_pii_instruction\": True,\n",
    "        \"show_pii_examples\": True,\n",
    "    })\n",
    "    \n",
    "    # Preprocessor ìƒì„±\n",
    "    preprocessor = Preprocess(\n",
    "        loaded_config['tokenizer']['bos_token'],\n",
    "        loaded_config['tokenizer']['eos_token'],\n",
    "        prompt_config=config['prompt'],\n",
    "        preprocess_config=config['preprocess'],\n",
    "        tfidf_extractor=None,  # balanced ìŠ¤íƒ€ì¼ì—ì„œëŠ” TF-IDF ë¯¸ì‚¬ìš©\n",
    "        regex_pattern_config=regex_pattern_config,  # 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ ì„¤ì •\n",
    "    )\n",
    "    \n",
    "    gen_config = config['generation']\n",
    "    \n",
    "    summaries = []\n",
    "    pii_preserved_count = 0  # PII ë§ˆìŠ¤í‚¹ ë³´ì¡´ ì¹´ìš´íŠ¸\n",
    "    \n",
    "    for idx in tqdm(range(len(test_df)), desc=\"ì¶”ë¡ \"):\n",
    "        dialogue = test_df.iloc[idx]['dialogue']\n",
    "        \n",
    "        # ì›ë³¸ ëŒ€í™”ì—ì„œ PII ë§ˆìŠ¤í‚¹ ì¶”ì¶œ\n",
    "        original_pii = get_pii_mask_tokens(dialogue)\n",
    "        \n",
    "        # ì „ì²˜ë¦¬\n",
    "        processed_dialogue = preprocessor.full_preprocess_pipeline(dialogue)\n",
    "        \n",
    "        # í† í°í™”\n",
    "        inputs = tokenizer(\n",
    "            processed_dialogue,\n",
    "            max_length=loaded_config['tokenizer']['encoder_max_len'],\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # ìƒì„±\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                num_beams=gen_config['num_beams'],\n",
    "                length_penalty=gen_config['length_penalty'],\n",
    "                repetition_penalty=gen_config['repetition_penalty'],\n",
    "                no_repeat_ngram_size=gen_config['no_repeat_ngram_size'],\n",
    "                max_length=gen_config['max_length'],\n",
    "                early_stopping=gen_config['early_stopping'],\n",
    "            )\n",
    "        \n",
    "        # ë””ì½”ë”© (í™”ì íƒœê·¸ ë³´ì¡´)\n",
    "        summary = decode_with_speaker_tags(tokenizer, generated_ids[0])\n",
    "        \n",
    "        # í›„ì²˜ë¦¬\n",
    "        summary = postprocess_summary(summary)\n",
    "        \n",
    "        # PII ë§ˆìŠ¤í‚¹ ë³´ì¡´ ì—¬ë¶€ í™•ì¸\n",
    "        if original_pii:\n",
    "            preserved = all(pii in summary for pii in original_pii)\n",
    "            if preserved:\n",
    "                pii_preserved_count += 1\n",
    "        \n",
    "        summaries.append(summary)\n",
    "    \n",
    "    # ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "    submission = pd.DataFrame({\n",
    "        'fname': test_df['fname'],\n",
    "        'summary': summaries\n",
    "    })\n",
    "    \n",
    "    submission.to_csv(output_path, index=False)\n",
    "    print(f\"\\nâœ… ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "    print(f\"   í‰ê·  ê¸¸ì´: {np.mean([len(s) for s in summaries]):.1f}ì\")\n",
    "    \n",
    "    # í™”ì íƒœê·¸ í¬í•¨ ì—¬ë¶€ í™•ì¸\n",
    "    has_speaker_tags = sum(1 for s in summaries if '#Person' in s)\n",
    "    print(f\"   í™”ì íƒœê·¸ í¬í•¨: {has_speaker_tags}/{len(summaries)}ê°œ ({has_speaker_tags/len(summaries)*100:.1f}%)\")\n",
    "    \n",
    "    # PII ë§ˆìŠ¤í‚¹ ë³´ì¡´ ë¹„ìœ¨\n",
    "    pii_total = sum(1 for d in test_df['dialogue'] if get_pii_mask_tokens(d))\n",
    "    if pii_total > 0:\n",
    "        print(f\"   PII ë§ˆìŠ¤í‚¹ ë³´ì¡´: {pii_preserved_count}/{pii_total}ê°œ ({pii_preserved_count/pii_total*100:.1f}%)\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# =============================================================================\n",
    "# ì œì¶œ íŒŒì¼ ìƒì„± ì‹¤í–‰\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nğŸš€ Phase 6 ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\")\n",
    "\n",
    "# Test ë°ì´í„° ë¡œë“œ\n",
    "test_df_phase6_submit = pd.read_csv(\"./data/test.csv\")\n",
    "print(f\"ğŸ“Š Test ë°ì´í„°: {len(test_df_phase6_submit)}ê°œ\")\n",
    "\n",
    "# Phase 6 ì œì¶œ íŒŒì¼ ìƒì„± (balanced: 6-1 + 6-2)\n",
    "submission_phase6 = generate_phase6_submission(\n",
    "    eval_model, eval_tokenizer, test_df_phase6_submit,\n",
    "    PHASE6_CONFIG,\n",
    "    \"./prediction/submit_phase6_balanced.csv\"\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# keyword_aware ìŠ¤íƒ€ì¼ë¡œë„ ìƒì„± (TF-IDF + 6-1 + 6-2)\n",
    "# =============================================================================\n",
    "\n",
    "PHASE6_KEYWORD_CONFIG = {\n",
    "    **PHASE6_CONFIG,\n",
    "    \"prompt\": {\n",
    "        \"use_prompt\": True,\n",
    "        \"prompt_style\": \"keyword_aware\",  # TF-IDF + 6-1 + 6-2\n",
    "        \"use_turn_separator\": False,\n",
    "        \"turn_separator\": \" [SEP] \",\n",
    "        \"tfidf_top_k\": 5,\n",
    "    },\n",
    "}\n",
    "\n",
    "def generate_phase6_keyword_submission(model, tokenizer, test_df, config, output_path):\n",
    "    \"\"\"Phase 6 keyword_aware ìŠ¤íƒ€ì¼ (TF-IDF + 6-1 + 6-2)\"\"\"\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    regex_pattern_config = config.get('regex_pattern', {\n",
    "        \"use_speaker_count\": True,\n",
    "        \"use_pii_instruction\": True,\n",
    "        \"show_pii_examples\": True,\n",
    "    })\n",
    "    \n",
    "    # TF-IDF extractor í¬í•¨\n",
    "    preprocessor = Preprocess(\n",
    "        loaded_config['tokenizer']['bos_token'],\n",
    "        loaded_config['tokenizer']['eos_token'],\n",
    "        prompt_config=config['prompt'],\n",
    "        preprocess_config=config['preprocess'],\n",
    "        tfidf_extractor=tfidf_extractor,  # TF-IDF ì‚¬ìš©\n",
    "        regex_pattern_config=regex_pattern_config,\n",
    "    )\n",
    "    \n",
    "    gen_config = config['generation']\n",
    "    summaries = []\n",
    "    \n",
    "    for idx in tqdm(range(len(test_df)), desc=\"ì¶”ë¡  (keyword_aware)\"):\n",
    "        dialogue = test_df.iloc[idx]['dialogue']\n",
    "        processed_dialogue = preprocessor.full_preprocess_pipeline(dialogue)\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            processed_dialogue,\n",
    "            max_length=loaded_config['tokenizer']['encoder_max_len'],\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                num_beams=gen_config['num_beams'],\n",
    "                length_penalty=gen_config['length_penalty'],\n",
    "                repetition_penalty=gen_config['repetition_penalty'],\n",
    "                no_repeat_ngram_size=gen_config['no_repeat_ngram_size'],\n",
    "                max_length=gen_config['max_length'],\n",
    "                early_stopping=gen_config['early_stopping'],\n",
    "            )\n",
    "        \n",
    "        summary = decode_with_speaker_tags(tokenizer, generated_ids[0])\n",
    "        summary = postprocess_summary(summary)\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'fname': test_df['fname'],\n",
    "        'summary': summaries\n",
    "    })\n",
    "    \n",
    "    submission.to_csv(output_path, index=False)\n",
    "    print(f\"\\nâœ… ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "    print(f\"   í‰ê·  ê¸¸ì´: {np.mean([len(s) for s in summaries]):.1f}ì\")\n",
    "    \n",
    "    has_speaker_tags = sum(1 for s in summaries if '#Person' in s)\n",
    "    print(f\"   í™”ì íƒœê·¸ í¬í•¨: {has_speaker_tags}/{len(summaries)}ê°œ ({has_speaker_tags/len(summaries)*100:.1f}%)\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "print(\"\\nğŸš€ Phase 6 keyword_aware ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\")\n",
    "submission_phase6_keyword = generate_phase6_keyword_submission(\n",
    "    eval_model, eval_tokenizer, test_df_phase6_submit,\n",
    "    PHASE6_KEYWORD_CONFIG,\n",
    "    \"./prediction/submit_phase6_keyword.csv\"\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# ìƒ˜í”Œ ë¹„êµ ì¶œë ¥\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“‹ Phase 6 ì œì¶œ ìƒ˜í”Œ ë¹„êµ:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# PII ë§ˆìŠ¤í‚¹ì´ ìˆëŠ” Test ìƒ˜í”Œ ì°¾ê¸°\n",
    "test_with_pii = []\n",
    "for i, dialogue in enumerate(test_df_phase6_submit['dialogue']):\n",
    "    if get_pii_mask_tokens(dialogue):\n",
    "        test_with_pii.append(i)\n",
    "        if len(test_with_pii) >= 3:\n",
    "            break\n",
    "\n",
    "print(f\"\\nPII ë§ˆìŠ¤í‚¹ì´ ìˆëŠ” Test ìƒ˜í”Œ: {test_with_pii}\")\n",
    "\n",
    "for i in test_with_pii[:2]:\n",
    "    dialogue = test_df_phase6_submit.iloc[i]['dialogue']\n",
    "    pii_masks = get_pii_mask_tokens(dialogue)\n",
    "    \n",
    "    print(f\"\\n[Test {i}] PII ë§ˆìŠ¤í‚¹: {pii_masks}\")\n",
    "    print(f\"  Balanced: {submission_phase6['summary'].iloc[i][:100]}...\")\n",
    "    print(f\"  Keyword:  {submission_phase6_keyword['summary'].iloc[i][:100]}...\")\n",
    "\n",
    "# ê¸°ì¡´ ì œì¶œê³¼ ë¹„êµ\n",
    "print(\"\\nğŸ“‹ ê¸°ì¡´ ìµœì í™” ì œì¶œê³¼ ë¹„êµ:\")\n",
    "try:\n",
    "    old_submission = pd.read_csv(\"./prediction/submit_optimized_v2.csv\")\n",
    "    for i in range(3):\n",
    "        print(f\"\\n[Test {i}]\")\n",
    "        print(f\"  ê¸°ì¡´:     {old_submission['summary'].iloc[i][:70]}...\")\n",
    "        print(f\"  Phase6:   {submission_phase6['summary'].iloc[i][:70]}...\")\n",
    "except:\n",
    "    print(\"   ê¸°ì¡´ ì œì¶œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… Phase 6 ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "ğŸ“¤ ìƒì„±ëœ ì œì¶œ íŒŒì¼:\n",
    "  1. ./prediction/submit_phase6_balanced.csv\n",
    "     - 6-1 í™”ì ìˆ˜ ëª…ì‹œ + 6-2 PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ\n",
    "     \n",
    "  2. ./prediction/submit_phase6_keyword.csv\n",
    "     - TF-IDF í‚¤ì›Œë“œ + 6-1 + 6-2 í†µí•©\n",
    "\n",
    "ğŸ¯ ê¸°ëŒ€ íš¨ê³¼:\n",
    "  - 6-1: \"í•œ ì‚¬ëŒë§Œ ìš”ì•½í•˜ëŠ”\" ë²„ê·¸ ê°ì†Œ\n",
    "  - 6-2: PII ë§ˆìŠ¤í‚¹ í† í° ë³´ì¡´ìœ¼ë¡œ ROUGE ì ìˆ˜ í–¥ìƒ\n",
    "\n",
    "ğŸ“ ë¦¬ë”ë³´ë“œ ì œì¶œ ìˆœì„œ:\n",
    "  1) submit_phase6_balanced.csv â†’ ê¸°ë³¸ 6-1, 6-2 ì „ëµ\n",
    "  2) submit_phase6_keyword.csv â†’ TF-IDF í‚¤ì›Œë“œ ì¶”ê°€ íš¨ê³¼ í™•ì¸\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "083ea69907bb48d4a8fff919bac51aad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08d05bc20a96432badd459e1ffaf868e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "13651c09564a4337b8274c1cb436faa5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14f6c91d6c634379b498586c51e606e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21d2e54b5a0a4f79973a512105da43eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2307c6dcbe0141acb5e61baae19cade7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "285007b45236478ca147c6df752c8da4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a190bda0b72407e9a953cd2104dd3b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2fd3d7bbcd6948d8904d33001f95ea03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3645438ace1f4596a8dbc157b48c1521": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14f6c91d6c634379b498586c51e606e0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_08d05bc20a96432badd459e1ffaf868e",
      "value": " 295/295 [00:00&lt;00:00, 21.3kB/s]"
     }
    },
    "3a04e871b74b45d7bf02fd33bb103577": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3bcd6b6b956347b29e1efa20a1d00542": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c284a826f6843f6aa47eacad478ac30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_affff1d8a89e4c14955d1b2aa39ff1ab",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_13651c09564a4337b8274c1cb436faa5",
      "value": "tokenizer.json: 100%"
     }
    },
    "45187decb58b4ad39ad532259c6277e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4747b668e2fa4ab58a449446f80030f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "52095cc7087243ac916055e569fd22f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c18f0e3bc35e44d9915c3f84cd282a26",
      "max": 109,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3a04e871b74b45d7bf02fd33bb103577",
      "value": 109
     }
    },
    "58001a60eacc44d5b38a68648adccde4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58c794fb7ce543a39fdf66d757f6eeab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f5fde5b0ac840a18bd5cc380e564ff6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_45187decb58b4ad39ad532259c6277e5",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "5dfcf310ca9e4e2794076098a5d69cea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3c284a826f6843f6aa47eacad478ac30",
       "IPY_MODEL_6caedd60c6b747469c82930be1f95d6d",
       "IPY_MODEL_64f2218f899d446393cfea44f206f0a6"
      ],
      "layout": "IPY_MODEL_d068f541df3f438dbd5138863e64b2f2"
     }
    },
    "64f2218f899d446393cfea44f206f0a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d22fbc2c5dbf422399e496c9b500025a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_775d8bbeceac4e2da4f21ab6235c89ed",
      "value": " 682k/682k [00:00&lt;00:00, 5.40MB/s]"
     }
    },
    "6caedd60c6b747469c82930be1f95d6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3bcd6b6b956347b29e1efa20a1d00542",
      "max": 682133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2fd3d7bbcd6948d8904d33001f95ea03",
      "value": 682133
     }
    },
    "6f5fde5b0ac840a18bd5cc380e564ff6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "775d8bbeceac4e2da4f21ab6235c89ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8a6464a355f7464c989033965d418a8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2307c6dcbe0141acb5e61baae19cade7",
      "max": 295,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4747b668e2fa4ab58a449446f80030f5",
      "value": 295
     }
    },
    "a15af9e8158f4903b9189f3d322a5ef3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac00d6c2cf974b33a628acb3f1471316",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_285007b45236478ca147c6df752c8da4",
      "value": " 109/109 [00:00&lt;00:00, 9.44kB/s]"
     }
    },
    "ac00d6c2cf974b33a628acb3f1471316": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "affff1d8a89e4c14955d1b2aa39ff1ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c18f0e3bc35e44d9915c3f84cd282a26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d068f541df3f438dbd5138863e64b2f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d22fbc2c5dbf422399e496c9b500025a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de1a3f7701c243839fe03b930a9b9e30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ebc22683058a4f229c5588e52fc93536",
       "IPY_MODEL_52095cc7087243ac916055e569fd22f3",
       "IPY_MODEL_a15af9e8158f4903b9189f3d322a5ef3"
      ],
      "layout": "IPY_MODEL_21d2e54b5a0a4f79973a512105da43eb"
     }
    },
    "e920dbc173c045d1a32143349f1dff8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_58c794fb7ce543a39fdf66d757f6eeab",
       "IPY_MODEL_8a6464a355f7464c989033965d418a8a",
       "IPY_MODEL_3645438ace1f4596a8dbc157b48c1521"
      ],
      "layout": "IPY_MODEL_58001a60eacc44d5b38a68648adccde4"
     }
    },
    "ebc22683058a4f229c5588e52fc93536": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_083ea69907bb48d4a8fff919bac51aad",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2a190bda0b72407e9a953cd2104dd3b2",
      "value": "special_tokens_map.json: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
