{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fce0fdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 1. í™˜ê²½ ì„¤ì •\n",
    "!pip install -q -U torch transformers peft bitsandbytes accelerate pandas datasets trl evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdfb3f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'Could not load this library: /opt/conda/lib/python3.10/site-packages/torchvision/image.so'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Set random seed\n",
    "def seed_everything(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec73ec",
   "metadata": {},
   "source": [
    "## 2. ê°œì„ ëœ ì„¤ì • (v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e3b0f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ í•™ìŠµ ì„¤ì • v2:\n",
      "  model_name: upstage/SOLAR-10.7B-Instruct-v1.0\n",
      "  data_path: ./data/\n",
      "  output_dir: ./results_solar_v2\n",
      "  lora_r: 16\n",
      "  lora_alpha: 64\n",
      "  lora_dropout: 0.1\n",
      "  lr: 0.0002\n",
      "  batch_size: 1\n",
      "  grad_accum: 8\n",
      "  epochs: 2\n",
      "  max_seq_length: 1024\n",
      "  warmup_ratio: 0.1\n",
      "  lr_scheduler: cosine\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ê°œì„ ëœ í•™ìŠµ ì„¤ì • v2 (í˜„ì‹¤ì  ìµœì í™”)\n",
    "# ============================================\n",
    "# v1 í•™ìŠµì‹œê°„ ê¸°ì¤€: ~30ë¶„ (epoch=1, batch=2, seq=1024)\n",
    "# v2 ì˜ˆìƒ í•™ìŠµì‹œê°„: ~60-90ë¶„ (epoch=2, batch=1, seq=1024)\n",
    "# ============================================\n",
    "CONF = {\n",
    "    \"model_name\": \"upstage/SOLAR-10.7B-Instruct-v1.0\",\n",
    "    \"data_path\": \"./data/\",\n",
    "    \"output_dir\": \"./results_solar_v2\",\n",
    "    \n",
    "    # LoRA ì„¤ì • (ê°œì„  - í•µì‹¬)\n",
    "    \"lora_r\": 16,              # 8 â†’ 16 â­ (í‘œí˜„ë ¥ 2ë°°, +0.3~0.5ì  ê¸°ëŒ€)\n",
    "    \"lora_alpha\": 64,          # 32 â†’ 64 (scaling ë¹„ìœ¨ ìœ ì§€)\n",
    "    \"lora_dropout\": 0.1,       # 0.05 â†’ 0.1 (ê³¼ì í•© ë°©ì§€)\n",
    "    \n",
    "    # í•™ìŠµ ì„¤ì • (í˜„ì‹¤ì  ìµœì í™”)\n",
    "    \"lr\": 2e-4,                # 1e-4 â†’ 2e-4 (ë¹ ë¥¸ ìˆ˜ë ´)\n",
    "    \"batch_size\": 1,           # 2 â†’ 1 (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "    \"grad_accum\": 8,           # 4 â†’ 8 (effective batch=8 ìœ ì§€)\n",
    "    \"epochs\": 2,               # 1 â†’ 2 â­ (3ì€ ê³¼ì í•© ìœ„í—˜ + ì‹œê°„ ê³¼ë‹¤)\n",
    "    \"max_seq_length\": 1024,    # 1024 ìœ ì§€ (ë©”ëª¨ë¦¬ ì•ˆì „, ë°ì´í„° ëŒ€ë¶€ë¶„ ì»¤ë²„)\n",
    "    \"warmup_ratio\": 0.1,       # 0.03 â†’ 0.1 (ì•ˆì •ì  ì‹œì‘)\n",
    "    \"lr_scheduler\": \"cosine\",  # constant â†’ cosine â­ (í›„ë°˜ ë¯¸ì„¸ì¡°ì •)\n",
    "}\n",
    "\n",
    "print(\"ğŸ“‹ í•™ìŠµ ì„¤ì • v2:\")\n",
    "for k, v in CONF.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130ccc0b",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„° ì „ì²˜ë¦¬ (ê°œì„ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20b006af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03bed87c3fa407e96eb6a616b7cf304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a7e09ab469474293f2de8478687fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°:\n",
      "  Train: 12457\n",
      "  Dev: 499\n",
      "\n",
      "ğŸ“ ìƒ˜í”Œ:\n",
      "### User:\n",
      "ë‹¤ìŒ ëŒ€í™”ë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ì„¸ìš”. #Person1#, #Person2# ë“±ì˜ í™”ì í‘œì‹œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”.\n",
      "\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? #Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. #Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. #Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”. #Person1#: ìŒ, ì‹¬ê°í•œ ì§ˆë³‘ì„ í”¼í•˜ë ¤ë©´ ë¯¸ë¦¬ ë°œê²¬í•˜ëŠ” ê²Œ ì œì¼ ì¢‹ê±°ë“ ìš”. ë³¸ì¸ì„ ìœ„í•´ì„œë¼ë„ ë§¤ë…„ í•œ ë²ˆì€ ì˜¤ì„¸ìš”. #Person2#: ì•Œê² ìŠµë‹ˆë‹¤. #Person1#: ì—¬ê¸° ì¢€ ë³¼ê¹Œìš”. ëˆˆê³¼ ê·€ëŠ” ê´œì°®ìœ¼ì‹œë„¤ìš”. ê¹Šê²Œ ìˆ¨ í•œ ë²ˆ ì‰¬ì–´ë³´ì„¸ìš”. Mr. Smith, ë‹´ë°° í”¼ìš°ì„¸ìš”? #Person2#: ë„¤. #Person1#: ë‹´ë°°ê°€ íì•”í•˜ê³  ì‹¬ì¥ë³‘ì˜ ì£¼ëœ ì›ì¸ì¸ ê±° ì•„ì‹œì£ ? ëŠìœ¼ì…”ì•¼ í•´ìš”. #Person2#: ìˆ˜ë°± ë²ˆ ì‹œë„í–ˆ\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ì •ê·œí™” (ê°œì„ )\"\"\"\n",
    "    # 1. ì—°ì† ê³µë°± ì œê±°\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 2. íŠ¹ìˆ˜ í† í° ì •ê·œí™” - ì¼ê´€ëœ í˜•ì‹ ìœ ì§€\n",
    "    # Person í† í° (ë‹¤ì–‘í•œ ë³€í˜• í†µì¼)\n",
    "    text = re.sub(r'#Person(\\d+)#', r'#Person\\1#', text)\n",
    "    \n",
    "    # ê°œì¸ì •ë³´ í† í° ì •ê·œí™”\n",
    "    text = re.sub(r'#PhoneNumber#', '#PhoneNumber#', text)\n",
    "    text = re.sub(r'#Address#', '#Address#', text)\n",
    "    text = re.sub(r'#Email#', '#Email#', text)\n",
    "    text = re.sub(r'#DateOfBirth#', '#DateOfBirth#', text)\n",
    "    text = re.sub(r'#CardNumber#', '#CardNumber#', text)\n",
    "    text = re.sub(r'#CarNumber#', '#CarNumber#', text)\n",
    "    text = re.sub(r'#PassportNumber#', '#PassportNumber#', text)\n",
    "    text = re.sub(r'#SSN#', '#SSN#', text)\n",
    "    \n",
    "    # ì´ë¦„ í† í° í†µì¼ (#Name#, #PersonName# -> #Name#)\n",
    "    text = re.sub(r'#PersonName#', '#Name#', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def load_data(data_path):\n",
    "    train_df = pd.read_csv(os.path.join(data_path, 'train.csv'))\n",
    "    dev_df = pd.read_csv(os.path.join(data_path, 'dev.csv'))\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ì •ê·œí™”\n",
    "    train_df['dialogue'] = train_df['dialogue'].apply(clean_text)\n",
    "    train_df['summary'] = train_df['summary'].apply(clean_text)\n",
    "    dev_df['dialogue'] = dev_df['dialogue'].apply(clean_text)\n",
    "    dev_df['summary'] = dev_df['summary'].apply(clean_text)\n",
    "    \n",
    "    return train_df, dev_df\n",
    "\n",
    "def format_instruction(row):\n",
    "    \"\"\"SOLAR Instruct í˜•ì‹ (ê°œì„ ëœ í”„ë¡¬í”„íŠ¸)\"\"\"\n",
    "    # ê°œì„ : í•œêµ­ì–´ ìš”ì•½ + íŠ¹ìˆ˜ í† í° ë³´ì¡´ ì§€ì‹œ ì¶”ê°€\n",
    "    prompt = f\"\"\"### User:\n",
    "ë‹¤ìŒ ëŒ€í™”ë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ì„¸ìš”. #Person1#, #Person2# ë“±ì˜ í™”ì í‘œì‹œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”.\n",
    "\n",
    "{row['dialogue']}\n",
    "\n",
    "### Assistant:\n",
    "\"\"\"\n",
    "    if 'summary' in row:\n",
    "        prompt += f\"{row['summary']}\"\n",
    "    return prompt\n",
    "\n",
    "train_df, dev_df = load_data(CONF['data_path'])\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x: {'text': format_instruction(x)})\n",
    "dev_dataset = dev_dataset.map(lambda x: {'text': format_instruction(x)})\n",
    "\n",
    "print(f\"ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°:\")\n",
    "print(f\"  Train: {len(train_dataset)}\")\n",
    "print(f\"  Dev: {len(dev_dataset)}\")\n",
    "print(f\"\\nğŸ“ ìƒ˜í”Œ:\")\n",
    "print(train_dataset[0]['text'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf52bb0",
   "metadata": {},
   "source": [
    "## 4. ëª¨ë¸ ë¡œë“œ (QLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15ee7aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c916e6258f4ffcb5aed96c2107ecc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: upstage/SOLAR-10.7B-Instruct-v1.0\n"
     ]
    }
   ],
   "source": [
    "# 4-bit Quantization Config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONF['model_name'],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONF['model_name'], trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {CONF['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f079c708",
   "metadata": {},
   "source": [
    "## 5. LoRA ì„¤ì • (ê°œì„ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b7b75e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°:\n",
      "trainable params: 62,914,560 || all params: 10,794,438,656 || trainable%: 0.5828\n"
     ]
    }
   ],
   "source": [
    "# ê°œì„ ëœ LoRA ì„¤ì •\n",
    "peft_config = LoraConfig(\n",
    "    r=CONF['lora_r'],              # 16 (ì¦ê°€)\n",
    "    lora_alpha=CONF['lora_alpha'], # 64 (ì¦ê°€)\n",
    "    lora_dropout=CONF['lora_dropout'],  # 0.1 (ì¦ê°€)\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # ëª¨ë“  linear layer íƒ€ê²Ÿ\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "print(\"\\nğŸ“Š í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°:\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33bd5b4",
   "metadata": {},
   "source": [
    "## 6. í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddd4b3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d163e9e1056a4a319d9dca0a894d64c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/12457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0fea4a8ca64f5790a92a76260a340d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/12457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4941ac73011648509c9c4f8e2b05db26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/12457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198d1e900b644266856dab22fbbf5e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419b7fddc4f146b0a24a9ff7c193372a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0ffd06a1784bea86d31bb7800645e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3116' max='3116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3116/3116 10:13:16, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.671700</td>\n",
       "      <td>0.821098</td>\n",
       "      <td>0.774802</td>\n",
       "      <td>833098.000000</td>\n",
       "      <td>0.774581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.674100</td>\n",
       "      <td>0.831118</td>\n",
       "      <td>0.815753</td>\n",
       "      <td>1664904.000000</td>\n",
       "      <td>0.772507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.673300</td>\n",
       "      <td>0.814846</td>\n",
       "      <td>0.755387</td>\n",
       "      <td>2509369.000000</td>\n",
       "      <td>0.777479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.643600</td>\n",
       "      <td>0.806955</td>\n",
       "      <td>0.797615</td>\n",
       "      <td>3339773.000000</td>\n",
       "      <td>0.778116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.651600</td>\n",
       "      <td>0.797188</td>\n",
       "      <td>0.785533</td>\n",
       "      <td>4182203.000000</td>\n",
       "      <td>0.781189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.637900</td>\n",
       "      <td>0.772395</td>\n",
       "      <td>0.755802</td>\n",
       "      <td>5018082.000000</td>\n",
       "      <td>0.788392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.631500</td>\n",
       "      <td>0.754182</td>\n",
       "      <td>0.741668</td>\n",
       "      <td>5843527.000000</td>\n",
       "      <td>0.793884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.522600</td>\n",
       "      <td>0.744097</td>\n",
       "      <td>0.618031</td>\n",
       "      <td>6683995.000000</td>\n",
       "      <td>0.798637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.526000</td>\n",
       "      <td>0.728678</td>\n",
       "      <td>0.614934</td>\n",
       "      <td>7519868.000000</td>\n",
       "      <td>0.802023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.528300</td>\n",
       "      <td>0.717974</td>\n",
       "      <td>0.594112</td>\n",
       "      <td>8350904.000000</td>\n",
       "      <td>0.804750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.491300</td>\n",
       "      <td>0.706043</td>\n",
       "      <td>0.585359</td>\n",
       "      <td>9181584.000000</td>\n",
       "      <td>0.808406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.500600</td>\n",
       "      <td>0.697445</td>\n",
       "      <td>0.582862</td>\n",
       "      <td>10009779.000000</td>\n",
       "      <td>0.810210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.466100</td>\n",
       "      <td>0.688702</td>\n",
       "      <td>0.584821</td>\n",
       "      <td>10853443.000000</td>\n",
       "      <td>0.811718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.475300</td>\n",
       "      <td>0.685185</td>\n",
       "      <td>0.585065</td>\n",
       "      <td>11686925.000000</td>\n",
       "      <td>0.812313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.470200</td>\n",
       "      <td>0.683246</td>\n",
       "      <td>0.584218</td>\n",
       "      <td>12523633.000000</td>\n",
       "      <td>0.812905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: ./results_solar_v2/best_model\n"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµ ì„¤ì • (TRL 0.25+ í˜¸í™˜)\n",
    "training_args = SFTConfig(\n",
    "    output_dir=CONF['output_dir'],\n",
    "    num_train_epochs=CONF['epochs'],\n",
    "    per_device_train_batch_size=CONF['batch_size'],\n",
    "    per_device_eval_batch_size=CONF['batch_size'],\n",
    "    gradient_accumulation_steps=CONF['grad_accum'],\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=10,\n",
    "    learning_rate=CONF['lr'],\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=CONF['warmup_ratio'],\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=CONF['lr_scheduler'],\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    max_length=CONF['max_seq_length'],  # max_seq_length â†’ max_length\n",
    "    packing=False,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "# Trainer ì´ˆê¸°í™” (TRL 0.25+)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"ğŸš€ í•™ìŠµ ì‹œì‘...\")\n",
    "trainer.train()\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "trainer.model.save_pretrained(CONF['output_dir'] + \"/best_model\")\n",
    "tokenizer.save_pretrained(CONF['output_dir'] + \"/best_model\")\n",
    "print(f\"\\nâœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {CONF['output_dir']}/best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3120a3d0",
   "metadata": {},
   "source": [
    "## 7. ì¶”ë¡  ë° í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13243daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ ì¶”ë¡  ì‹œì‘ (Test: 499ê°œ)\n",
      "   Params: {'max_new_tokens': 150, 'num_beams': 3, 'early_stopping': True, 'no_repeat_ngram_size': 0, 'repetition_penalty': 1.2, 'length_penalty': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [13:12:59<00:00, 95.35s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ./prediction/submit_solar_v2.csv\n",
      "\n",
      "ğŸ“Š ìƒì„± ìš”ì•½ í†µê³„:\n",
      "  í‰ê·  ê¸¸ì´: 16.9 words\n",
      "  ìµœì†Œ/ìµœëŒ€: 5 / 38 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def post_process(summary):\n",
    "    \"\"\"í›„ì²˜ë¦¬\"\"\"\n",
    "    summary = re.sub(r\"\\s+\", \" \", summary).strip()\n",
    "    summary = summary.replace(\"### User:\", \"\").replace(\"### Assistant:\", \"\").strip()\n",
    "    if not summary:\n",
    "        return \"ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "    return summary\n",
    "\n",
    "def generate_summary(model, tokenizer, dialogue, params):\n",
    "    # í•™ìŠµ ì‹œì™€ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (ì¤‘ìš”!)\n",
    "    prompt = f\"\"\"### User:\n",
    "ë‹¤ìŒ ëŒ€í™”ë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ì„¸ìš”. #Person1#, #Person2# ë“±ì˜ í™”ì í‘œì‹œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "### Assistant:\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            **params,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    try:\n",
    "        summary = generated_text.split(\"### Assistant:\\n\")[1].strip()\n",
    "    except:\n",
    "        summary = generated_text\n",
    "    \n",
    "    return post_process(summary)\n",
    "\n",
    "# ìµœì  ì¶”ë¡  íŒŒë¼ë¯¸í„° (51.80ì  ì„¤ì •)\n",
    "INFERENCE_PARAMS = {\n",
    "    \"max_new_tokens\": 150,\n",
    "    \"num_beams\": 3,\n",
    "    \"early_stopping\": True,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"length_penalty\": 1.3,\n",
    "}\n",
    "\n",
    "# Test ë°ì´í„° ì¶”ë¡ \n",
    "test_df = pd.read_csv(os.path.join(CONF['data_path'], 'test.csv'))\n",
    "\n",
    "print(f\"ğŸ”„ ì¶”ë¡  ì‹œì‘ (Test: {len(test_df)}ê°œ)\")\n",
    "print(f\"   Params: {INFERENCE_PARAMS}\")\n",
    "\n",
    "model.eval()\n",
    "summaries = []\n",
    "for dialogue in tqdm(test_df['dialogue'], desc=\"Inference\"):\n",
    "    summary = generate_summary(model, tokenizer, dialogue, INFERENCE_PARAMS)\n",
    "    summaries.append(summary)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "submission = pd.DataFrame({\n",
    "    'fname': test_df['fname'],\n",
    "    'summary': summaries\n",
    "})\n",
    "\n",
    "output_path = \"./prediction/submit_solar_v5.csv\"\n",
    "submission.to_csv(output_path, index=False)\n",
    "print(f\"\\nâœ… ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "\n",
    "# í†µê³„\n",
    "lengths = [len(s.split()) for s in summaries]\n",
    "print(f\"\\nğŸ“Š ìƒì„± ìš”ì•½ í†µê³„:\")\n",
    "print(f\"  í‰ê·  ê¸¸ì´: {sum(lengths)/len(lengths):.1f} words\")\n",
    "print(f\"  ìµœì†Œ/ìµœëŒ€: {min(lengths)} / {max(lengths)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b21e5e",
   "metadata": {},
   "source": [
    "## 8. ì¶”ë¡  íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í—˜ (ì¬í•™ìŠµ ì—†ì´ ì ìˆ˜ í–¥ìƒ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eb2081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2 ëª¨ë¸ ì¶”ë¡  íŒŒë¼ë¯¸í„° ìµœì í™”\n",
    "# ëª©í‘œ: ROUGE-2 í–¥ìƒ (í˜„ì¬ 42.19% â†’ 42.5%+ ëª©í‘œ)\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ì‹¤í—˜í•  íŒŒë¼ë¯¸í„° ì¡°í•©\n",
    "EXPERIMENTS = {\n",
    "    \"exp1_simple_prompt\": {\n",
    "        \"prompt_type\": \"simple\",  # í”„ë¡¬í”„íŠ¸ ë‹¨ìˆœí™”\n",
    "        \"params\": {\n",
    "            \"max_new_tokens\": 150,\n",
    "            \"num_beams\": 3,\n",
    "            \"repetition_penalty\": 1.2,\n",
    "            \"length_penalty\": 1.3,\n",
    "        }\n",
    "    },\n",
    "    \"exp2_len_1.25\": {\n",
    "        \"prompt_type\": \"simple\",\n",
    "        \"params\": {\n",
    "            \"max_new_tokens\": 150,\n",
    "            \"num_beams\": 3,\n",
    "            \"repetition_penalty\": 1.2,\n",
    "            \"length_penalty\": 1.25,  # 1.3 â†’ 1.25\n",
    "        }\n",
    "    },\n",
    "    \"exp3_len_1.35\": {\n",
    "        \"prompt_type\": \"simple\",\n",
    "        \"params\": {\n",
    "            \"max_new_tokens\": 150,\n",
    "            \"num_beams\": 3,\n",
    "            \"repetition_penalty\": 1.2,\n",
    "            \"length_penalty\": 1.35,  # 1.3 â†’ 1.35\n",
    "        }\n",
    "    },\n",
    "    \"exp4_rep_1.15\": {\n",
    "        \"prompt_type\": \"simple\",\n",
    "        \"params\": {\n",
    "            \"max_new_tokens\": 150,\n",
    "            \"num_beams\": 3,\n",
    "            \"repetition_penalty\": 1.15,  # 1.2 â†’ 1.15\n",
    "            \"length_penalty\": 1.3,\n",
    "        }\n",
    "    },\n",
    "    \"exp5_rep_1.25\": {\n",
    "        \"prompt_type\": \"simple\",\n",
    "        \"params\": {\n",
    "            \"max_new_tokens\": 150,\n",
    "            \"num_beams\": 3,\n",
    "            \"repetition_penalty\": 1.25,  # 1.2 â†’ 1.25\n",
    "            \"length_penalty\": 1.3,\n",
    "        }\n",
    "    },\n",
    "    \"exp6_beam_4\": {\n",
    "        \"prompt_type\": \"simple\",\n",
    "        \"params\": {\n",
    "            \"max_new_tokens\": 150,\n",
    "            \"num_beams\": 4,  # 3 â†’ 4\n",
    "            \"repetition_penalty\": 1.2,\n",
    "            \"length_penalty\": 1.3,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "def generate_with_prompt_type(model, tokenizer, dialogue, params, prompt_type=\"simple\"):\n",
    "    \"\"\"í”„ë¡¬í”„íŠ¸ íƒ€ì…ë³„ ìƒì„±\"\"\"\n",
    "    if prompt_type == \"simple\":\n",
    "        # ë‹¨ìˆœí•œ í”„ë¡¬í”„íŠ¸ (v1 ìŠ¤íƒ€ì¼ì— ê°€ê¹ê²Œ)\n",
    "        prompt = f\"### User:\\nSummarize the following dialogue:\\n\\n{dialogue}\\n\\n### Assistant:\\n\"\n",
    "    else:\n",
    "        # ì›ë˜ v2 í”„ë¡¬í”„íŠ¸\n",
    "        prompt = f\"\"\"### User:\n",
    "ë‹¤ìŒ ëŒ€í™”ë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ì„¸ìš”. #Person1#, #Person2# ë“±ì˜ í™”ì í‘œì‹œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "### Assistant:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            **params,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    try:\n",
    "        summary = generated_text.split(\"### Assistant:\\n\")[1].strip()\n",
    "    except:\n",
    "        summary = generated_text\n",
    "    \n",
    "    return post_process(summary)\n",
    "\n",
    "print(\"ğŸ”¬ ì¶”ë¡  íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í—˜ ì‹œì‘\")\n",
    "print(f\"ì´ {len(EXPERIMENTS)}ê°œ ì‹¤í—˜ ì§„í–‰\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275371c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dev ë°ì´í„°ë¡œ ë¹ ë¥¸ ê²€ì¦ (ì‹œê°„ ì ˆì•½)\n",
    "# Devì—ì„œ ê°€ì¥ ì¢‹ì€ ì„¤ì •ì„ Testì— ì ìš©\n",
    "\n",
    "from evaluate import load\n",
    "rouge = load('rouge')\n",
    "\n",
    "def evaluate_on_dev(model, tokenizer, dev_df, params, prompt_type=\"simple\"):\n",
    "    \"\"\"Dev ë°ì´í„°ë¡œ ROUGE í‰ê°€\"\"\"\n",
    "    summaries = []\n",
    "    references = []\n",
    "    \n",
    "    print(f\"   Generating summaries on Dev ({len(dev_df)} samples)...\")\n",
    "    for idx, row in tqdm(dev_df.iterrows(), total=len(dev_df), leave=False):\n",
    "        summary = generate_with_prompt_type(model, tokenizer, row['dialogue'], params, prompt_type)\n",
    "        summaries.append(summary)\n",
    "        references.append(row['summary'])\n",
    "    \n",
    "    # ROUGE ê³„ì‚°\n",
    "    results = rouge.compute(predictions=summaries, references=references)\n",
    "    \n",
    "    return {\n",
    "        'rouge1': results['rouge1'] * 100,\n",
    "        'rouge2': results['rouge2'] * 100,\n",
    "        'rougeL': results['rougeL'] * 100,\n",
    "        'final': (results['rouge1'] + results['rouge2'] + results['rougeL']) / 3 * 100\n",
    "    }\n",
    "\n",
    "# Dev ë°ì´í„° ë¡œë“œ\n",
    "dev_df_eval = pd.read_csv(os.path.join(CONF['data_path'], 'dev.csv'))\n",
    "\n",
    "print(\"ğŸ“Š Dev ë°ì´í„° í‰ê°€ ì‹œì‘\\n\")\n",
    "results_summary = []\n",
    "\n",
    "for exp_name, exp_config in EXPERIMENTS.items():\n",
    "    print(f\"ğŸ§ª {exp_name}\")\n",
    "    print(f\"   Prompt: {exp_config['prompt_type']}\")\n",
    "    print(f\"   Params: {exp_config['params']}\")\n",
    "    \n",
    "    scores = evaluate_on_dev(\n",
    "        model, \n",
    "        tokenizer, \n",
    "        dev_df_eval, \n",
    "        exp_config['params'],\n",
    "        exp_config['prompt_type']\n",
    "    )\n",
    "    \n",
    "    results_summary.append({\n",
    "        'experiment': exp_name,\n",
    "        'prompt': exp_config['prompt_type'],\n",
    "        **exp_config['params'],\n",
    "        **scores\n",
    "    })\n",
    "    \n",
    "    print(f\"   âœ… ROUGE-1: {scores['rouge1']:.2f}, ROUGE-2: {scores['rouge2']:.2f}, ROUGE-L: {scores['rougeL']:.2f}\")\n",
    "    print(f\"   â­ Final: {scores['final']:.4f}\\n\")\n",
    "\n",
    "# ê²°ê³¼ ì •ë¦¬\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "results_df = results_df.sort_values('final', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“ˆ ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ (Final Score ìˆœ)\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\nğŸ† Best Setting:\")\n",
    "best = results_df.iloc[0]\n",
    "print(f\"   Experiment: {best['experiment']}\")\n",
    "print(f\"   Final Score: {best['final']:.4f}\")\n",
    "print(f\"   ROUGE-2: {best['rouge2']:.2f} (ëª©í‘œ: 42.5+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b8006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best ì„¤ì •ìœ¼ë¡œ Test ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "\n",
    "print(\"ğŸš€ Best ì„¤ì •ìœ¼ë¡œ Test ë°ì´í„° ì¶”ë¡  ì‹œì‘\")\n",
    "best_exp_name = results_df.iloc[0]['experiment']\n",
    "best_config = EXPERIMENTS[best_exp_name]\n",
    "\n",
    "print(f\"\\nğŸ“‹ Best Setting: {best_exp_name}\")\n",
    "print(f\"   Prompt Type: {best_config['prompt_type']}\")\n",
    "print(f\"   Parameters: {best_config['params']}\\n\")\n",
    "\n",
    "# Test ë°ì´í„° ì¶”ë¡ \n",
    "test_df = pd.read_csv(os.path.join(CONF['data_path'], 'test.csv'))\n",
    "test_summaries = []\n",
    "\n",
    "model.eval()\n",
    "for dialogue in tqdm(test_df['dialogue'], desc=\"Test Inference\"):\n",
    "    summary = generate_with_prompt_type(\n",
    "        model, \n",
    "        tokenizer, \n",
    "        dialogue, \n",
    "        best_config['params'],\n",
    "        best_config['prompt_type']\n",
    "    )\n",
    "    test_summaries.append(summary)\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ì €ì¥\n",
    "submission = pd.DataFrame({\n",
    "    'fname': test_df['fname'],\n",
    "    'summary': test_summaries\n",
    "})\n",
    "\n",
    "output_path = f\"./prediction/submit_solar_v2_optimized_{best_exp_name}.csv\"\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "\n",
    "# í†µê³„\n",
    "lengths = [len(s.split()) for s in test_summaries]\n",
    "print(f\"\\nğŸ“Š ìƒì„± ìš”ì•½ í†µê³„:\")\n",
    "print(f\"   í‰ê·  ê¸¸ì´: {sum(lengths)/len(lengths):.1f} words\")\n",
    "print(f\"   ìµœì†Œ/ìµœëŒ€: {min(lengths)} / {max(lengths)} words\")\n",
    "print(f\"\\nğŸ¯ ì˜ˆìƒ ì ìˆ˜ (Dev ê¸°ì¤€): {best['final']:.4f}\")\n",
    "print(f\"   ì´ì „ ìµœê³ : 51.8026\")\n",
    "print(f\"   ì´ì „ v2: 51.7703\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
