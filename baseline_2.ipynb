{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6n-Ps2nkVsBb"
   },
   "source": [
    "# **ğŸ’ğŸ»ğŸ—¨ï¸ğŸ’ğŸ»â€â™‚ï¸ëŒ€í™” ìš”ì•½ Baseline code**\n",
    "> **Dialogue Summarization** ê²½ì§„ëŒ€íšŒì— ì˜¤ì‹  ì—¬ëŸ¬ë¶„ í™˜ì˜í•©ë‹ˆë‹¤! ğŸ‰    \n",
    "> ë³¸ ëŒ€íšŒì—ì„œëŠ” ìµœì†Œ 2ëª…ì—ì„œ ìµœëŒ€ 7ëª…ì´ ë“±ì¥í•˜ì—¬ ë‚˜ëˆ„ëŠ” ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” BART ê¸°ë°˜ ëª¨ë¸ì˜ baseline codeë¥¼ ì œê³µí•©ë‹ˆë‹¤.     \n",
    "> ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì¼ìƒ ëŒ€í™”ì— ëŒ€í•œ ìš”ì•½ì„ íš¨ê³¼ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNq_LylZa1ug"
   },
   "source": [
    "## âš™ï¸ ë°ì´í„° ë° í™˜ê²½ì„¤ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjCiuI_V4glr"
   },
   "source": [
    "### 1) í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYqDF_-r2ToB"
   },
   "source": [
    "- í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•œ í›„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zbZ7SU9P2TYN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import yaml\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from rouge import Rouge # ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration, BartConfig\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "import wandb # ëª¨ë¸ í•™ìŠµ ê³¼ì •ì„ ì†ì‰½ê²Œ Trackingí•˜ê³ , ì‹œê°í™”í•  ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Qq46k6_CNQn"
   },
   "source": [
    "## ğŸ”§ ì„¤ì • ì •ì˜í•˜ê¸°\n",
    "\n",
    "### 3-1. Special Token ë™ì  ìˆ˜ì§‘ ë° ë“±ë¡\n",
    "- í•™ìŠµ ë°ì´í„°ì—ì„œ ì‹¤ì œ ë“±ì¥í•˜ëŠ” ëª¨ë“  ë§ˆìŠ¤í‚¹/ë°œí™”ì í† í°ì„ ìë™ ìˆ˜ì§‘\n",
    "- `additional_special_tokens`ë¡œ ë“±ë¡í•˜ì—¬ í† í°ì´ ë¶„í•´ë˜ì§€ ì•Šë„ë¡ í•¨\n",
    "- `model.resize_token_embeddings(len(tokenizer))` í˜¸ì¶œë¡œ ì„ë² ë”© ì‚¬ì´ì¦ˆ ì¡°ì •\n",
    "\n",
    "### 3-2. í”„ë¡¬í”„íŠ¸ ë¹Œë” (ë°œí™”ì/turn êµ¬ì¡° í™œìš©)\n",
    "- `build_summary_prompt()`: ìš”ì•½ íƒœìŠ¤í¬ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì•Œë ¤ì£¼ëŠ” í”„ë¡¬í”„íŠ¸ ì¶”ê°€\n",
    "- `prompt_style` ì˜µì…˜: \"default\", \"speaker_aware\", \"minimal\", \"none\"\n",
    "- ë°œí™”ì ìˆ˜ì— ë”°ë¥¸ ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„± ê°€ëŠ¥\n",
    "\n",
    "### 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬ (ìŠ¬ë­/ê°íƒ„ì‚¬/ì˜ì„±ì–´ ì •ê·œí™”)\n",
    "- `normalize_slang()`: ã…‹ã…‹â†’ì›ƒê¸°ë‹¤, ã… ã… â†’ìŠ¬í”„ë‹¤ ë“± êµ¬ì–´ì²´â†’ë¬¸ì–´ì²´ ë³€í™˜\n",
    "- ìš”ì•½ë¬¸ì´ ë¬¸ì–´ì²´ì´ë¯€ë¡œ, ëª¨ë¸ì´ ì§ì ‘ í•´ì„í•˜ëŠ” ê²ƒë³´ë‹¤ ë¯¸ë¦¬ ë³€í™˜í•˜ëŠ” ê²Œ íš¨ê³¼ì \n",
    "- `SLANG_MAP`: ì•½ 50ê°œ ì´ìƒì˜ ìŠ¬ë­/ê°íƒ„ì‚¬/ì¸í„°ë„· ìš©ì–´ ë§¤í•‘\n",
    "- `normalize_repeated_chars()`: ê³¼ë„í•œ ë°˜ë³µ ë¬¸ì ì •ê·œí™” (ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ â†’ ã…‹ã…‹ã…‹ã…‹)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197,
     "referenced_widgets": [
      "e920dbc173c045d1a32143349f1dff8e",
      "58c794fb7ce543a39fdf66d757f6eeab",
      "8a6464a355f7464c989033965d418a8a",
      "3645438ace1f4596a8dbc157b48c1521",
      "58001a60eacc44d5b38a68648adccde4",
      "6f5fde5b0ac840a18bd5cc380e564ff6",
      "45187decb58b4ad39ad532259c6277e5",
      "2307c6dcbe0141acb5e61baae19cade7",
      "4747b668e2fa4ab58a449446f80030f5",
      "14f6c91d6c634379b498586c51e606e0",
      "08d05bc20a96432badd459e1ffaf868e",
      "5dfcf310ca9e4e2794076098a5d69cea",
      "3c284a826f6843f6aa47eacad478ac30",
      "6caedd60c6b747469c82930be1f95d6d",
      "64f2218f899d446393cfea44f206f0a6",
      "d068f541df3f438dbd5138863e64b2f2",
      "affff1d8a89e4c14955d1b2aa39ff1ab",
      "13651c09564a4337b8274c1cb436faa5",
      "3bcd6b6b956347b29e1efa20a1d00542",
      "2fd3d7bbcd6948d8904d33001f95ea03",
      "d22fbc2c5dbf422399e496c9b500025a",
      "775d8bbeceac4e2da4f21ab6235c89ed",
      "de1a3f7701c243839fe03b930a9b9e30",
      "ebc22683058a4f229c5588e52fc93536",
      "52095cc7087243ac916055e569fd22f3",
      "a15af9e8158f4903b9189f3d322a5ef3",
      "21d2e54b5a0a4f79973a512105da43eb",
      "083ea69907bb48d4a8fff919bac51aad",
      "2a190bda0b72407e9a953cd2104dd3b2",
      "c18f0e3bc35e44d9915c3f84cd282a26",
      "3a04e871b74b45d7bf02fd33bb103577",
      "ac00d6c2cf974b33a628acb3f1471316",
      "285007b45236478ca147c6df752c8da4"
     ]
    },
    "id": "gZOE9TInCQHJ",
    "outputId": "8ce58487-6199-408c-cb37-49af1e218bc2"
   },
   "outputs": [],
   "source": [
    "# config ì„¤ì •ì— tokenizer ëª¨ë“ˆì´ ì‚¬ìš©ë˜ë¯€ë¡œ ë¯¸ë¦¬ tokenizerë¥¼ ì •ì˜í•´ì¤ë‹ˆë‹¤.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vsACJI7CVb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”– ë™ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ Special Tokens:\n",
      "============================================================\n",
      "ì´ 22ê°œ: ['#Address#', '#Alex#', '#Bob#', '#CarNumber#', '#CardNumber#', '#DateOfBirth#', '#Email#', '#Kristin#', '#Liliana#', '#Name#', '#PassportNumber#', '#Person1#', '#Person2#', '#Person3#', '#Person4#', '#Person5#', '#Person6#', '#Person7#', '#PersonName#', '#PhoneNumber#', '#Price#', '#SSN#']\n",
      "\n",
      "============================================================\n",
      "ğŸ” 6. ì •ê·œí‘œí˜„ì‹ ê¸°ë°˜ íŒ¨í„´ í™œìš© ì „ëµ:\n",
      "============================================================\n",
      "  6-1. í™”ì ìˆ˜ ëª…ì‹œ: True\n",
      "  6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: True\n",
      "\n",
      "ğŸ“Š ëŒ€í™” êµ¬ì¡° ë¶„ì„ ì˜ˆì‹œ:\n",
      "  - í™”ì ìˆ˜: 2ëª…\n",
      "  - í™”ì í† í°: {'#Person1#', '#Person2#'}\n",
      "  - PII ë§ˆìŠ¤í‚¹ ì—¬ë¶€: False\n",
      "  - PII ë§ˆìŠ¤í‚¹ ì˜ˆì‹œ: []\n",
      "\n",
      "============================================================\n",
      "ğŸ—£ï¸ êµ¬ì–´ì²´ ì „ì²˜ë¦¬ (ìŠ¬ë­/ê°íƒ„ì‚¬ ì •ê·œí™”):\n",
      "============================================================\n",
      "ì´ 59ê°œ ìŠ¬ë­ ë§¤í•‘ ì •ì˜\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™”:\n",
      "============================================================\n",
      "âœ… TF-IDF í•™ìŠµ ì™„ë£Œ: 2000ê°œ feature\n",
      "\n",
      "ğŸ“ TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì˜ˆì‹œ:\n",
      "  í‚¤ì›Œë“œ: ë§¤ë…„ / í•œ / ë²ˆ / í•´ìš” / ì˜ì‚¬ ì„ ìƒë‹˜\n",
      "\n",
      "============================================================\n",
      "ğŸ“ ê¸¸ì´/ì£¼ì œ ê¸°ë°˜ ë™ì  ì¶”ë¡  ì„¤ì •:\n",
      "============================================================\n",
      "ê¸¸ì´ ì¹´í…Œê³ ë¦¬:\n",
      "  - short: ~500ì, max_tokens=60, beams=4\n",
      "  - medium: ~1500ì, max_tokens=80, beams=5\n",
      "  - long: ~infì, max_tokens=100, beams=6\n",
      "\n",
      "ì£¼ì œ ì¹´í…Œê³ ë¦¬:\n",
      "  - medical: max_tokens=100, length_penalty=1.3\n",
      "  - insurance: max_tokens=100, length_penalty=1.3\n",
      "  - finance: max_tokens=95, length_penalty=1.2\n",
      "  - travel: max_tokens=85, length_penalty=1.1\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ ì „ì²´ ì„¤ì • ìš”ì•½:\n",
      "============================================================\n",
      "\n",
      "ğŸ”– 3-1. Special Tokens:\n",
      "  - ë™ì  ìˆ˜ì§‘: 22ê°œ\n",
      "\n",
      "ğŸ“ 3-2. í”„ë¡¬í”„íŠ¸ ì„¤ì •:\n",
      "  - í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: True\n",
      "  - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: balanced\n",
      "  - TF-IDF í‚¤ì›Œë“œ ìˆ˜: 5ê°œ\n",
      "\n",
      "ğŸ” 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš© ì „ëµ:\n",
      "  - 6-1. í™”ì ìˆ˜ ëª…ì‹œ: True\n",
      "  - 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: True\n",
      "\n",
      "ğŸ—£ï¸ 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬:\n",
      "  - ìŠ¬ë­ ì •ê·œí™”: True\n",
      "\n",
      "ğŸ“ 5-2. ë™ì  ì¶”ë¡ :\n",
      "  - ê¸¸ì´ ê¸°ë°˜: True\n",
      "  - ì£¼ì œ ê¸°ë°˜: True\n",
      "\n",
      "------------------------------------------------------------\n",
      "ğŸ“ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ (6-1, 6-2 ì ìš©):\n",
      "------------------------------------------------------------\n",
      "[1. ì›ë³¸ ëŒ€í™” (ì• 150ì)]:\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? \n",
      "#Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. \n",
      "#Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. \n",
      "#Person...\n",
      "\n",
      "[2. ëŒ€í™” êµ¬ì¡° ë¶„ì„]:\n",
      "  - í™”ì ìˆ˜: 2ëª…\n",
      "  - PII ë§ˆìŠ¤í‚¹ í† í°: []\n",
      "\n",
      "[3. ìŠ¬ë­ ì •ê·œí™” í›„]:\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? #Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. #Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. #Person2#:...\n",
      "\n",
      "[4. TF-IDF í‚¤ì›Œë“œ]: ë§¤ë…„ / í•œ / ë²ˆ / í•´ìš” / ì˜ì‚¬ ì„ ìƒë‹˜\n",
      "\n",
      "[5. 'balanced' í”„ë¡¬í”„íŠ¸ ì ìš© (6-1, 6-2)]:\n",
      "ë‹¤ìŒì€ 2ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ë‘ ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \n",
      "\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? #Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. #Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. #Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”. #Person1#: ìŒ, ì‹¬ê°í•œ ì§ˆë³‘ì„ í”¼í•˜ë ¤ë©´ ë¯¸ë¦¬ ë°œê²¬í•˜ëŠ” ê²Œ ì œì¼ ì¢‹ê±°ë“ ìš”. ë³¸ì¸ì„ ìœ„í•´ì„œë¼ë„ ë§¤ë…„ í•œ ë²ˆì€ ì˜¤ì„¸ìš”. #Person2#: ì•Œê² ìŠµë‹ˆë‹¤. #Person1#: ì—¬ê¸° ì¢€ ë³¼ê¹Œìš”. ëˆˆê³¼ ê·€ëŠ” ê´œì°®ìœ¼ì‹œë„¤ìš”. ê¹Šê²Œ ìˆ¨ í•œ ë²ˆ ì‰¬ì–´ë³´ì„¸ìš”. Mr. Smith, ë‹´ë°° í”¼ìš°ì„¸ìš”? #Pe...\n",
      "\n",
      "[6. 'keyword_aware' í”„ë¡¬í”„íŠ¸ ì ìš© (TF-IDF + 6-1, 6-2)]:\n",
      "ë‹¤ìŒì€ 2ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ë‘ ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. ì£¼ìš” í‚¤ì›Œë“œ: ë§¤ë…„ / í•œ / ë²ˆ / í•´ìš” / ì˜ì‚¬ ì„ ìƒë‹˜. \n",
      "\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? #Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. #Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. #Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”. #Person1#: ìŒ, ì‹¬ê°í•œ ì§ˆë³‘ì„ í”¼í•˜ë ¤ë©´ ë¯¸ë¦¬ ë°œê²¬í•˜ëŠ” ê²Œ ì œì¼ ì¢‹ê±°ë“ ìš”. ë³¸ì¸ì„ ìœ„í•´ì„œë¼ë„ ë§¤ë…„ í•œ ë²ˆì€ ì˜¤ì„¸ìš”. #Person2#: ì•Œê² ìŠµë‹ˆë‹¤. #Person1#: ì—¬ê¸° ì¢€ ë³¼ê¹Œìš”. ëˆˆê³¼ ê·€ëŠ” ê´œì°®ìœ¼ì‹œë„¤ìš”. ê¹Šê²Œ ìˆ¨ ...\n",
      "\n",
      "[7. ë™ì  ì¶”ë¡  ì„¤ì •]:\n",
      "  - ê¸¸ì´ ì¹´í…Œê³ ë¦¬: medium\n",
      "  - ê°ì§€ëœ ì£¼ì œ: medical\n",
      "  - max_new_tokens: 100\n",
      "\n",
      "âœ… decode_with_speaker_tags, postprocess_summary í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n",
      "âœ… TF-IDF í•™ìŠµ ì™„ë£Œ: 2000ê°œ feature\n",
      "\n",
      "ğŸ“ TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì˜ˆì‹œ:\n",
      "  í‚¤ì›Œë“œ: ë§¤ë…„ / í•œ / ë²ˆ / í•´ìš” / ì˜ì‚¬ ì„ ìƒë‹˜\n",
      "\n",
      "============================================================\n",
      "ğŸ“ ê¸¸ì´/ì£¼ì œ ê¸°ë°˜ ë™ì  ì¶”ë¡  ì„¤ì •:\n",
      "============================================================\n",
      "ê¸¸ì´ ì¹´í…Œê³ ë¦¬:\n",
      "  - short: ~500ì, max_tokens=60, beams=4\n",
      "  - medium: ~1500ì, max_tokens=80, beams=5\n",
      "  - long: ~infì, max_tokens=100, beams=6\n",
      "\n",
      "ì£¼ì œ ì¹´í…Œê³ ë¦¬:\n",
      "  - medical: max_tokens=100, length_penalty=1.3\n",
      "  - insurance: max_tokens=100, length_penalty=1.3\n",
      "  - finance: max_tokens=95, length_penalty=1.2\n",
      "  - travel: max_tokens=85, length_penalty=1.1\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ ì „ì²´ ì„¤ì • ìš”ì•½:\n",
      "============================================================\n",
      "\n",
      "ğŸ”– 3-1. Special Tokens:\n",
      "  - ë™ì  ìˆ˜ì§‘: 22ê°œ\n",
      "\n",
      "ğŸ“ 3-2. í”„ë¡¬í”„íŠ¸ ì„¤ì •:\n",
      "  - í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: True\n",
      "  - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: balanced\n",
      "  - TF-IDF í‚¤ì›Œë“œ ìˆ˜: 5ê°œ\n",
      "\n",
      "ğŸ” 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš© ì „ëµ:\n",
      "  - 6-1. í™”ì ìˆ˜ ëª…ì‹œ: True\n",
      "  - 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: True\n",
      "\n",
      "ğŸ—£ï¸ 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬:\n",
      "  - ìŠ¬ë­ ì •ê·œí™”: True\n",
      "\n",
      "ğŸ“ 5-2. ë™ì  ì¶”ë¡ :\n",
      "  - ê¸¸ì´ ê¸°ë°˜: True\n",
      "  - ì£¼ì œ ê¸°ë°˜: True\n",
      "\n",
      "------------------------------------------------------------\n",
      "ğŸ“ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ (6-1, 6-2 ì ìš©):\n",
      "------------------------------------------------------------\n",
      "[1. ì›ë³¸ ëŒ€í™” (ì• 150ì)]:\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? \n",
      "#Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. \n",
      "#Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. \n",
      "#Person...\n",
      "\n",
      "[2. ëŒ€í™” êµ¬ì¡° ë¶„ì„]:\n",
      "  - í™”ì ìˆ˜: 2ëª…\n",
      "  - PII ë§ˆìŠ¤í‚¹ í† í°: []\n",
      "\n",
      "[3. ìŠ¬ë­ ì •ê·œí™” í›„]:\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? #Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. #Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. #Person2#:...\n",
      "\n",
      "[4. TF-IDF í‚¤ì›Œë“œ]: ë§¤ë…„ / í•œ / ë²ˆ / í•´ìš” / ì˜ì‚¬ ì„ ìƒë‹˜\n",
      "\n",
      "[5. 'balanced' í”„ë¡¬í”„íŠ¸ ì ìš© (6-1, 6-2)]:\n",
      "ë‹¤ìŒì€ 2ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ë‘ ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \n",
      "\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? #Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. #Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. #Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”. #Person1#: ìŒ, ì‹¬ê°í•œ ì§ˆë³‘ì„ í”¼í•˜ë ¤ë©´ ë¯¸ë¦¬ ë°œê²¬í•˜ëŠ” ê²Œ ì œì¼ ì¢‹ê±°ë“ ìš”. ë³¸ì¸ì„ ìœ„í•´ì„œë¼ë„ ë§¤ë…„ í•œ ë²ˆì€ ì˜¤ì„¸ìš”. #Person2#: ì•Œê² ìŠµë‹ˆë‹¤. #Person1#: ì—¬ê¸° ì¢€ ë³¼ê¹Œìš”. ëˆˆê³¼ ê·€ëŠ” ê´œì°®ìœ¼ì‹œë„¤ìš”. ê¹Šê²Œ ìˆ¨ í•œ ë²ˆ ì‰¬ì–´ë³´ì„¸ìš”. Mr. Smith, ë‹´ë°° í”¼ìš°ì„¸ìš”? #Pe...\n",
      "\n",
      "[6. 'keyword_aware' í”„ë¡¬í”„íŠ¸ ì ìš© (TF-IDF + 6-1, 6-2)]:\n",
      "ë‹¤ìŒì€ 2ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ë‘ ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. ì£¼ìš” í‚¤ì›Œë“œ: ë§¤ë…„ / í•œ / ë²ˆ / í•´ìš” / ì˜ì‚¬ ì„ ìƒë‹˜. \n",
      "\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? #Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. #Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. #Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”. #Person1#: ìŒ, ì‹¬ê°í•œ ì§ˆë³‘ì„ í”¼í•˜ë ¤ë©´ ë¯¸ë¦¬ ë°œê²¬í•˜ëŠ” ê²Œ ì œì¼ ì¢‹ê±°ë“ ìš”. ë³¸ì¸ì„ ìœ„í•´ì„œë¼ë„ ë§¤ë…„ í•œ ë²ˆì€ ì˜¤ì„¸ìš”. #Person2#: ì•Œê² ìŠµë‹ˆë‹¤. #Person1#: ì—¬ê¸° ì¢€ ë³¼ê¹Œìš”. ëˆˆê³¼ ê·€ëŠ” ê´œì°®ìœ¼ì‹œë„¤ìš”. ê¹Šê²Œ ìˆ¨ ...\n",
      "\n",
      "[7. ë™ì  ì¶”ë¡  ì„¤ì •]:\n",
      "  - ê¸¸ì´ ì¹´í…Œê³ ë¦¬: medium\n",
      "  - ê°ì§€ëœ ì£¼ì œ: medical\n",
      "  - max_new_tokens: 100\n",
      "\n",
      "âœ… decode_with_speaker_tags, postprocess_summary í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3-1. Special Token ë™ì  ìˆ˜ì§‘ ë° ë“±ë¡\n",
    "# ============================================================================\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# ë°ì´í„° ê²½ë¡œ ì •ì˜ (ì»¤ë„ ì¬ì‹œì‘ í›„ì—ë„ ë™ì‘í•˜ë„ë¡)\n",
    "data_path = \"./data/\"\n",
    "\n",
    "# ë§ˆìŠ¤í‚¹ í† í° & ë°œí™”ì í† í° íŒ¨í„´ ì •ì˜\n",
    "MASK_PAT = r\"#\\w+#\"      # ëª¨ë“  #...# í˜•íƒœì˜ í† í°\n",
    "PERSON_PAT = r\"#Person\\d+#\"  # ë°œí™”ì í† í°ë§Œ\n",
    "# 6-1, 6-2ì—ì„œ í™œìš©í•  ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹ íŒ¨í„´ (ë°œí™”ì ì œì™¸)\n",
    "PII_MASK_PAT = r\"#(?!Person\\d+#)\\w+#\"  # #Person1# ë“± ì œì™¸í•œ ë§ˆìŠ¤í‚¹ í† í°\n",
    "\n",
    "def get_mask_tokens(text):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ì—ì„œ ëª¨ë“  ë§ˆìŠ¤í‚¹ í† í° ì¶”ì¶œ (#Word# í˜•íƒœ)\"\"\"\n",
    "    return re.findall(MASK_PAT, text)\n",
    "\n",
    "def get_person_tokens(text):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ì—ì„œ ë°œí™”ì í† í°ë§Œ ì¶”ì¶œ (#PersonN# í˜•íƒœ)\"\"\"\n",
    "    return re.findall(PERSON_PAT, text)\n",
    "\n",
    "def get_pii_mask_tokens(text):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ì—ì„œ ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹ í† í°ë§Œ ì¶”ì¶œ (ë°œí™”ì ì œì™¸)\"\"\"\n",
    "    return re.findall(PII_MASK_PAT, text)\n",
    "\n",
    "def collect_special_tokens_from_data(dialogues):\n",
    "    \"\"\"\n",
    "    í•™ìŠµ ë°ì´í„°ì—ì„œ ì‹¤ì œ ë“±ì¥í•˜ëŠ” ëª¨ë“  special token ìˆ˜ì§‘\n",
    "    - #Person1#, #Person2# ë“± ë°œí™”ì í† í°\n",
    "    - #PhoneNumber#, #Address# ë“± ë§ˆìŠ¤í‚¹ í† í°\n",
    "    \"\"\"\n",
    "    all_tokens = set()\n",
    "    for d in dialogues:\n",
    "        all_tokens.update(get_mask_tokens(d))\n",
    "    return sorted(list(all_tokens))\n",
    "\n",
    "# Train ë°ì´í„°ì—ì„œ special tokens ìˆ˜ì§‘\n",
    "train_file = os.path.join(data_path, 'train.csv')\n",
    "train_df_for_tokens = pd.read_csv(train_file)\n",
    "DYNAMIC_SPECIAL_TOKENS = collect_special_tokens_from_data(train_df_for_tokens['dialogue'])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ”– ë™ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ Special Tokens:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ì´ {len(DYNAMIC_SPECIAL_TOKENS)}ê°œ: {DYNAMIC_SPECIAL_TOKENS}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. ì •ê·œí‘œí˜„ì‹ ê¸°ë°˜ íŒ¨í„´ í™œìš© ì „ëµ\n",
    "# ============================================================================\n",
    "# 6-1. í™”ì ìˆ˜ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ì— í¬í•¨\n",
    "# 6-2. ë§ˆìŠ¤í‚¹ëœ ì •ë³´(#PhoneNumber# ë“±)ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë¼ê³  ì§€ì‹œ\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_dialogue_structure(dialogue_text):\n",
    "    \"\"\"\n",
    "    ëŒ€í™” êµ¬ì¡° ë¶„ì„: í™”ì ìˆ˜, ë§ˆìŠ¤í‚¹ í† í° ì •ë³´ ì¶”ì¶œ\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            'speakers': set of speaker tokens,\n",
    "            'num_speakers': number of speakers,\n",
    "            'pii_masks': set of PII masking tokens,\n",
    "            'has_pii_masks': whether PII masks exist\n",
    "        }\n",
    "    \"\"\"\n",
    "    speakers = set(get_person_tokens(dialogue_text))\n",
    "    pii_masks = set(get_pii_mask_tokens(dialogue_text))\n",
    "    \n",
    "    return {\n",
    "        'speakers': speakers,\n",
    "        'num_speakers': len(speakers) if speakers else 2,  # ê¸°ë³¸ê°’ 2ëª…\n",
    "        'pii_masks': pii_masks,\n",
    "        'has_pii_masks': len(pii_masks) > 0,\n",
    "        'pii_mask_examples': list(pii_masks)[:3]  # ì˜ˆì‹œ ìµœëŒ€ 3ê°œ\n",
    "    }\n",
    "\n",
    "def build_speaker_aware_prompt(num_speakers):\n",
    "    \"\"\"\n",
    "    6-1. í™”ì ìˆ˜ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì•Œë ¤ì£¼ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "    - \"í•œ ì‚¬ëŒ ì´ì•¼ê¸°ë§Œ ìš”ì•½í•˜ëŠ”\" ë²„ê·¸ ë°©ì§€\n",
    "    \"\"\"\n",
    "    if num_speakers == 1:\n",
    "        return \"ë‹¤ìŒì€ 1ëª…ì˜ ë…ë°±ì…ë‹ˆë‹¤. \"\n",
    "    elif num_speakers == 2:\n",
    "        return f\"ë‹¤ìŒì€ 2ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ë‘ ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \"\n",
    "    else:\n",
    "        return f\"ë‹¤ìŒì€ {num_speakers}ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ê° ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \"\n",
    "\n",
    "def build_pii_mask_instruction(pii_masks, show_examples=True):\n",
    "    \"\"\"\n",
    "    6-2. ë§ˆìŠ¤í‚¹ëœ ì •ë³´ ìœ ì§€ ì§€ì‹œ í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "    - ìš”ì•½ë¬¸ì—ì„œ ì—‰ëš±í•œ ì „í™”ë²ˆí˜¸/ì£¼ì†Œ ìƒì„± ë°©ì§€\n",
    "    - ROUGE ì ìˆ˜ í–¥ìƒ (ë ˆí¼ëŸ°ìŠ¤ì™€ ë™ì¼í•œ ë§ˆìŠ¤í‚¹ í† í° ìœ ì§€)\n",
    "    \"\"\"\n",
    "    if not pii_masks:\n",
    "        return \"\"\n",
    "    \n",
    "    if show_examples:\n",
    "        examples = list(pii_masks)[:3]  # ìµœëŒ€ 3ê°œ ì˜ˆì‹œ\n",
    "        examples_str = \", \".join(examples)\n",
    "        return (\n",
    "            f\"ëŒ€í™” ì¤‘ ê°œì¸ì •ë³´ëŠ” {examples_str} ì™€ ê°™ì´ ë§ˆìŠ¤í‚¹ë˜ì–´ ìˆìŠµë‹ˆë‹¤. \"\n",
    "            \"ìš”ì•½ë¬¸ì—ì„œë„ ì´ ë§ˆìŠ¤í‚¹ í‘œí˜„ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”. \"\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            \"ëŒ€í™” ì¤‘ ê°œì¸ì •ë³´ëŠ” ì´ë¯¸ ë§ˆìŠ¤í‚¹ë˜ì–´ ìˆìŠµë‹ˆë‹¤. \"\n",
    "            \"ìš”ì•½ë¬¸ì—ì„œë„ ë§ˆìŠ¤í‚¹ í‘œí˜„ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•´ ì£¼ì„¸ìš”. \"\n",
    "        )\n",
    "\n",
    "# 6. ì„¤ì •\n",
    "REGEX_PATTERN_CONFIG = {\n",
    "    \"use_speaker_count\": True,      # 6-1. í™”ì ìˆ˜ ëª…ì‹œ\n",
    "    \"use_pii_mask_instruction\": True,  # 6-2. ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ\n",
    "    \"show_pii_examples\": True,       # ë§ˆìŠ¤í‚¹ ì˜ˆì‹œ í‘œì‹œ ì—¬ë¶€\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ” 6. ì •ê·œí‘œí˜„ì‹ ê¸°ë°˜ íŒ¨í„´ í™œìš© ì „ëµ:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  6-1. í™”ì ìˆ˜ ëª…ì‹œ: {REGEX_PATTERN_CONFIG['use_speaker_count']}\")\n",
    "print(f\"  6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: {REGEX_PATTERN_CONFIG['use_pii_mask_instruction']}\")\n",
    "\n",
    "# ì˜ˆì‹œ í…ŒìŠ¤íŠ¸\n",
    "sample_analysis = analyze_dialogue_structure(train_df_for_tokens['dialogue'].iloc[0])\n",
    "print(f\"\\nğŸ“Š ëŒ€í™” êµ¬ì¡° ë¶„ì„ ì˜ˆì‹œ:\")\n",
    "print(f\"  - í™”ì ìˆ˜: {sample_analysis['num_speakers']}ëª…\")\n",
    "print(f\"  - í™”ì í† í°: {sample_analysis['speakers']}\")\n",
    "print(f\"  - PII ë§ˆìŠ¤í‚¹ ì—¬ë¶€: {sample_analysis['has_pii_masks']}\")\n",
    "print(f\"  - PII ë§ˆìŠ¤í‚¹ ì˜ˆì‹œ: {sample_analysis['pii_mask_examples']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬: ìŠ¬ë­/ê°íƒ„ì‚¬/ì˜ì„±ì–´ ì •ê·œí™”\n",
    "# ============================================================================\n",
    "\n",
    "# ìŠ¬ë­/ê°íƒ„ì‚¬/ì˜ì„±ì–´ â†’ ë¬¸ì–´ì²´ ë§¤í•‘ ì‚¬ì „\n",
    "SLANG_MAP = {\n",
    "    # ì›ƒìŒ í‘œí˜„\n",
    "    \"ã…‹ã…‹ã…‹ã…‹ã…‹\": \"ë§¤ìš° ì›ƒê¸°ë‹¤\",\n",
    "    \"ã…‹ã…‹ã…‹ã…‹\": \"ë§¤ìš° ì›ƒê¸°ë‹¤\",\n",
    "    \"ã…‹ã…‹ã…‹\": \"ì›ƒê¸°ë‹¤\",\n",
    "    \"ã…‹ã…‹\": \"ì›ƒê¸°ë‹¤\",\n",
    "    \"ã…ã…ã…ã…\": \"ì›ƒê¸°ë‹¤\",\n",
    "    \"ã…ã…ã…\": \"ì›ƒê¸°ë‹¤\",\n",
    "    \"ã…ã…\": \"ì›ƒê¸°ë‹¤\",\n",
    "    \"ã…‹\": \"\",  # ë‹¨ì¼ ã…‹ëŠ” ì œê±°\n",
    "    \"ã…\": \"\",  # ë‹¨ì¼ ã…ëŠ” ì œê±°\n",
    "    \n",
    "    # ìŠ¬í””/ìš¸ìŒ í‘œí˜„\n",
    "    \"ã… ã… ã… ã… \": \"ë§¤ìš° ìŠ¬í”„ë‹¤\",\n",
    "    \"ã… ã… ã… \": \"ìŠ¬í”„ë‹¤\",\n",
    "    \"ã… ã… \": \"ìŠ¬í”„ë‹¤\",\n",
    "    \"ã…œã…œã…œã…œ\": \"ë§¤ìš° ìŠ¬í”„ë‹¤\",\n",
    "    \"ã…œã…œã…œ\": \"ìŠ¬í”„ë‹¤\",\n",
    "    \"ã…œã…œ\": \"ìŠ¬í”„ë‹¤\",\n",
    "    \"ã… \": \"\",\n",
    "    \"ã…œ\": \"\",\n",
    "    \n",
    "    # ë†€ëŒ/ê°íƒ„ í‘œí˜„\n",
    "    \"í—\": \"ë†€ëë‹¤\",\n",
    "    \"í—ã…‹\": \"ë†€ëë‹¤\",\n",
    "    \"ëŒ€ë°•\": \"ë†€ëë‹¤\",\n",
    "    \"ëŒ€ë°•ã…‹ã…‹\": \"ë†€ëë‹¤\",\n",
    "    \"ì™€\": \"ë†€ëë‹¤\",\n",
    "    \"ì™€ìš°\": \"ë†€ëë‹¤\",\n",
    "    \"ìš°ì™€\": \"ë†€ëë‹¤\",\n",
    "    \"ì˜¤ì˜¤\": \"ë†€ëë‹¤\",\n",
    "    \"ì˜¤ì˜¤ì˜¤\": \"ë§¤ìš° ë†€ëë‹¤\",\n",
    "    \"ã…‡ã…‡\": \"ê·¸ë ‡ë‹¤\",\n",
    "    \"ã„´ã„´\": \"ì•„ë‹ˆë‹¤\",\n",
    "    \n",
    "    # ê¸ì •/ë™ì˜ í‘œí˜„\n",
    "    \"ã„±ã„±\": \"ê°€ì\",\n",
    "    \"ã„±ã„±ã„±\": \"ë¹¨ë¦¬ ê°€ì\",\n",
    "    \"ã…‡ã…‹\": \"ì•Œê² ë‹¤\",\n",
    "    \"ã…‡ã…‹ã…‡ã…‹\": \"ì•Œê² ë‹¤\",\n",
    "    \"ë„¹\": \"ë„¤\",\n",
    "    \"ë„µ\": \"ë„¤\",\n",
    "    \"ìš¤\": \"ìš”\",\n",
    "    \"ìš©\": \"ìš”\",\n",
    "    \"ì¥¬\": \"ì¤˜\",\n",
    "    \n",
    "    # ë¶€ì •/ì§œì¦ í‘œí˜„\n",
    "    \"ì•„ã…‹ã…‹\": \"ì•„\",\n",
    "    \"ì—íœ´\": \"í•œìˆ¨\",\n",
    "    \"ã…¡ã…¡\": \"ì§œì¦ë‚œë‹¤\",\n",
    "    \";;\": \"ë‹¹í™©ìŠ¤ëŸ½ë‹¤\",\n",
    "    \";;;\": \"ë§¤ìš° ë‹¹í™©ìŠ¤ëŸ½ë‹¤\",\n",
    "    \n",
    "    # ê°•ì¡° í‘œí˜„\n",
    "    \"ì§„ì§œã…‹ã…‹\": \"ì •ë§\",\n",
    "    \"ë ˆì•Œ\": \"ì •ë§\",\n",
    "    \"ë¦¬ì–¼\": \"ì •ë§\",\n",
    "    \"ì°\": \"ì§„ì§œ\",\n",
    "    \"ì©ë‹¤\": \"ëŒ€ë‹¨í•˜ë‹¤\",\n",
    "    \"ì©”ì–´\": \"ëŒ€ë‹¨í•˜ë‹¤\",\n",
    "    \"ì§±\": \"ìµœê³ ë‹¤\",\n",
    "    \n",
    "    # ê¸°íƒ€ ì¸í„°ë„· ìš©ì–´\n",
    "    \"ã„·ã„·\": \"ë†€ëë‹¤\",\n",
    "    \"ã„·ã„·ã„·\": \"ë§¤ìš° ë†€ëë‹¤\",\n",
    "    \"ã…‚ã…‚\": \"ì•ˆë…•\",\n",
    "    \"ã…ƒã…ƒ\": \"ì•ˆë…•\",\n",
    "    \"ã„³\": \"ê°ì‚¬\",\n",
    "    \"ã„±ã……\": \"ê°ì‚¬\",\n",
    "    \"ã…ˆã……\": \"ì£„ì†¡\",\n",
    "    \"ã…ã„¹\": \"ëª¨ë¥´ê² ë‹¤\",\n",
    "    \"ã„¹ã…‡\": \"ì •ë§\",\n",
    "    \"ã…‡ã„·\": \"ì–´ë””\",\n",
    "}\n",
    "\n",
    "# ë°˜ë³µ ë¬¸ì ì •ê·œí™” íŒ¨í„´\n",
    "REPEAT_PATTERNS = [\n",
    "    (r'ã…‹{5,}', 'ã…‹ã…‹ã…‹ã…‹'),\n",
    "    (r'ã…{5,}', 'ã…ã…ã…ã…'),\n",
    "    (r'ã… {5,}', 'ã… ã… ã… ã… '),\n",
    "    (r'ã…œ{5,}', 'ã…œã…œã…œã…œ'),\n",
    "    (r'\\.{4,}', '...'),\n",
    "    (r'!{3,}', '!!'),\n",
    "    (r'\\?{3,}', '??'),\n",
    "    (r'~{3,}', '~~'),\n",
    "]\n",
    "\n",
    "def normalize_repeated_chars(text):\n",
    "    \"\"\"ë°˜ë³µ ë¬¸ìë¥¼ ì •ê·œí™”\"\"\"\n",
    "    for pattern, replacement in REPEAT_PATTERNS:\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "    return text\n",
    "\n",
    "def normalize_slang(text):\n",
    "    \"\"\"ìŠ¬ë­/ê°íƒ„ì‚¬/ì˜ì„±ì–´ë¥¼ ë¬¸ì–´ì²´ë¡œ ë³€í™˜\"\"\"\n",
    "    text = normalize_repeated_chars(text)\n",
    "    sorted_slang = sorted(SLANG_MAP.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "    for slang, replacement in sorted_slang:\n",
    "        if slang in text:\n",
    "            if replacement:\n",
    "                text = text.replace(slang, f\" {replacement} \")\n",
    "            else:\n",
    "                text = text.replace(slang, \" \")\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def preprocess_dialogue(text, normalize_slang_flag=True):\n",
    "    \"\"\"ëŒ€í™” ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return text\n",
    "    if normalize_slang_flag:\n",
    "        text = normalize_slang(text)\n",
    "    return text\n",
    "\n",
    "PREPROCESS_CONFIG = {\n",
    "    \"normalize_slang\": True,\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ—£ï¸ êµ¬ì–´ì²´ ì „ì²˜ë¦¬ (ìŠ¬ë­/ê°íƒ„ì‚¬ ì •ê·œí™”):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ì´ {len(SLANG_MAP)}ê°œ ìŠ¬ë­ ë§¤í•‘ ì •ì˜\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ë° í”„ë¡¬í”„íŠ¸ í™œìš©\n",
    "# ============================================================================\n",
    "# (sklearnì€ ì…€ ìƒë‹¨ì—ì„œ ì´ë¯¸ importë¨)\n",
    "\n",
    "# TF-IDF ì„¤ì •\n",
    "TFIDF_CONFIG = {\n",
    "    \"min_df\": 2,           # ìµœì†Œ ë¬¸ì„œ ë¹ˆë„\n",
    "    \"max_df\": 0.9,         # ìµœëŒ€ ë¬¸ì„œ ë¹ˆë„ (90% ì´ìƒ ë“±ì¥ ë‹¨ì–´ ì œì™¸)\n",
    "    \"max_features\": 2000,  # ìµœëŒ€ feature ìˆ˜\n",
    "    \"ngram_range\": (1, 2), # unigram + bigram\n",
    "    \"top_k\": 5,            # ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š TF-IDF ì„¤ì •:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  min_df: {TFIDF_CONFIG['min_df']}\")\n",
    "print(f\"  max_df: {TFIDF_CONFIG['max_df']}\")\n",
    "print(f\"  max_features: {TFIDF_CONFIG['max_features']}\")\n",
    "print(f\"  ngram_range: {TFIDF_CONFIG['ngram_range']}\")\n",
    "print(f\"  top_k: {TFIDF_CONFIG['top_k']}\")\n",
    "\n",
    "# TF-IDF Vectorizer ì´ˆê¸°í™” (í•œêµ­ì–´ ë¶ˆìš©ì–´ í¬í•¨)\n",
    "KOREAN_STOPWORDS = [\n",
    "    'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼',\n",
    "    'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 'ì´ë‹¤', 'ìˆë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆ˜', 'ê²ƒ', 'ë“±',\n",
    "    'ê·¸', 'ì €', 'ì´ëŸ°', 'ì €ëŸ°', 'ê·¸ëŸ°', 'ì–´ë–¤', 'ë¬´ìŠ¨', 'ë­', 'ë­”ê°€', 'ë­˜',\n",
    "    'ë‚˜', 'ë„ˆ', 'ìš°ë¦¬', 'ì €í¬', 'ê·¸ë…€', 'ê·¸', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ê·¸ê²ƒ',\n",
    "    'ì•„', 'ì–´', 'ì˜¤', 'ì˜ˆ', 'ë„¤', 'ì‘', 'ìŒ', 'ì—„', 'ì•„ë‹ˆ', 'ì•ˆ',\n",
    "    'ì¢€', 'ì˜', 'ë”', 'ëœ', 'ë§ì´', 'ì¡°ê¸ˆ', 'ì•„ì£¼', 'ë§¤ìš°', 'ì •ë§', 'ì§„ì§œ',\n",
    "    'ê·¸ë˜ì„œ', 'ê·¸ëŸ¬ë©´', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ë¦¬ê³ ', 'ë˜', 'ë˜í•œ',\n",
    "    '#Person1#', '#Person2#', '#Person3#', '#Person4#', '#Person5#',\n",
    "]\n",
    "\n",
    "class TFIDFKeywordExtractor:\n",
    "    \"\"\"TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œê¸°\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus=None, max_features=1000, ngram_range=(1, 2)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            corpus: TF-IDF í•™ìŠµìš© ì½”í¼ìŠ¤ (ëŒ€í™” ë¦¬ìŠ¤íŠ¸)\n",
    "            max_features: ìµœëŒ€ feature ìˆ˜\n",
    "            ngram_range: n-gram ë²”ìœ„\n",
    "        \"\"\"\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            ngram_range=ngram_range,\n",
    "            token_pattern=r'[ê°€-í£]+',  # í•œê¸€ë§Œ ì¶”ì¶œ\n",
    "            stop_words=KOREAN_STOPWORDS,\n",
    "        )\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        if corpus is not None:\n",
    "            self.fit(corpus)\n",
    "    \n",
    "    def fit(self, corpus):\n",
    "        \"\"\"ì½”í¼ìŠ¤ë¡œ TF-IDF í•™ìŠµ\"\"\"\n",
    "        # ë§ˆìŠ¤í‚¹ í† í° ì œê±° í›„ í•™ìŠµ\n",
    "        cleaned_corpus = [re.sub(MASK_PAT, ' ', doc) for doc in corpus]\n",
    "        self.vectorizer.fit(cleaned_corpus)\n",
    "        self.feature_names = self.vectorizer.get_feature_names_out()\n",
    "        self.is_fitted = True\n",
    "        print(f\"âœ… TF-IDF í•™ìŠµ ì™„ë£Œ: {len(self.feature_names)}ê°œ feature\")\n",
    "    \n",
    "    def extract_keywords(self, text, top_k=5):\n",
    "        \"\"\"\n",
    "        ë‹¨ì¼ ë¬¸ì„œì—ì„œ TF-IDF ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "        \n",
    "        Args:\n",
    "            text: ëŒ€í™” í…ìŠ¤íŠ¸\n",
    "            top_k: ì¶”ì¶œí•  í‚¤ì›Œë“œ ìˆ˜\n",
    "        \n",
    "        Returns:\n",
    "            List of (keyword, score) tuples\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            return []\n",
    "        \n",
    "        # ë§ˆìŠ¤í‚¹ í† í° ì œê±°\n",
    "        cleaned_text = re.sub(MASK_PAT, ' ', text)\n",
    "        \n",
    "        try:\n",
    "            tfidf_vector = self.vectorizer.transform([cleaned_text])\n",
    "            scores = tfidf_vector.toarray()[0]\n",
    "            \n",
    "            # ìƒìœ„ kê°œ ì¸ë±ìŠ¤\n",
    "            top_indices = scores.argsort()[-top_k:][::-1]\n",
    "            \n",
    "            keywords = []\n",
    "            for idx in top_indices:\n",
    "                if scores[idx] > 0:\n",
    "                    keywords.append((self.feature_names[idx], scores[idx]))\n",
    "            \n",
    "            return keywords\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def get_keyword_string(self, text, top_k=5, separator=\" / \"):\n",
    "        \"\"\"í‚¤ì›Œë“œë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜\"\"\"\n",
    "        keywords = self.extract_keywords(text, top_k)\n",
    "        if not keywords:\n",
    "            return \"\"\n",
    "        return separator.join([kw for kw, score in keywords])\n",
    "\n",
    "# TF-IDF ì¶”ì¶œê¸° ì´ˆê¸°í™” (í•™ìŠµ ë°ì´í„° ê¸°ë°˜)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™”:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tfidf_extractor = TFIDFKeywordExtractor(\n",
    "    corpus=train_df_for_tokens['dialogue'].tolist(),\n",
    "    max_features=2000,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "# ì˜ˆì‹œ í…ŒìŠ¤íŠ¸\n",
    "sample_text = train_df_for_tokens['dialogue'].iloc[0]\n",
    "sample_keywords = tfidf_extractor.get_keyword_string(sample_text, top_k=5)\n",
    "print(f\"\\nğŸ“ TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì˜ˆì‹œ:\")\n",
    "print(f\"  í‚¤ì›Œë“œ: {sample_keywords}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5-1. TF-IDF í‚¤ì›Œë“œ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ë¹Œë” (6-1, 6-2 í†µí•©)\n",
    "# ============================================================================\n",
    "\n",
    "def build_tfidf_keyword_prompt(dialogue_text, tfidf_extractor, top_k=5, prompt_style=\"keyword\",\n",
    "                                use_speaker_count=True, use_pii_instruction=True):\n",
    "    \"\"\"\n",
    "    TF-IDF í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± (6-1, 6-2 í†µí•©)\n",
    "    \n",
    "    Args:\n",
    "        dialogue_text: ì›ë³¸ ëŒ€í™” í…ìŠ¤íŠ¸\n",
    "        tfidf_extractor: TFIDFKeywordExtractor ì¸ìŠ¤í„´ìŠ¤\n",
    "        top_k: ì¶”ì¶œí•  í‚¤ì›Œë“œ ìˆ˜\n",
    "        prompt_style: \"keyword\" (í‚¤ì›Œë“œë§Œ), \"keyword_aware\" (í‚¤ì›Œë“œ+ë°œí™”ììˆ˜)\n",
    "        use_speaker_count: 6-1. í™”ì ìˆ˜ ëª…ì‹œ ì—¬ë¶€\n",
    "        use_pii_instruction: 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ ì—¬ë¶€\n",
    "    \n",
    "    Returns:\n",
    "        í”„ë¡¬í”„íŠ¸ê°€ ì¶”ê°€ëœ ëŒ€í™” í…ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    # ëŒ€í™” êµ¬ì¡° ë¶„ì„\n",
    "    analysis = analyze_dialogue_structure(dialogue_text)\n",
    "    num_speakers = analysis['num_speakers']\n",
    "    pii_masks = analysis['pii_masks']\n",
    "    \n",
    "    # í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "    keywords = tfidf_extractor.get_keyword_string(dialogue_text, top_k=top_k)\n",
    "    \n",
    "    prompt_parts = []\n",
    "    \n",
    "    # 6-1. í™”ì ìˆ˜ ëª…ì‹œ (keyword_aware ìŠ¤íƒ€ì¼ì—ì„œë§Œ)\n",
    "    if prompt_style == \"keyword_aware\" and use_speaker_count:\n",
    "        prompt_parts.append(build_speaker_aware_prompt(num_speakers))\n",
    "    elif prompt_style == \"keyword\":\n",
    "        prompt_parts.append(\"ë‹¤ìŒ ëŒ€í™”ë¥¼ í•œ ë¬¸ë‹¨ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì–´ì²´ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \")\n",
    "    \n",
    "    # TF-IDF í‚¤ì›Œë“œ ì¶”ê°€\n",
    "    if keywords:\n",
    "        prompt_parts.append(f\"ì£¼ìš” í‚¤ì›Œë“œ: {keywords}. \")\n",
    "    \n",
    "    # 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ\n",
    "    if use_pii_instruction and pii_masks:\n",
    "        prompt_parts.append(build_pii_mask_instruction(pii_masks, show_examples=REGEX_PATTERN_CONFIG['show_pii_examples']))\n",
    "    \n",
    "    prompt_parts.append(\"\\n\\n\")\n",
    "    \n",
    "    prompt = \"\".join(prompt_parts)\n",
    "    return prompt + dialogue_text\n",
    "\n",
    "# ============================================================================\n",
    "# 5-2. ëŒ€í™” ê¸¸ì´/ì£¼ì œë³„ ë™ì  ì¶”ë¡  ì„¤ì •\n",
    "# ============================================================================\n",
    "\n",
    "# ê¸¸ì´ ê¸°ë°˜ ì¶”ë¡  ì„¤ì •\n",
    "LENGTH_BASED_INFERENCE_CONFIG = {\n",
    "    # ì§§ì€ ëŒ€í™” (500ì ì´í•˜)\n",
    "    \"short\": {\n",
    "        \"char_threshold\": 500,\n",
    "        \"max_new_tokens\": 60,\n",
    "        \"num_beams\": 4,\n",
    "        \"length_penalty\": 1.0,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "    },\n",
    "    # ì¤‘ê°„ ëŒ€í™” (500~1500ì)\n",
    "    \"medium\": {\n",
    "        \"char_threshold\": 1500,\n",
    "        \"max_new_tokens\": 80,\n",
    "        \"num_beams\": 5,\n",
    "        \"length_penalty\": 1.0,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "    },\n",
    "    # ê¸´ ëŒ€í™” (1500ì ì´ˆê³¼)\n",
    "    \"long\": {\n",
    "        \"char_threshold\": float('inf'),\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"num_beams\": 6,\n",
    "        \"length_penalty\": 1.2,  # ê¸´ ëŒ€í™”ëŠ” ìš”ì•½ë„ ê¸¸ê²Œ\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "    },\n",
    "}\n",
    "\n",
    "# ì£¼ì œ í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ë¡  ì„¤ì • (íŠ¹ì • ì£¼ì œëŠ” ë” ê¸´ ìš”ì•½ í•„ìš”)\n",
    "TOPIC_KEYWORDS = {\n",
    "    \"medical\": [\"ë³‘ì›\", \"ì˜ì‚¬\", \"ì¹˜ë£Œ\", \"ì§„ë£Œ\", \"ìˆ˜ìˆ \", \"ì•½\", \"ì¦ìƒ\", \"ì§„ë‹¨\", \"ê²€ì‚¬\"],\n",
    "    \"insurance\": [\"ë³´í—˜\", \"ê°€ì…\", \"ë³´ì¥\", \"ì²­êµ¬\", \"ê³„ì•½\", \"ë‚©ì…\", \"ë³´ìƒ\", \"ì•½ê´€\"],\n",
    "    \"finance\": [\"ëŒ€ì¶œ\", \"ì´ì\", \"ê¸ˆë¦¬\", \"ê³„ì¢Œ\", \"ì†¡ê¸ˆ\", \"ì…ê¸ˆ\", \"ì¶œê¸ˆ\", \"íˆ¬ì\", \"ì£¼ì‹\"],\n",
    "    \"travel\": [\"ì—¬í–‰\", \"ë¹„í–‰ê¸°\", \"í˜¸í…”\", \"ì˜ˆì•½\", \"ê´€ê´‘\", \"ìˆ™ì†Œ\", \"í•­ê³µ\"],\n",
    "}\n",
    "\n",
    "TOPIC_INFERENCE_CONFIG = {\n",
    "    \"medical\": {\"max_new_tokens\": 100, \"length_penalty\": 1.3},\n",
    "    \"insurance\": {\"max_new_tokens\": 100, \"length_penalty\": 1.3},\n",
    "    \"finance\": {\"max_new_tokens\": 95, \"length_penalty\": 1.2},\n",
    "    \"travel\": {\"max_new_tokens\": 85, \"length_penalty\": 1.1},\n",
    "    \"default\": {\"max_new_tokens\": 80, \"length_penalty\": 1.0},\n",
    "}\n",
    "\n",
    "def detect_topic(dialogue_text):\n",
    "    \"\"\"ëŒ€í™” ì£¼ì œ ê°ì§€\"\"\"\n",
    "    text_lower = dialogue_text.lower()\n",
    "    \n",
    "    topic_scores = {}\n",
    "    for topic, keywords in TOPIC_KEYWORDS.items():\n",
    "        score = sum(1 for kw in keywords if kw in text_lower)\n",
    "        if score > 0:\n",
    "            topic_scores[topic] = score\n",
    "    \n",
    "    if topic_scores:\n",
    "        return max(topic_scores, key=topic_scores.get)\n",
    "    return \"default\"\n",
    "\n",
    "def get_length_category(dialogue_text):\n",
    "    \"\"\"ëŒ€í™” ê¸¸ì´ ì¹´í…Œê³ ë¦¬ ë°˜í™˜\"\"\"\n",
    "    length = len(dialogue_text)\n",
    "    \n",
    "    if length <= LENGTH_BASED_INFERENCE_CONFIG[\"short\"][\"char_threshold\"]:\n",
    "        return \"short\"\n",
    "    elif length <= LENGTH_BASED_INFERENCE_CONFIG[\"medium\"][\"char_threshold\"]:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"long\"\n",
    "\n",
    "def get_dynamic_inference_config(dialogue_text, use_topic=True, use_length=True):\n",
    "    \"\"\"\n",
    "    ëŒ€í™” íŠ¹ì„±ì— ë”°ë¥¸ ë™ì  ì¶”ë¡  ì„¤ì • ë°˜í™˜\n",
    "    \n",
    "    Args:\n",
    "        dialogue_text: ëŒ€í™” í…ìŠ¤íŠ¸\n",
    "        use_topic: ì£¼ì œ ê¸°ë°˜ ì„¤ì • ì‚¬ìš© ì—¬ë¶€\n",
    "        use_length: ê¸¸ì´ ê¸°ë°˜ ì„¤ì • ì‚¬ìš© ì—¬ë¶€\n",
    "    \n",
    "    Returns:\n",
    "        dict: ì¶”ë¡  ì„¤ì • (max_new_tokens, num_beams, length_penalty ë“±)\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"max_new_tokens\": 80,\n",
    "        \"num_beams\": 5,\n",
    "        \"length_penalty\": 1.0,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "    }\n",
    "    \n",
    "    # 1. ê¸¸ì´ ê¸°ë°˜ ì„¤ì •\n",
    "    if use_length:\n",
    "        length_cat = get_length_category(dialogue_text)\n",
    "        length_config = LENGTH_BASED_INFERENCE_CONFIG[length_cat]\n",
    "        config.update({\n",
    "            \"max_new_tokens\": length_config[\"max_new_tokens\"],\n",
    "            \"num_beams\": length_config[\"num_beams\"],\n",
    "            \"length_penalty\": length_config[\"length_penalty\"],\n",
    "            \"no_repeat_ngram_size\": length_config[\"no_repeat_ngram_size\"],\n",
    "        })\n",
    "    \n",
    "    # 2. ì£¼ì œ ê¸°ë°˜ ì„¤ì • (ê¸¸ì´ë³´ë‹¤ ìš°ì„ )\n",
    "    if use_topic:\n",
    "        topic = detect_topic(dialogue_text)\n",
    "        if topic != \"default\":\n",
    "            topic_config = TOPIC_INFERENCE_CONFIG[topic]\n",
    "            config[\"max_new_tokens\"] = max(config[\"max_new_tokens\"], topic_config[\"max_new_tokens\"])\n",
    "            config[\"length_penalty\"] = max(config[\"length_penalty\"], topic_config[\"length_penalty\"])\n",
    "    \n",
    "    return config\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“ ê¸¸ì´/ì£¼ì œ ê¸°ë°˜ ë™ì  ì¶”ë¡  ì„¤ì •:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ê¸¸ì´ ì¹´í…Œê³ ë¦¬:\")\n",
    "for cat, cfg in LENGTH_BASED_INFERENCE_CONFIG.items():\n",
    "    print(f\"  - {cat}: ~{cfg['char_threshold']}ì, max_tokens={cfg['max_new_tokens']}, beams={cfg['num_beams']}\")\n",
    "\n",
    "print(\"\\nì£¼ì œ ì¹´í…Œê³ ë¦¬:\")\n",
    "for topic in TOPIC_KEYWORDS.keys():\n",
    "    cfg = TOPIC_INFERENCE_CONFIG[topic]\n",
    "    print(f\"  - {topic}: max_tokens={cfg['max_new_tokens']}, length_penalty={cfg['length_penalty']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3-2. í”„ë¡¬í”„íŠ¸ ë¹Œë” í•¨ìˆ˜ - ë°œí™”ì/turn êµ¬ì¡° í™œìš© (6-1, 6-2 í†µí•©)\n",
    "# ============================================================================\n",
    "\n",
    "def build_summary_prompt(dialogue_text, prompt_style=\"default\", tfidf_extractor=None, top_k=5,\n",
    "                         use_speaker_count=True, use_pii_instruction=True):\n",
    "    \"\"\"\n",
    "    ëŒ€í™”ì— ìš”ì•½ í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì—ê²Œ íƒœìŠ¤í¬ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì•Œë¦¼\n",
    "    6-1. í™”ì ìˆ˜ ëª…ì‹œ: \"í•œ ì‚¬ëŒ ì´ì•¼ê¸°ë§Œ ìš”ì•½í•˜ëŠ”\" ë²„ê·¸ ë°©ì§€\n",
    "    6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: ROUGE ì ìˆ˜ í–¥ìƒ\n",
    "    \n",
    "    Args:\n",
    "        dialogue_text: ì›ë³¸ ëŒ€í™” í…ìŠ¤íŠ¸\n",
    "        prompt_style: í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼\n",
    "            - \"default\": ê¸°ë³¸ í”„ë¡¬í”„íŠ¸\n",
    "            - \"speaker_aware\": ë°œí™”ì ìˆ˜ í¬í•¨\n",
    "            - \"minimal\": ìµœì†Œ í”„ë¡¬í”„íŠ¸\n",
    "            - \"keyword\": TF-IDF í‚¤ì›Œë“œ í¬í•¨ (5-1)\n",
    "            - \"keyword_aware\": í‚¤ì›Œë“œ + ë°œí™”ì ìˆ˜ (5-1)\n",
    "            - \"balanced\": 6-1, 6-2 ì „ëµ ì ìš© (NEW)\n",
    "            - \"none\": í”„ë¡¬í”„íŠ¸ ì—†ìŒ\n",
    "        tfidf_extractor: TFIDFKeywordExtractor ì¸ìŠ¤í„´ìŠ¤ (keyword ìŠ¤íƒ€ì¼ìš©)\n",
    "        top_k: í‚¤ì›Œë“œ ì¶”ì¶œ ê°œìˆ˜\n",
    "        use_speaker_count: 6-1. í™”ì ìˆ˜ ëª…ì‹œ ì—¬ë¶€\n",
    "        use_pii_instruction: 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ ì—¬ë¶€\n",
    "    \n",
    "    Returns:\n",
    "        í”„ë¡¬í”„íŠ¸ê°€ ì¶”ê°€ëœ ëŒ€í™” í…ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    if prompt_style == \"none\":\n",
    "        return dialogue_text\n",
    "    \n",
    "    # ëŒ€í™” êµ¬ì¡° ë¶„ì„ (6-1, 6-2ìš©)\n",
    "    analysis = analyze_dialogue_structure(dialogue_text)\n",
    "    num_speakers = analysis['num_speakers']\n",
    "    pii_masks = analysis['pii_masks']\n",
    "    \n",
    "    # 5-1. TF-IDF í‚¤ì›Œë“œ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ (6-1, 6-2 í†µí•©)\n",
    "    if prompt_style in [\"keyword\", \"keyword_aware\"] and tfidf_extractor is not None:\n",
    "        return build_tfidf_keyword_prompt(\n",
    "            dialogue_text, tfidf_extractor, top_k, prompt_style,\n",
    "            use_speaker_count=use_speaker_count,\n",
    "            use_pii_instruction=use_pii_instruction\n",
    "        )\n",
    "    \n",
    "    prompt_parts = []\n",
    "    \n",
    "    # 6-1. í™”ì ìˆ˜ ëª…ì‹œ í”„ë¡¬í”„íŠ¸\n",
    "    if prompt_style == \"balanced\":\n",
    "        # ìƒˆë¡œìš´ \"balanced\" ìŠ¤íƒ€ì¼: 6-1, 6-2 ì „ëµ ëª¨ë‘ ì ìš©\n",
    "        prompt_parts.append(build_speaker_aware_prompt(num_speakers))\n",
    "    elif prompt_style == \"speaker_aware\":\n",
    "        prompt_parts.append(build_speaker_aware_prompt(num_speakers))\n",
    "    elif prompt_style == \"minimal\":\n",
    "        prompt_parts.append(\"ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ì„¸ìš”. \")\n",
    "    else:  # default\n",
    "        if use_speaker_count:\n",
    "            prompt_parts.append(build_speaker_aware_prompt(num_speakers))\n",
    "        else:\n",
    "            prompt_parts.append(\"ë‹¤ìŒì€ ì—¬ëŸ¬ ì‚¬ëŒì´ ë‚˜ëˆˆ ì¼ìƒ ëŒ€í™”ì…ë‹ˆë‹¤. ì´ ëŒ€í™”ë¥¼ í•œ ë¬¸ë‹¨ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì–´ì²´ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \")\n",
    "    \n",
    "    # 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ (balanced, default, speaker_aware ìŠ¤íƒ€ì¼ì—ì„œ)\n",
    "    if use_pii_instruction and pii_masks and prompt_style in [\"balanced\", \"default\", \"speaker_aware\"]:\n",
    "        prompt_parts.append(build_pii_mask_instruction(pii_masks, show_examples=REGEX_PATTERN_CONFIG['show_pii_examples']))\n",
    "    \n",
    "    prompt_parts.append(\"\\n\\n\")\n",
    "    \n",
    "    prompt = \"\".join(prompt_parts)\n",
    "    return prompt + dialogue_text\n",
    "\n",
    "def add_turn_separators(dialogue_text, separator=\"[SEP]\"):\n",
    "    \"\"\"ë°œí™” turn ì‚¬ì´ì— êµ¬ë¶„ì ì¶”ê°€\"\"\"\n",
    "    lines = dialogue_text.strip().split('\\n')\n",
    "    result_lines = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if i > 0 and re.match(PERSON_PAT, line.strip()):\n",
    "            result_lines.append(f\"{separator} {line}\")\n",
    "        else:\n",
    "            result_lines.append(line)\n",
    "    return '\\n'.join(result_lines)\n",
    "\n",
    "# ============================================================================\n",
    "# ì„¤ì • ì—…ë°ì´íŠ¸ (6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš© ì „ëµ ë°˜ì˜)\n",
    "# ============================================================================\n",
    "\n",
    "config_data = {\n",
    "    \"general\": {\n",
    "        \"data_path\": \"./data/\",\n",
    "        \"model_name\": \"digit82/kobart-summarization\",\n",
    "        \"output_dir\": \"./prediction_kobart_v1\"\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "        \"encoder_max_len\": 1024,\n",
    "        \"decoder_max_len\": 100,\n",
    "        \"bos_token\": f\"{tokenizer.bos_token}\",\n",
    "        \"eos_token\": f\"{tokenizer.eos_token}\",\n",
    "        \"special_tokens\": DYNAMIC_SPECIAL_TOKENS\n",
    "    },\n",
    "    # 3-2. í”„ë¡¬í”„íŠ¸ ì„¤ì • (6-1, 6-2 í†µí•©)\n",
    "    \"prompt\": {\n",
    "        \"use_prompt\": True,\n",
    "        \"prompt_style\": \"balanced\",  # NEW: 6-1, 6-2 ì „ëµ ì ìš©\n",
    "        \"use_turn_separator\": False,\n",
    "        \"turn_separator\": \"[SEP]\",\n",
    "        \"tfidf_top_k\": 5,  # 5-1. TF-IDF í‚¤ì›Œë“œ ê°œìˆ˜\n",
    "    },\n",
    "    # 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš© ì„¤ì • (NEW)\n",
    "    \"regex_pattern\": {\n",
    "        \"use_speaker_count\": True,       # 6-1. í™”ì ìˆ˜ ëª…ì‹œ\n",
    "        \"use_pii_instruction\": True,     # 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ\n",
    "        \"show_pii_examples\": True,       # ë§ˆìŠ¤í‚¹ ì˜ˆì‹œ í‘œì‹œ\n",
    "    },\n",
    "    # 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •\n",
    "    \"preprocess\": {\n",
    "        \"normalize_slang\": True,\n",
    "    },\n",
    "    # 5-2. ë™ì  ì¶”ë¡  ì„¤ì •\n",
    "    \"dynamic_inference\": {\n",
    "        \"use_length_based\": True,   # ê¸¸ì´ ê¸°ë°˜ ë™ì  ì„¤ì • ì‚¬ìš©\n",
    "        \"use_topic_based\": True,    # ì£¼ì œ ê¸°ë°˜ ë™ì  ì„¤ì • ì‚¬ìš©\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"num_train_epochs\": 20,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"per_device_eval_batch_size\": 32,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lr_scheduler_type\": 'cosine',\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"eval_strategy\": 'epoch',\n",
    "        \"save_strategy\": 'epoch',\n",
    "        \"save_total_limit\": 5,\n",
    "        \"fp16\": True,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"seed\": 42,\n",
    "        \"logging_dir\": \"./logs\",\n",
    "        \"logging_strategy\": \"epoch\",\n",
    "        \"predict_with_generate\": True,\n",
    "        \"generation_max_length\": 100,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_threshold\": 0.001,\n",
    "        \"report_to\": \"wandb\"\n",
    "    },\n",
    "    \"wandb\": {\n",
    "        \"entity\": \"fc_bootcamp\",\n",
    "        \"project\": \"kobart_v1\",\n",
    "        \"name\": \"baseline_v0\"\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"ckt_path\": \"model ckt path\",\n",
    "        \"result_path\": \"./prediction/\",\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "        \"early_stopping\": True,\n",
    "        \"generate_max_length\": 100,\n",
    "        \"num_beams\": 4,\n",
    "        \"batch_size\": 32,\n",
    "        \"remove_tokens\": ['<usr>', f\"{tokenizer.bos_token}\", f\"{tokenizer.eos_token}\", f\"{tokenizer.pad_token}\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# ì„¤ì • ìš”ì•½ ì¶œë ¥\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“‹ ì „ì²´ ì„¤ì • ìš”ì•½:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nğŸ”– 3-1. Special Tokens:\")\n",
    "print(f\"  - ë™ì  ìˆ˜ì§‘: {len(DYNAMIC_SPECIAL_TOKENS)}ê°œ\")\n",
    "\n",
    "print(\"\\nğŸ“ 3-2. í”„ë¡¬í”„íŠ¸ ì„¤ì •:\")\n",
    "print(f\"  - í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {config_data['prompt']['use_prompt']}\")\n",
    "print(f\"  - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: {config_data['prompt']['prompt_style']}\")\n",
    "print(f\"  - TF-IDF í‚¤ì›Œë“œ ìˆ˜: {config_data['prompt']['tfidf_top_k']}ê°œ\")\n",
    "\n",
    "print(\"\\nğŸ” 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš© ì „ëµ:\")\n",
    "print(f\"  - 6-1. í™”ì ìˆ˜ ëª…ì‹œ: {config_data['regex_pattern']['use_speaker_count']}\")\n",
    "print(f\"  - 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: {config_data['regex_pattern']['use_pii_instruction']}\")\n",
    "\n",
    "print(\"\\nğŸ—£ï¸ 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬:\")\n",
    "print(f\"  - ìŠ¬ë­ ì •ê·œí™”: {config_data['preprocess']['normalize_slang']}\")\n",
    "\n",
    "print(\"\\nğŸ“ 5-2. ë™ì  ì¶”ë¡ :\")\n",
    "print(f\"  - ê¸¸ì´ ê¸°ë°˜: {config_data['dynamic_inference']['use_length_based']}\")\n",
    "print(f\"  - ì£¼ì œ ê¸°ë°˜: {config_data['dynamic_inference']['use_topic_based']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ (6-1, 6-2 í†µí•©)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"ğŸ“ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ (6-1, 6-2 ì ìš©):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "sample_dialogue = train_df_for_tokens['dialogue'].iloc[0]\n",
    "print(f\"[1. ì›ë³¸ ëŒ€í™” (ì• 150ì)]:\\n{sample_dialogue[:150]}...\")\n",
    "\n",
    "# ëŒ€í™” êµ¬ì¡° ë¶„ì„\n",
    "sample_analysis = analyze_dialogue_structure(sample_dialogue)\n",
    "print(f\"\\n[2. ëŒ€í™” êµ¬ì¡° ë¶„ì„]:\")\n",
    "print(f\"  - í™”ì ìˆ˜: {sample_analysis['num_speakers']}ëª…\")\n",
    "print(f\"  - PII ë§ˆìŠ¤í‚¹ í† í°: {sample_analysis['pii_mask_examples']}\")\n",
    "\n",
    "sample_preprocessed = preprocess_dialogue(sample_dialogue, normalize_slang_flag=True)\n",
    "print(f\"\\n[3. ìŠ¬ë­ ì •ê·œí™” í›„]:\\n{sample_preprocessed[:150]}...\")\n",
    "\n",
    "sample_keywords = tfidf_extractor.get_keyword_string(sample_preprocessed, top_k=5)\n",
    "print(f\"\\n[4. TF-IDF í‚¤ì›Œë“œ]: {sample_keywords}\")\n",
    "\n",
    "# 6-1, 6-2 í†µí•© í”„ë¡¬í”„íŠ¸ (balanced ìŠ¤íƒ€ì¼)\n",
    "sample_prompted = build_summary_prompt(\n",
    "    sample_preprocessed, \n",
    "    prompt_style=\"balanced\",\n",
    "    use_speaker_count=True,\n",
    "    use_pii_instruction=True\n",
    ")\n",
    "print(f\"\\n[5. 'balanced' í”„ë¡¬í”„íŠ¸ ì ìš© (6-1, 6-2)]:\\n{sample_prompted[:400]}...\")\n",
    "\n",
    "# keyword_aware ìŠ¤íƒ€ì¼ ì˜ˆì‹œ\n",
    "sample_keyword_aware = build_summary_prompt(\n",
    "    sample_preprocessed,\n",
    "    prompt_style=\"keyword_aware\",\n",
    "    tfidf_extractor=tfidf_extractor,\n",
    "    top_k=5,\n",
    "    use_speaker_count=True,\n",
    "    use_pii_instruction=True\n",
    ")\n",
    "print(f\"\\n[6. 'keyword_aware' í”„ë¡¬í”„íŠ¸ ì ìš© (TF-IDF + 6-1, 6-2)]:\\n{sample_keyword_aware[:400]}...\")\n",
    "\n",
    "# ë™ì  ì¶”ë¡  ì„¤ì • ì˜ˆì‹œ\n",
    "dynamic_config = get_dynamic_inference_config(sample_dialogue, use_topic=True, use_length=True)\n",
    "print(f\"\\n[7. ë™ì  ì¶”ë¡  ì„¤ì •]:\")\n",
    "print(f\"  - ê¸¸ì´ ì¹´í…Œê³ ë¦¬: {get_length_category(sample_dialogue)}\")\n",
    "print(f\"  - ê°ì§€ëœ ì£¼ì œ: {detect_topic(sample_dialogue)}\")\n",
    "print(f\"  - max_new_tokens: {dynamic_config['max_new_tokens']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜: ë””ì½”ë”© & í›„ì²˜ë¦¬\n",
    "# ============================================================================\n",
    "\n",
    "def decode_with_speaker_tags(tokenizer, output_ids, remove_tokens=None):\n",
    "    \"\"\"\n",
    "    í™”ì íƒœê·¸(#Person1# ë“±)ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ë””ì½”ë”©\n",
    "    skip_special_tokens=Falseë¡œ ë””ì½”ë”© í›„, ë¶ˆí•„ìš”í•œ í† í°ë§Œ ì œê±°\n",
    "    \"\"\"\n",
    "    if remove_tokens is None:\n",
    "        remove_tokens = ['<usr>', tokenizer.bos_token, tokenizer.eos_token, tokenizer.pad_token]\n",
    "    \n",
    "    # skip_special_tokens=Falseë¡œ ë””ì½”ë”© (í™”ì íƒœê·¸ ë³´ì¡´)\n",
    "    text = tokenizer.decode(output_ids, skip_special_tokens=False)\n",
    "    \n",
    "    # ë¶ˆí•„ìš”í•œ í† í°ë§Œ ì œê±°\n",
    "    for token in remove_tokens:\n",
    "        if token:\n",
    "            text = text.replace(token, '')\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def postprocess_summary(summary):\n",
    "    \"\"\"\n",
    "    ìš”ì•½ë¬¸ í›„ì²˜ë¦¬:\n",
    "    1. ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°\n",
    "    2. í™”ì íƒœê·¸ ì•ë’¤ ê³µë°± ì •ë¦¬\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # ì—°ì† ê³µë°± ì œê±°\n",
    "    summary = ' '.join(summary.split())\n",
    "    \n",
    "    # í™”ì íƒœê·¸ ì•ë’¤ ê³µë°± ì •ë¦¬ (#Person1# ì€ â†’ #Person1#ì€)\n",
    "    summary = re.sub(r'(#Person\\d+#)\\s+', r'\\1 ', summary)\n",
    "    \n",
    "    return summary.strip()\n",
    "\n",
    "print(\"\\nâœ… decode_with_speaker_tags, postprocess_summary í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cm7ob25lHBkR"
   },
   "source": [
    "- ì°¸ê³ âœ…    \n",
    ": wandb ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„  entity, project, nameë¥¼ ì§€ì •í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤. wandb í™ˆí˜ì´ì§€ì— ê°€ì…í•œ í›„ ì–»ì€ ì •ë³´ë¥¼ ì…ë ¥í•˜ì—¬ ì‘ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "REJybO5UCabF"
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ì˜ êµ¬ì„± ì •ë³´ë¥¼ YAML íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "config_path = \"./config.yaml\"\n",
    "with open(config_path, \"w\") as file:\n",
    "    yaml.dump(config_data, file, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObEASD6Wj6pl"
   },
   "source": [
    "### 3) Configuration ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JUBm_6RqlYpV",
    "outputId": "4b1c8c44-c6a9-40f1-adbd-72d48f0c983b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dynamic_inference': {'use_length_based': True, 'use_topic_based': True},\n",
      " 'general': {'data_path': './data/',\n",
      "             'model_name': 'digit82/kobart-summarization',\n",
      "             'output_dir': './prediction_kobart_v1'},\n",
      " 'inference': {'batch_size': 32,\n",
      "               'ckt_path': 'model ckt path',\n",
      "               'early_stopping': True,\n",
      "               'generate_max_length': 100,\n",
      "               'no_repeat_ngram_size': 2,\n",
      "               'num_beams': 4,\n",
      "               'remove_tokens': ['<usr>', '<s>', '</s>', '<pad>'],\n",
      "               'result_path': './prediction/'},\n",
      " 'preprocess': {'normalize_slang': True},\n",
      " 'prompt': {'prompt_style': 'balanced',\n",
      "            'tfidf_top_k': 5,\n",
      "            'turn_separator': '[SEP]',\n",
      "            'use_prompt': True,\n",
      "            'use_turn_separator': False},\n",
      " 'regex_pattern': {'show_pii_examples': True,\n",
      "                   'use_pii_instruction': True,\n",
      "                   'use_speaker_count': True},\n",
      " 'tokenizer': {'bos_token': '<s>',\n",
      "               'decoder_max_len': 100,\n",
      "               'encoder_max_len': 1024,\n",
      "               'eos_token': '</s>',\n",
      "               'special_tokens': ['#Address#',\n",
      "                                  '#Alex#',\n",
      "                                  '#Bob#',\n",
      "                                  '#CarNumber#',\n",
      "                                  '#CardNumber#',\n",
      "                                  '#DateOfBirth#',\n",
      "                                  '#Email#',\n",
      "                                  '#Kristin#',\n",
      "                                  '#Liliana#',\n",
      "                                  '#Name#',\n",
      "                                  '#PassportNumber#',\n",
      "                                  '#Person1#',\n",
      "                                  '#Person2#',\n",
      "                                  '#Person3#',\n",
      "                                  '#Person4#',\n",
      "                                  '#Person5#',\n",
      "                                  '#Person6#',\n",
      "                                  '#Person7#',\n",
      "                                  '#PersonName#',\n",
      "                                  '#PhoneNumber#',\n",
      "                                  '#Price#',\n",
      "                                  '#SSN#']},\n",
      " 'training': {'do_eval': True,\n",
      "              'do_train': True,\n",
      "              'early_stopping_patience': 3,\n",
      "              'early_stopping_threshold': 0.001,\n",
      "              'eval_strategy': 'epoch',\n",
      "              'fp16': True,\n",
      "              'generation_max_length': 100,\n",
      "              'gradient_accumulation_steps': 1,\n",
      "              'learning_rate': 2e-05,\n",
      "              'load_best_model_at_end': True,\n",
      "              'logging_dir': './logs',\n",
      "              'logging_strategy': 'epoch',\n",
      "              'lr_scheduler_type': 'cosine',\n",
      "              'num_train_epochs': 20,\n",
      "              'optim': 'adamw_torch',\n",
      "              'overwrite_output_dir': True,\n",
      "              'per_device_eval_batch_size': 32,\n",
      "              'per_device_train_batch_size': 16,\n",
      "              'predict_with_generate': True,\n",
      "              'report_to': 'wandb',\n",
      "              'save_strategy': 'epoch',\n",
      "              'save_total_limit': 5,\n",
      "              'seed': 42,\n",
      "              'warmup_ratio': 0.1,\n",
      "              'weight_decay': 0.01},\n",
      " 'wandb': {'entity': 'fc_bootcamp',\n",
      "           'name': 'baseline_v0',\n",
      "           'project': 'kobart_v1'}}\n"
     ]
    }
   ],
   "source": [
    "# ì €ì¥ëœ config íŒŒì¼ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "config_path = \"./config.yaml\"\n",
    "\n",
    "with open(config_path, \"r\") as file:\n",
    "    loaded_config = yaml.safe_load(file)\n",
    "\n",
    "# ë¶ˆëŸ¬ì˜¨ config íŒŒì¼ì˜ ì „ì²´ ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "pprint(loaded_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xRSbKEVslhwO",
    "outputId": "40ba5c67-574e-4f86-cbac-13e9f01c588a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_path': './data/',\n",
       " 'model_name': 'digit82/kobart-summarization',\n",
       " 'output_dir': './prediction_kobart_v1'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì‹¤í—˜ì— ì“°ì¼ ë°ì´í„°ì˜ ê²½ë¡œ, ì‚¬ìš©ë  ëª¨ë¸, ëª¨ë¸ì˜ ìµœì¢… ì¶œë ¥ ê²°ê³¼ë¥¼ ì €ì¥í•  ê²½ë¡œì— ëŒ€í•´ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "loaded_config['general']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ê³³ì— ì‚¬ìš©ìê°€ ì €ì¥í•œ ë°ì´í„° dir ì„¤ì •í•˜ê¸°\n",
    "# loaded_config['general']['data_path'] = \"data_path\"\n",
    "\n",
    "# Ensure compatibility with Transformers: they expect the key to be 'evaluation_strategy'.\n",
    "# Some configs may use 'eval_strategy' (short form). Normalize to 'evaluation_strategy'.\n",
    "if 'eval_strategy' in loaded_config['training'] and 'evaluation_strategy' not in loaded_config['training']:\n",
    "    loaded_config['training']['evaluation_strategy'] = loaded_config['training'].pop('eval_strategy')\n",
    "\n",
    "# (legacy) if someone used 'evaluation_strategy' already, keep it as-is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1pvFmIOqljv1",
    "outputId": "958c9b06-90de-4872-b2fb-cf739a655b4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'decoder_max_len': 100,\n",
       " 'encoder_max_len': 1024,\n",
       " 'eos_token': '</s>',\n",
       " 'special_tokens': ['#Address#',\n",
       "  '#Alex#',\n",
       "  '#Bob#',\n",
       "  '#CarNumber#',\n",
       "  '#CardNumber#',\n",
       "  '#DateOfBirth#',\n",
       "  '#Email#',\n",
       "  '#Kristin#',\n",
       "  '#Liliana#',\n",
       "  '#Name#',\n",
       "  '#PassportNumber#',\n",
       "  '#Person1#',\n",
       "  '#Person2#',\n",
       "  '#Person3#',\n",
       "  '#Person4#',\n",
       "  '#Person5#',\n",
       "  '#Person6#',\n",
       "  '#Person7#',\n",
       "  '#PersonName#',\n",
       "  '#PhoneNumber#',\n",
       "  '#Price#',\n",
       "  '#SSN#']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ í•˜ê¸° ìœ„í•´ tokenization ê³¼ì •ì—ì„œ í•„ìš”í•œ ì •ë³´ë“¤ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "loaded_config['tokenizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEvwCIBVll-h",
    "outputId": "ca010ac3-05be-4983-d665-2a653f0ced0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'do_eval': True,\n",
       " 'do_train': True,\n",
       " 'early_stopping_patience': 3,\n",
       " 'early_stopping_threshold': 0.001,\n",
       " 'fp16': True,\n",
       " 'generation_max_length': 100,\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'learning_rate': 2e-05,\n",
       " 'load_best_model_at_end': True,\n",
       " 'logging_dir': './logs',\n",
       " 'logging_strategy': 'epoch',\n",
       " 'lr_scheduler_type': 'cosine',\n",
       " 'num_train_epochs': 20,\n",
       " 'optim': 'adamw_torch',\n",
       " 'overwrite_output_dir': True,\n",
       " 'per_device_eval_batch_size': 32,\n",
       " 'per_device_train_batch_size': 16,\n",
       " 'predict_with_generate': True,\n",
       " 'report_to': 'wandb',\n",
       " 'save_strategy': 'epoch',\n",
       " 'save_total_limit': 5,\n",
       " 'seed': 42,\n",
       " 'warmup_ratio': 0.1,\n",
       " 'weight_decay': 0.01,\n",
       " 'evaluation_strategy': 'epoch'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ëª¨ë¸ì´ í›ˆë ¨ ì‹œ ì ìš©ë  ë§¤ê°œë³€ìˆ˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "loaded_config['training']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xhqHf1njlnyg",
    "outputId": "be9519c6-118b-4e4f-ea11-4bcca0ac42bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity': 'fc_bootcamp', 'name': 'baseline_v0', 'project': 'kobart_v1'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ëª¨ë¸ í•™ìŠµ ê³¼ì •ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ëŠ” wandb ì„¤ì • ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "loaded_config['wandb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ì„ íƒ) ì´ê³³ì— ì‚¬ìš©ìê°€ ì‚¬ìš©í•  wandb config ì„¤ì •\n",
    "loaded_config['wandb']['entity'] = \"fc_bootcamp\"\n",
    "loaded_config['wandb']['project'] = \"kobart_v1\"\n",
    "loaded_config['wandb']['name'] = \"baseline_v0\"\n",
    "\n",
    "# wandb ë¹„í™œì„±í™” (ê¶Œí•œ ë¬¸ì œ í•´ê²°)\n",
    "loaded_config['training']['report_to'] = \"wandb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Wandb setup helper (ì¶”ê°€) ---\n",
    "# ì‚¬ìš©ë²• ì˜ˆì‹œ:\n",
    "# cfg = setup_wandb(api_key=\"<YOUR_API_KEY>\", entity=\"your-entity\", project=\"your-project\", run_name=\"exp-01\")\n",
    "# import wandb\n",
    "# wandb.init(**cfg)\n",
    "\n",
    "import os\n",
    "\n",
    "def setup_wandb(api_key: str = None, entity: str = None, project: str = None, run_name: str = None, mode: str = 'online'):\n",
    "    \"\"\"\n",
    "    Helper to set WANDB API key and login programmatically.\n",
    "\n",
    "    - api_key: your W&B API key (string). If provided, sets `WANDB_API_KEY` env var.\n",
    "    - entity: W&B team or user name (maps to `entity` in `wandb.init`).\n",
    "    - project: W&B project name (maps to `project` in `wandb.init`).\n",
    "    - run_name: a human-friendly run name (maps to `name` in `wandb.init`).\n",
    "    - mode: 'online' or 'offline' (useful when running without internet or CI).\n",
    "\n",
    "    Returns a dict that can be passed to `wandb.init(**cfg)` or used to populate `loaded_config['wandb']`.\n",
    "    \"\"\"\n",
    "    if api_key:\n",
    "        os.environ['WANDB_API_KEY'] = '624410f236117b79b2ee07a4ccef31c7514a5e03'\n",
    "\n",
    "    try:\n",
    "        import wandb\n",
    "    except Exception as e:\n",
    "        raise ImportError(\"wandb is not installed. Install with `pip install wandb` and try again.\") from e\n",
    "\n",
    "    env_key = os.environ.get('WANDB_API_KEY')\n",
    "    if env_key is None:\n",
    "        print(\"Warning: No WANDB_API_KEY found in environment. Call setup_wandb(api_key=...) or set WANDB_API_KEY manually.\")\n",
    "\n",
    "    # Perform login (this will use the env var if present)\n",
    "    try:\n",
    "        wandb.login(key=env_key)\n",
    "    except Exception as e:\n",
    "        print(\"wandb.login() failed:\", e)\n",
    "\n",
    "    cfg = {\n",
    "        'mode': mode\n",
    "    }\n",
    "    if entity:\n",
    "        cfg['entity'] = fc_bootcamp\n",
    "    if project:\n",
    "        cfg['project'] = kobart_v1\n",
    "    if run_name:\n",
    "        cfg['name'] = baseline_v0\n",
    "\n",
    "    print(\"W&B helper ready. Use: import wandb; wandb.init(**cfg)\")\n",
    "    return cfg\n",
    "\n",
    "# ì˜ˆì‹œ (ì£¼ì„ í•´ì œ í›„ ì‚¬ìš©):\n",
    "# cfg = setup_wandb(api_key=\"<YOUR_API_KEY>\", entity=\"your-team-or-username\", project=\"my-project\", run_name=\"run-1\")\n",
    "# import wandb\n",
    "# wandb.init(**cfg)\n",
    "# ----------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fm4gxPRVlppj",
    "outputId": "1342aa36-3934-4f73-c912-e7d35fe6df06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'ckt_path': 'model ckt path',\n",
       " 'early_stopping': True,\n",
       " 'generate_max_length': 100,\n",
       " 'no_repeat_ngram_size': 2,\n",
       " 'num_beams': 4,\n",
       " 'remove_tokens': ['<usr>', '<s>', '</s>', '<pad>'],\n",
       " 'result_path': './prediction/'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ëª¨ë¸ì´ ìµœì¢… ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ê¸° ìœ„í•œ ë§¤ê°œë³€ìˆ˜ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "loaded_config['inference']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2zt0b-8ogCL"
   },
   "source": [
    "### 4) ë°ì´í„° ë¶ˆëŸ¬ì™€ì„œ í™•ì¸í•´ë³´ê¸°\n",
    "- ì‹¤í—˜ì—ì„œ ì“°ì¼ ë°ì´í„°ë¥¼ loadí•˜ì—¬ ë°ì´í„°ì˜ êµ¬ì¡°ì™€ ë‚´ìš©ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "- Train, dev, test ìˆœì„œëŒ€ë¡œ 12457, 499, 250ê°œ ì”© ë°ì´í„°ê°€ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "QFHIE2G04y-K",
    "outputId": "19312d21-f5bf-495f-c626-cc17b82024a4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12452</th>\n",
       "      <td>train_12455</td>\n",
       "      <td>#Person1#: ì•ˆë…•í•˜ì„¸ìš”. í˜¹ì‹œ ë§¨ì²´ìŠ¤í„°ì—ì„œ ì˜¤ì‹  Mr. Green ë§ìœ¼ì‹ ê°€ìš”...</td>\n",
       "      <td>Tan Lingì€ í°ë¨¸ë¦¬ì™€ ìˆ˜ì—¼ì´ íŠ¹ì§•ì¸ Mr. Greenì„ ë§ì´í•˜ì—¬ í˜¸í…”ë¡œ ì•ˆë‚´í•©...</td>\n",
       "      <td>í˜¸í…” ì•ˆë‚´</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12453</th>\n",
       "      <td>train_12456</td>\n",
       "      <td>#Person1#: Mister Ewingì´ ìš°ë¦¬ íšŒì˜ì¥ì— 4ì‹œì— ì˜¤ë¼ê³  í–ˆì§€, ë§...</td>\n",
       "      <td>#Person1#ê³¼ #Person2#ëŠ” Mister Ewingì˜ ìš”ì²­ì— ë”°ë¼ íšŒì˜ì¥...</td>\n",
       "      <td>íšŒì˜ ì¤€ë¹„</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12454</th>\n",
       "      <td>train_12457</td>\n",
       "      <td>#Person1#: ì˜¤ëŠ˜ ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\\n#Person2#: ì°¨ë¥¼ ë¹Œë¦¬ê³  ì‹¶...</td>\n",
       "      <td>#Person2#ëŠ” #Person1#ì˜ ë„ì›€ìœ¼ë¡œ 5ì¼ ë™ì•ˆ ì†Œí˜•ì°¨ë¥¼ ëŒ€ì—¬í•©ë‹ˆë‹¤.</td>\n",
       "      <td>ì°¨ëŸ‰ ëŒ€ì—¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12455</th>\n",
       "      <td>train_12458</td>\n",
       "      <td>#Person1#: ë„ˆ ì˜¤ëŠ˜ ì¢€ ê¸°ë¶„ ì•ˆ ì¢‹ì•„ ë³´ì¸ë‹¤? ë¬´ìŠ¨ ì¼ ìˆì–´?\\n#Pers...</td>\n",
       "      <td>#Person2#ì˜ ì–´ë¨¸ë‹ˆê°€ ì§ì¥ì„ ìƒìœ¼ì…¨ë‹¤. #Person2#ëŠ” ì–´ë¨¸ë‹ˆê°€ ìš°ìš¸í•´í•˜...</td>\n",
       "      <td>ì‹¤ì§ê³¼ ëŒ€ì²˜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12456</th>\n",
       "      <td>train_12459</td>\n",
       "      <td>#Person1#: ì—„ë§ˆ, ë‚˜ ë‹¤ìŒ ì£¼ í† ìš”ì¼ì— ì´ëª¨ë¶€ë„¤ ê°€ì¡± ë³´ëŸ¬ ê°€ëŠ”ë°, ì˜¤ëŠ˜ ...</td>\n",
       "      <td>#Person1#ì€ ë‹¤ìŒ ì£¼ í† ìš”ì¼ì— ì´ëª¨ë¶€ë„¤ ê°€ì¡±ì„ ë°©ë¬¸í•˜ê¸° ìœ„í•´ ì§ì„ ì‹¸ì•¼ í•˜ëŠ”...</td>\n",
       "      <td>ê°€ì¡± ë°©ë¬¸ ì¤€ë¹„</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             fname                                           dialogue  \\\n",
       "12452  train_12455  #Person1#: ì•ˆë…•í•˜ì„¸ìš”. í˜¹ì‹œ ë§¨ì²´ìŠ¤í„°ì—ì„œ ì˜¤ì‹  Mr. Green ë§ìœ¼ì‹ ê°€ìš”...   \n",
       "12453  train_12456  #Person1#: Mister Ewingì´ ìš°ë¦¬ íšŒì˜ì¥ì— 4ì‹œì— ì˜¤ë¼ê³  í–ˆì§€, ë§...   \n",
       "12454  train_12457  #Person1#: ì˜¤ëŠ˜ ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\\n#Person2#: ì°¨ë¥¼ ë¹Œë¦¬ê³  ì‹¶...   \n",
       "12455  train_12458  #Person1#: ë„ˆ ì˜¤ëŠ˜ ì¢€ ê¸°ë¶„ ì•ˆ ì¢‹ì•„ ë³´ì¸ë‹¤? ë¬´ìŠ¨ ì¼ ìˆì–´?\\n#Pers...   \n",
       "12456  train_12459  #Person1#: ì—„ë§ˆ, ë‚˜ ë‹¤ìŒ ì£¼ í† ìš”ì¼ì— ì´ëª¨ë¶€ë„¤ ê°€ì¡± ë³´ëŸ¬ ê°€ëŠ”ë°, ì˜¤ëŠ˜ ...   \n",
       "\n",
       "                                                 summary     topic  \n",
       "12452  Tan Lingì€ í°ë¨¸ë¦¬ì™€ ìˆ˜ì—¼ì´ íŠ¹ì§•ì¸ Mr. Greenì„ ë§ì´í•˜ì—¬ í˜¸í…”ë¡œ ì•ˆë‚´í•©...     í˜¸í…” ì•ˆë‚´  \n",
       "12453  #Person1#ê³¼ #Person2#ëŠ” Mister Ewingì˜ ìš”ì²­ì— ë”°ë¼ íšŒì˜ì¥...     íšŒì˜ ì¤€ë¹„  \n",
       "12454       #Person2#ëŠ” #Person1#ì˜ ë„ì›€ìœ¼ë¡œ 5ì¼ ë™ì•ˆ ì†Œí˜•ì°¨ë¥¼ ëŒ€ì—¬í•©ë‹ˆë‹¤.     ì°¨ëŸ‰ ëŒ€ì—¬  \n",
       "12455  #Person2#ì˜ ì–´ë¨¸ë‹ˆê°€ ì§ì¥ì„ ìƒìœ¼ì…¨ë‹¤. #Person2#ëŠ” ì–´ë¨¸ë‹ˆê°€ ìš°ìš¸í•´í•˜...    ì‹¤ì§ê³¼ ëŒ€ì²˜  \n",
       "12456  #Person1#ì€ ë‹¤ìŒ ì£¼ í† ìš”ì¼ì— ì´ëª¨ë¶€ë„¤ ê°€ì¡±ì„ ë°©ë¬¸í•˜ê¸° ìœ„í•´ ì§ì„ ì‹¸ì•¼ í•˜ëŠ”...  ê°€ì¡± ë°©ë¬¸ ì¤€ë¹„  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configì— ì €ì¥ëœ ë°ì´í„° ê²½ë¡œë¥¼ í†µí•´ trainê³¼ validation dataë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "data_path = loaded_config['general']['data_path']\n",
    "\n",
    "# train dataì˜ êµ¬ì¡°ì™€ ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "train_df = pd.read_csv(os.path.join(data_path,'train.csv'))\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "FAGaYvNZ09Sq",
    "outputId": "bf8bf286-19e7-469d-ffae-41e6ad795ae6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>dev_495</td>\n",
       "      <td>#Person1#: ìƒˆí•´ê°€ ë˜ë‹ˆê¹Œ ë‚˜ë„ ìƒˆ ì¶œë°œì„ í•˜ê¸°ë¡œ í–ˆì–´.\\n#Person2#...</td>\n",
       "      <td>#Person1#ì€ ìƒˆí•´ì— ë‹´ë°°ë¥¼ ëŠê³  ì»¤ë°ì•„ì›ƒ í•˜ê¸°ë¡œ ê²°ì‹¬í–ˆìŠµë‹ˆë‹¤. #Person...</td>\n",
       "      <td>ìƒˆí•´ ê²°ì‹¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>dev_496</td>\n",
       "      <td>#Person1#: ë„ˆ Joeë‘ ê²°í˜¼í–ˆì§€?\\n#Person2#: Joe? ë¬´ìŠ¨ ë§ì´...</td>\n",
       "      <td>#Person1#ì€ #Person2#ê°€ Joeì™€ ê²°í˜¼í–ˆë‹¤ê³  ìƒê°í•˜ì§€ë§Œ, #Perso...</td>\n",
       "      <td>ì‚¬ë‘ê³¼ ê²°í˜¼ ì˜¤í•´</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>dev_497</td>\n",
       "      <td>#Person1#: ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”, ì•„ì¤Œë§ˆ?\\n#Person2#: ì œ ì°¨ì—ì„œ ...</td>\n",
       "      <td>#Person2#ì˜ ì°¨ì—ì„œ ì†Œë¦¬ê°€ ë‚˜ë©°, ë¸Œë ˆì´í¬ ìˆ˜ë¦¬ê°€ í•„ìš”í•œ ìƒí™©ì…ë‹ˆë‹¤. #Pe...</td>\n",
       "      <td>ì°¨ëŸ‰ ì†ŒìŒ ë° ìˆ˜ë¦¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>dev_498</td>\n",
       "      <td>#Person1#: ì—¬ë³´ì„¸ìš”, ì•„ë§ˆì¡´ ê³ ê° ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\\n#...</td>\n",
       "      <td>#Person2#ê°€ ì•„ë§ˆì¡´ ê³ ê° ì„œë¹„ìŠ¤ì— ì „í™”í•˜ì—¬ ì•„ë§ˆì¡´ì—ì„œ êµ¬ë§¤í•œ ì±…ì— 53í˜ì´ì§€...</td>\n",
       "      <td>ì±… í˜ì´ì§€ ëˆ„ë½</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>dev_499</td>\n",
       "      <td>#Person1#: ë²Œì¨ ì—¬ë¦„ì´ ë‹¤ê°€ì˜¤ë‹¤ë‹ˆ ë¯¿ê¸°ì§€ ì•Šì•„. \\n#Person2#: ë§...</td>\n",
       "      <td>#Person2#ëŠ” ì—¬ë¦„ë°©í•™ ë™ì•ˆ íŒŒí‹°ì—ì„œ ì¼í•˜ëŠ” íšŒì‚¬ì—ì„œ ì¼í•˜ë©°, ì£¼ë¡œ ìŒì‹ ì¤€ë¹„...</td>\n",
       "      <td>ì—¬ë¦„ë°©í•™ ì¼ìë¦¬</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fname                                           dialogue  \\\n",
       "494  dev_495  #Person1#: ìƒˆí•´ê°€ ë˜ë‹ˆê¹Œ ë‚˜ë„ ìƒˆ ì¶œë°œì„ í•˜ê¸°ë¡œ í–ˆì–´.\\n#Person2#...   \n",
       "495  dev_496  #Person1#: ë„ˆ Joeë‘ ê²°í˜¼í–ˆì§€?\\n#Person2#: Joe? ë¬´ìŠ¨ ë§ì´...   \n",
       "496  dev_497  #Person1#: ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”, ì•„ì¤Œë§ˆ?\\n#Person2#: ì œ ì°¨ì—ì„œ ...   \n",
       "497  dev_498  #Person1#: ì—¬ë³´ì„¸ìš”, ì•„ë§ˆì¡´ ê³ ê° ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\\n#...   \n",
       "498  dev_499  #Person1#: ë²Œì¨ ì—¬ë¦„ì´ ë‹¤ê°€ì˜¤ë‹¤ë‹ˆ ë¯¿ê¸°ì§€ ì•Šì•„. \\n#Person2#: ë§...   \n",
       "\n",
       "                                               summary       topic  \n",
       "494  #Person1#ì€ ìƒˆí•´ì— ë‹´ë°°ë¥¼ ëŠê³  ì»¤ë°ì•„ì›ƒ í•˜ê¸°ë¡œ ê²°ì‹¬í–ˆìŠµë‹ˆë‹¤. #Person...       ìƒˆí•´ ê²°ì‹¬  \n",
       "495  #Person1#ì€ #Person2#ê°€ Joeì™€ ê²°í˜¼í–ˆë‹¤ê³  ìƒê°í•˜ì§€ë§Œ, #Perso...   ì‚¬ë‘ê³¼ ê²°í˜¼ ì˜¤í•´  \n",
       "496  #Person2#ì˜ ì°¨ì—ì„œ ì†Œë¦¬ê°€ ë‚˜ë©°, ë¸Œë ˆì´í¬ ìˆ˜ë¦¬ê°€ í•„ìš”í•œ ìƒí™©ì…ë‹ˆë‹¤. #Pe...  ì°¨ëŸ‰ ì†ŒìŒ ë° ìˆ˜ë¦¬  \n",
       "497  #Person2#ê°€ ì•„ë§ˆì¡´ ê³ ê° ì„œë¹„ìŠ¤ì— ì „í™”í•˜ì—¬ ì•„ë§ˆì¡´ì—ì„œ êµ¬ë§¤í•œ ì±…ì— 53í˜ì´ì§€...    ì±… í˜ì´ì§€ ëˆ„ë½  \n",
       "498  #Person2#ëŠ” ì—¬ë¦„ë°©í•™ ë™ì•ˆ íŒŒí‹°ì—ì„œ ì¼í•˜ëŠ” íšŒì‚¬ì—ì„œ ì¼í•˜ë©°, ì£¼ë¡œ ìŒì‹ ì¤€ë¹„...    ì—¬ë¦„ë°©í•™ ì¼ìë¦¬  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation dataì˜ êµ¬ì¡°ì™€ ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "val_df = pd.read_csv(os.path.join(data_path,'dev.csv'))\n",
    "val_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IIaIrpH4kWo"
   },
   "source": [
    "## 1. ë°ì´í„° ê°€ê³µ ë° ë°ì´í„°ì…‹ í´ë˜ìŠ¤ êµ¬ì¶•\n",
    "- csv file ì„ ë¶ˆëŸ¬ì™€ì„œ encoder ì™€ decoderì˜ ì…ë ¥í˜•íƒœë¡œ ê°€ê³µí•´ì¤ë‹ˆë‹¤.\n",
    "- ê°€ê³µëœ ë°ì´í„°ë¥¼ torch dataset class ë¡œ êµ¬ì¶•í•˜ì—¬ ëª¨ë¸ì— ì…ë ¥ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oWPawUUflwHa"
   },
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤ë¡œ, ë°ì´í„°ì…‹ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì…ë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "class Preprocess:\n",
    "    def __init__(self,\n",
    "            bos_token: str,\n",
    "            eos_token: str,\n",
    "            prompt_config: dict = None,           # 3-2. í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "            preprocess_config: dict = None,       # 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •\n",
    "            tfidf_extractor = None,               # 5-1. TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œê¸°\n",
    "            regex_pattern_config: dict = None,    # 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš© ì„¤ì • (NEW)\n",
    "        ) -> None:\n",
    "\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ ì„¤ì • ì €ì¥\n",
    "        self.prompt_config = prompt_config or {\n",
    "            \"use_prompt\": False,\n",
    "            \"prompt_style\": \"none\",\n",
    "            \"use_turn_separator\": False,\n",
    "            \"turn_separator\": \"[SEP]\",\n",
    "            \"tfidf_top_k\": 5,\n",
    "        }\n",
    "        \n",
    "        # 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì • ì €ì¥\n",
    "        self.preprocess_config = preprocess_config or {\n",
    "            \"normalize_slang\": False,\n",
    "        }\n",
    "        \n",
    "        # 5-1. TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œê¸°\n",
    "        self.tfidf_extractor = tfidf_extractor\n",
    "        \n",
    "        # 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš© ì„¤ì • (NEW)\n",
    "        self.regex_pattern_config = regex_pattern_config or {\n",
    "            \"use_speaker_count\": True,       # 6-1. í™”ì ìˆ˜ ëª…ì‹œ\n",
    "            \"use_pii_instruction\": True,     # 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ\n",
    "            \"show_pii_examples\": True,       # ë§ˆìŠ¤í‚¹ ì˜ˆì‹œ í‘œì‹œ\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def make_set_as_df(file_path, is_train=True):\n",
    "        \"\"\"ì‹¤í—˜ì— í•„ìš”í•œ ì»¬ëŸ¼ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\"\"\"\n",
    "        if is_train:\n",
    "            df = pd.read_csv(file_path)\n",
    "            train_df = df[['fname', 'dialogue', 'summary']]\n",
    "            return train_df\n",
    "        else:\n",
    "            df = pd.read_csv(file_path)\n",
    "            test_df = df[['fname', 'dialogue']]\n",
    "            return test_df\n",
    "\n",
    "    def apply_slang_normalization(self, text):\n",
    "        \"\"\"4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬: ìŠ¬ë­/ê°íƒ„ì‚¬/ì˜ì„±ì–´ ì •ê·œí™”\"\"\"\n",
    "        if self.preprocess_config.get(\"normalize_slang\", False):\n",
    "            return preprocess_dialogue(text, normalize_slang_flag=True)\n",
    "        return text\n",
    "\n",
    "    def apply_prompt_to_dialogue(self, dialogue_text):\n",
    "        \"\"\"\n",
    "        3-2, 5-1, 6-1, 6-2 í†µí•© í”„ë¡¬í”„íŠ¸ ì ìš©\n",
    "        \n",
    "        - 3-2: í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ ì ìš©\n",
    "        - 5-1: TF-IDF í‚¤ì›Œë“œ í¬í•¨\n",
    "        - 6-1: í™”ì ìˆ˜ ëª…ì‹œ (ê· í˜• ìˆëŠ” ìš”ì•½ ìœ ë„)\n",
    "        - 6-2: PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ (ROUGE ì ìˆ˜ í–¥ìƒ)\n",
    "        \"\"\"\n",
    "        text = dialogue_text\n",
    "        \n",
    "        # ë°œí™” êµ¬ë¶„ì ì ìš© (ì„ íƒì )\n",
    "        if self.prompt_config.get(\"use_turn_separator\", False):\n",
    "            text = add_turn_separators(text, self.prompt_config.get(\"turn_separator\", \"[SEP]\"))\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ ì ìš©\n",
    "        if self.prompt_config.get(\"use_prompt\", False):\n",
    "            prompt_style = self.prompt_config.get(\"prompt_style\", \"default\")\n",
    "            tfidf_top_k = self.prompt_config.get(\"tfidf_top_k\", 5)\n",
    "            \n",
    "            # 6-1, 6-2 ì„¤ì • ê°€ì ¸ì˜¤ê¸°\n",
    "            use_speaker_count = self.regex_pattern_config.get(\"use_speaker_count\", True)\n",
    "            use_pii_instruction = self.regex_pattern_config.get(\"use_pii_instruction\", True)\n",
    "            \n",
    "            # 5-1. TF-IDF í‚¤ì›Œë“œ ìŠ¤íƒ€ì¼ì¸ ê²½ìš° extractor ì „ë‹¬\n",
    "            if prompt_style in [\"keyword\", \"keyword_aware\"] and self.tfidf_extractor is not None:\n",
    "                text = build_summary_prompt(\n",
    "                    text, \n",
    "                    prompt_style=prompt_style, \n",
    "                    tfidf_extractor=self.tfidf_extractor, \n",
    "                    top_k=tfidf_top_k,\n",
    "                    use_speaker_count=use_speaker_count,      # 6-1\n",
    "                    use_pii_instruction=use_pii_instruction    # 6-2\n",
    "                )\n",
    "            else:\n",
    "                # ê¸°íƒ€ ìŠ¤íƒ€ì¼ì—ì„œë„ 6-1, 6-2 ì ìš©\n",
    "                text = build_summary_prompt(\n",
    "                    text, \n",
    "                    prompt_style=prompt_style,\n",
    "                    use_speaker_count=use_speaker_count,      # 6-1\n",
    "                    use_pii_instruction=use_pii_instruction    # 6-2\n",
    "                )\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def full_preprocess_pipeline(self, dialogue_text):\n",
    "        \"\"\"\n",
    "        ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸: ìŠ¬ë­ ì •ê·œí™” â†’ í”„ë¡¬í”„íŠ¸ ì ìš©\n",
    "        \n",
    "        ì²˜ë¦¬ ìˆœì„œ:\n",
    "        1. ìŠ¬ë­/ê°íƒ„ì‚¬/ì˜ì„±ì–´ ì •ê·œí™” (4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬)\n",
    "        2. ë°œí™” êµ¬ë¶„ì ì¶”ê°€ (ì„ íƒì )\n",
    "        3. í”„ë¡¬í”„íŠ¸ ì¶”ê°€ (3-2. í”„ë¡¬í”„íŠ¸ ì„¤ì •)\n",
    "           - TF-IDF í‚¤ì›Œë“œ í¬í•¨ (5-1)\n",
    "           - í™”ì ìˆ˜ ëª…ì‹œ (6-1)\n",
    "           - PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ (6-2)\n",
    "        \"\"\"\n",
    "        text = dialogue_text\n",
    "        \n",
    "        # Step 1: ìŠ¬ë­ ì •ê·œí™”\n",
    "        text = self.apply_slang_normalization(text)\n",
    "        \n",
    "        # Step 2 & 3: í”„ë¡¬í”„íŠ¸ ì ìš© (êµ¬ë¶„ì + TF-IDF + 6-1 + 6-2)\n",
    "        text = self.apply_prompt_to_dialogue(text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def make_input(self, dataset, is_test=False, tokenizer=None, encoder_max_len=None, \n",
    "                   prompt=None, apply_dynamic_prompt=True):\n",
    "        \"\"\"\n",
    "        BART ëª¨ë¸ì˜ ì…ë ¥, ì¶œë ¥ í˜•íƒœë¥¼ ë§ì¶”ê¸° ìœ„í•´ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸:\n",
    "        1. ìŠ¬ë­/ê°íƒ„ì‚¬ ì •ê·œí™” (4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬)\n",
    "        2. TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ (5-1)\n",
    "        3. í”„ë¡¬í”„íŠ¸ ì¶”ê°€ (3-2. í”„ë¡¬í”„íŠ¸ ì„¤ì •)\n",
    "        4. 6-1 í™”ì ìˆ˜ ëª…ì‹œ + 6-2 PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ\n",
    "        5. í† í° ê¸°ë°˜ truncation (ì„ íƒì )\n",
    "        \"\"\"\n",
    "        if is_test:\n",
    "            encoder_input = []\n",
    "            decoder_input = []\n",
    "            for d in dataset['dialogue']:\n",
    "                text = d\n",
    "                \n",
    "                # ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš©\n",
    "                if apply_dynamic_prompt:\n",
    "                    text = self.full_preprocess_pipeline(text)\n",
    "                elif prompt:\n",
    "                    text = self.apply_slang_normalization(text)\n",
    "                    text = apply_summary_prompt(text, prompt)\n",
    "                else:\n",
    "                    text = self.apply_slang_normalization(text)\n",
    "                \n",
    "                # í† í° ê¸°ë°˜ truncation\n",
    "                if tokenizer is not None and encoder_max_len is not None:\n",
    "                    text = smart_truncate_dialogue(text, tokenizer, encoder_max_len)\n",
    "                \n",
    "                encoder_input.append(text)\n",
    "                decoder_input.append(self.bos_token)\n",
    "            return encoder_input, decoder_input\n",
    "        else:\n",
    "            # í•™ìŠµ ë°ì´í„° ì²˜ë¦¬\n",
    "            encoder_input = []\n",
    "            for d in dataset['dialogue']:\n",
    "                text = d\n",
    "                \n",
    "                # ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš©\n",
    "                if apply_dynamic_prompt:\n",
    "                    text = self.full_preprocess_pipeline(text)\n",
    "                else:\n",
    "                    text = self.apply_slang_normalization(text)\n",
    "                \n",
    "                encoder_input.append(text)\n",
    "            \n",
    "            # Gold targetì„ ë””ì½”ë” ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "            decoder_input = dataset['summary'].apply(lambda x: self.bos_token + str(x))\n",
    "            decoder_output = dataset['summary'].apply(lambda x: str(x) + self.eos_token)\n",
    "            return encoder_input, decoder_input.tolist(), decoder_output.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6GDvodoF8sED"
   },
   "outputs": [],
   "source": [
    "# Trainì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "class DatasetForTrain(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, labels, len):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.labels = labels\n",
    "        self.len = len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()} # item[input_ids], item[attention_mask]\n",
    "        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()} # item2[input_ids], item2[attention_mask]\n",
    "        item2['decoder_input_ids'] = item2['input_ids']\n",
    "        item2['decoder_attention_mask'] = item2['attention_mask']\n",
    "        item2.pop('input_ids')\n",
    "        item2.pop('attention_mask')\n",
    "        item.update(item2) #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask]\n",
    "        item['labels'] = self.labels['input_ids'][idx] #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask], item[labels]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# Validationì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "class DatasetForVal(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, labels, len):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.labels = labels\n",
    "        self.len = len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()} # item[input_ids], item[attention_mask]\n",
    "        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()} # item2[input_ids], item2[attention_mask]\n",
    "        item2['decoder_input_ids'] = item2['input_ids']\n",
    "        item2['decoder_attention_mask'] = item2['attention_mask']\n",
    "        item2.pop('input_ids')\n",
    "        item2.pop('attention_mask')\n",
    "        item.update(item2) #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask]\n",
    "        item['labels'] = self.labels['input_ids'][idx] #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask], item[labels]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# Testì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "class DatasetForInference(Dataset):\n",
    "    def __init__(self, encoder_input, test_id, len):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.test_id = test_id\n",
    "        self.len = len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
    "        item['ID'] = self.test_id[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "hT9z4vvS2CCb"
   },
   "outputs": [],
   "source": [
    "# tokenization ê³¼ì •ê¹Œì§€ ì§„í–‰ëœ ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ì— ì…ë ¥ë  ë°ì´í„°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "def prepare_train_dataset(config, preprocessor, data_path, tokenizer):\n",
    "    \"\"\"\n",
    "    3-2 ê°œì„ : í”„ë¡¬í”„íŠ¸ ì„¤ì •ì´ ì ìš©ëœ ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "    \"\"\"\n",
    "    train_file_path = os.path.join(data_path,'train.csv')\n",
    "    val_file_path = os.path.join(data_path,'dev.csv')\n",
    "\n",
    "    # train, validationì— ëŒ€í•´ ê°ê° ë°ì´í„°í”„ë ˆì„ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
    "    train_data = preprocessor.make_set_as_df(train_file_path)\n",
    "    val_data = preprocessor.make_set_as_df(val_file_path)\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f'[ì›ë³¸] train_data dialogue:\\n {train_data[\"dialogue\"][0][:200]}...')\n",
    "    print(f'train_label:\\n {train_data[\"summary\"][0]}')\n",
    "\n",
    "    # 3-2. í”„ë¡¬í”„íŠ¸ ì ìš©ëœ ë°ì´í„°ë¡œ ë³€í™˜\n",
    "    encoder_input_train, decoder_input_train, decoder_output_train = preprocessor.make_input(\n",
    "        train_data, \n",
    "        apply_dynamic_prompt=True  # í”„ë¡¬í”„íŠ¸ ì„¤ì • ì‚¬ìš©\n",
    "    )\n",
    "    encoder_input_val, decoder_input_val, decoder_output_val = preprocessor.make_input(\n",
    "        val_data,\n",
    "        apply_dynamic_prompt=True\n",
    "    )\n",
    "    \n",
    "    # í”„ë¡¬í”„íŠ¸ ì ìš© í™•ì¸\n",
    "    print('-'*150)\n",
    "    print(f'[í”„ë¡¬í”„íŠ¸ ì ìš© í›„] encoder_input_train[0]:\\n {encoder_input_train[0][:300]}...')\n",
    "    print('-'*10, 'Load data complete', '-'*10)\n",
    "\n",
    "    tokenized_encoder_inputs = tokenizer(encoder_input_train, return_tensors=\"pt\", padding=True,\n",
    "                            add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
    "    tokenized_decoder_inputs = tokenizer(decoder_input_train, return_tensors=\"pt\", padding=True,\n",
    "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "    tokenized_decoder_ouputs = tokenizer(decoder_output_train, return_tensors=\"pt\", padding=True,\n",
    "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    train_inputs_dataset = DatasetForTrain(tokenized_encoder_inputs, tokenized_decoder_inputs, tokenized_decoder_ouputs,len(encoder_input_train))\n",
    "\n",
    "    val_tokenized_encoder_inputs = tokenizer(encoder_input_val, return_tensors=\"pt\", padding=True,\n",
    "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
    "    val_tokenized_decoder_inputs = tokenizer(decoder_input_val, return_tensors=\"pt\", padding=True,\n",
    "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "    val_tokenized_decoder_ouputs = tokenizer(decoder_output_val, return_tensors=\"pt\", padding=True,\n",
    "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    val_inputs_dataset = DatasetForVal(val_tokenized_encoder_inputs, val_tokenized_decoder_inputs, val_tokenized_decoder_ouputs,len(encoder_input_val))\n",
    "\n",
    "    print('-'*10, 'Make dataset complete', '-'*10)\n",
    "    \n",
    "    # í† í°í™” í›„ special token í™•ì¸\n",
    "    print(\"\\nğŸ“Š í† í°í™” ê²°ê³¼ í™•ì¸:\")\n",
    "    sample_tokens = tokenized_encoder_inputs['input_ids'][0][:50]\n",
    "    decoded_sample = tokenizer.decode(sample_tokens)\n",
    "    print(f\"   ì²« 50 í† í° ë””ì½”ë”©: {decoded_sample}...\")\n",
    "    \n",
    "    return train_inputs_dataset, val_inputs_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5sKIJ5K5Pz1"
   },
   "source": [
    "## 2. Trainer ë° Trainingargs êµ¬ì¶•í•˜ê¸°\n",
    "- Huggingface ì˜ Trainer ì™€ Training argumentsë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµì„ ì¼ê´„ì ìœ¼ë¡œ ì²˜ë¦¬í•´ì£¼ëŠ” í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "aQk8ILcEeGNz"
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì„±ëŠ¥ì— ëŒ€í•œ í‰ê°€ ì§€í‘œë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ë³¸ ëŒ€íšŒì—ì„œëŠ” ROUGE ì ìˆ˜ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "def compute_metrics(config,tokenizer,pred):\n",
    "    rouge = Rouge()\n",
    "    predictions = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    predictions[predictions == -100] = tokenizer.pad_token_id\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, clean_up_tokenization_spaces=True)\n",
    "    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # ì •í™•í•œ í‰ê°€ë¥¼ ìœ„í•´ ë¯¸ë¦¬ ì •ì˜ëœ ë¶ˆí•„ìš”í•œ ìƒì„±í† í°ë“¤ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "    replaced_predictions = decoded_preds.copy()\n",
    "    replaced_labels = labels.copy()\n",
    "    remove_tokens = config['inference']['remove_tokens']\n",
    "    for token in remove_tokens:\n",
    "        replaced_predictions = [sentence.replace(token,\" \") for sentence in replaced_predictions]\n",
    "        replaced_labels = [sentence.replace(token,\" \") for sentence in replaced_labels]\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[0]}\")\n",
    "    print(f\"GOLD: {replaced_labels[0]}\")\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[1]}\")\n",
    "    print(f\"GOLD: {replaced_labels[1]}\")\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[2]}\")\n",
    "    print(f\"GOLD: {replaced_labels[2]}\")\n",
    "\n",
    "    # ìµœì¢…ì ì¸ ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    results = rouge.get_scores(replaced_predictions, replaced_labels,avg=True)\n",
    "\n",
    "    # ROUGE ì ìˆ˜ ì¤‘ F-1 scoreë¥¼ í†µí•´ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "    result = {key: value[\"f\"] for key, value in results.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "RInkG8g-HjBi"
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì„±ëŠ¥ì— ëŒ€í•œ í‰ê°€ ì§€í‘œë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ë³¸ ëŒ€íšŒì—ì„œëŠ” ROUGE ì ìˆ˜ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "def compute_metrics(config,tokenizer,pred):\n",
    "    rouge = Rouge()\n",
    "    predictions = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    predictions[predictions == -100] = tokenizer.pad_token_id\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, clean_up_tokenization_spaces=True)\n",
    "    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # ì •í™•í•œ í‰ê°€ë¥¼ ìœ„í•´ ë¯¸ë¦¬ ì •ì˜ëœ ë¶ˆí•„ìš”í•œ ìƒì„±í† í°ë“¤ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "    replaced_predictions = decoded_preds.copy()\n",
    "    replaced_labels = labels.copy()\n",
    "    remove_tokens = config['inference']['remove_tokens']\n",
    "    for token in remove_tokens:\n",
    "        replaced_predictions = [sentence.replace(token,\" \") for sentence in replaced_predictions]\n",
    "        replaced_labels = [sentence.replace(token,\" \") for sentence in replaced_labels]\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[0]}\")\n",
    "    print(f\"GOLD: {replaced_labels[0]}\")\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[1]}\")\n",
    "    print(f\"GOLD: {replaced_labels[1]}\")\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[2]}\")\n",
    "    print(f\"GOLD: {replaced_labels[2]}\")\n",
    "\n",
    "    # ìµœì¢…ì ì¸ ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    results = rouge.get_scores(replaced_predictions, replaced_labels,avg=True)\n",
    "\n",
    "    # ROUGE ì ìˆ˜ ì¤‘ F-1 scoreë¥¼ í†µí•´ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "    result = {key: value[\"f\"] for key, value in results.items()}\n",
    "    return result\n",
    "\n",
    "\n",
    "# í•™ìŠµì„ ìœ„í•œ trainer í´ë˜ìŠ¤ì™€ ë§¤ê°œë³€ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "def load_trainer_for_train(config,generate_model,tokenizer,train_inputs_dataset,val_inputs_dataset):\n",
    "    print('-'*10, 'Make training arguments', '-'*10,)\n",
    "    # set training args\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=config['general']['output_dir'], # model output directory\n",
    "                overwrite_output_dir=config['training']['overwrite_output_dir'],\n",
    "                num_train_epochs=config['training']['num_train_epochs'],  # total number of training epochs\n",
    "                learning_rate=config['training']['learning_rate'], # learning_rate\n",
    "                per_device_train_batch_size=config['training']['per_device_train_batch_size'], # batch size per device during training\n",
    "                per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],# batch size for evaluation\n",
    "                warmup_ratio=config['training']['warmup_ratio'],  # number of warmup steps for learning rate scheduler\n",
    "                weight_decay=config['training']['weight_decay'],  # strength of weight decay\n",
    "                lr_scheduler_type=config['training']['lr_scheduler_type'],\n",
    "                optim =config['training']['optim'],\n",
    "                gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
    "                evaluation_strategy=config['training'].get('evaluation_strategy', 'epoch'), # transformers expects this key\n",
    "                save_strategy =config['training']['save_strategy'],\n",
    "                save_total_limit=config['training']['save_total_limit'], # number of total save model.\n",
    "                fp16=config['training']['fp16'],\n",
    "                load_best_model_at_end=config['training']['load_best_model_at_end'], # ìµœì¢…ì ìœ¼ë¡œ ê°€ì¥ ë†’ì€ ì ìˆ˜ ì €ì¥\n",
    "                seed=config['training']['seed'],\n",
    "                logging_dir=config['training']['logging_dir'], # directory for storing logs\n",
    "                logging_strategy=config['training']['logging_strategy'],\n",
    "                predict_with_generate=config['training']['predict_with_generate'], #To use BLEU or ROUGE score\n",
    "                generation_max_length=config['training']['generation_max_length'],\n",
    "                do_train=config['training']['do_train'],\n",
    "                do_eval=config['training']['do_eval'],\n",
    "                report_to=config['training']['report_to'] # (ì„ íƒ) wandbë¥¼ ì‚¬ìš©í•  ë•Œ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "            )\n",
    "\n",
    "    # (ì„ íƒ) wandb ì‚¬ìš© ì‹œì—ë§Œ ì´ˆê¸°í™”\n",
    "    if config['training']['report_to'] == 'wandb':\n",
    "        wandb.init(\n",
    "            entity=config['wandb']['entity'],\n",
    "            project=config['wandb']['project'],\n",
    "            name=config['wandb']['name'],\n",
    "        )\n",
    "        # ëª¨ë¸ checkpointë¥¼ wandbì— ì €ì¥í•˜ë„ë¡ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "        os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "        os.environ[\"WANDB_WATCH\"]=\"false\"\n",
    "\n",
    "    # Validation lossê°€ ë” ì´ìƒ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œ í•™ìŠµì„ ì¤‘ë‹¨ì‹œí‚¤ëŠ” EarlyStopping ê¸°ëŠ¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    MyCallback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=config['training']['early_stopping_patience'],\n",
    "        early_stopping_threshold=config['training']['early_stopping_threshold']\n",
    "    )\n",
    "    print('-'*10, 'Make training arguments complete', '-'*10,)\n",
    "    print('-'*10, 'Make trainer', '-'*10,)\n",
    "\n",
    "    # Trainer í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=generate_model, # ì‚¬ìš©ìê°€ ì‚¬ì „ í•™ìŠµí•˜ê¸° ìœ„í•´ ì‚¬ìš©í•  ëª¨ë¸ì„ ì…ë ¥í•©ë‹ˆë‹¤.\n",
    "        args=training_args,\n",
    "        train_dataset=train_inputs_dataset,\n",
    "        eval_dataset=val_inputs_dataset,\n",
    "        compute_metrics = lambda pred: compute_metrics(config,tokenizer, pred),\n",
    "        callbacks = [MyCallback]\n",
    "    )\n",
    "    print('-'*10, 'Make trainer complete', '-'*10,)\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "KKWHe8dE5fSx"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3-1. í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ (Special Token ë™ì  ë“±ë¡ í¬í•¨)\n",
    "# ============================================================================\n",
    "\n",
    "def load_tokenizer_and_model_for_train(config, device, verbose=True):\n",
    "    \"\"\"\n",
    "    í•™ìŠµì„ ìœ„í•œ tokenizerì™€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "    \n",
    "    3-1 ê°œì„ ì‚¬í•­:\n",
    "    - í•™ìŠµ ë°ì´í„°ì—ì„œ ë™ì ìœ¼ë¡œ ìˆ˜ì§‘í•œ special tokens ë“±ë¡\n",
    "    - model.resize_token_embeddings() í˜¸ì¶œë¡œ ì„ë² ë”© ì‚¬ì´ì¦ˆ ì¡°ì •\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print('-' * 10, 'Load tokenizer & model', '-' * 10)\n",
    "        print('-' * 10, f'Model Name : {config[\"general\"][\"model_name\"]}', '-' * 10)\n",
    "    \n",
    "    model_name = config['general']['model_name']\n",
    "    bart_config = BartConfig().from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    generate_model = BartForConditionalGeneration.from_pretrained(\n",
    "        config['general']['model_name'], \n",
    "        config=bart_config\n",
    "    )\n",
    "\n",
    "    # ============================================================\n",
    "    # 3-1. Special Tokens ë“±ë¡ (ë™ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ í† í° ì‚¬ìš©)\n",
    "    # ============================================================\n",
    "    special_tokens = config['tokenizer']['special_tokens']\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nğŸ“Œ ë“±ë¡í•  Special Tokens ({len(special_tokens)}ê°œ):\")\n",
    "        print(f\"   {special_tokens}\")\n",
    "    \n",
    "    # additional_special_tokensë¡œ ë“±ë¡\n",
    "    special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "    num_added = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nâœ… ìƒˆë¡œ ì¶”ê°€ëœ í† í° ìˆ˜: {num_added}\")\n",
    "        print(f\"   í† í¬ë‚˜ì´ì € vocab size: {len(tokenizer)}\")\n",
    "\n",
    "    # âš ï¸ ì¤‘ìš”: ëª¨ë¸ ì„ë² ë”© ì‚¬ì´ì¦ˆ ì¡°ì •\n",
    "    generate_model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   ëª¨ë¸ ì„ë² ë”© ì‚¬ì´ì¦ˆ ì¡°ì • ì™„ë£Œ: {generate_model.config.vocab_size}\")\n",
    "    \n",
    "    generate_model.to(device)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "        print(\"ğŸ” Special Token í™•ì¸:\")\n",
    "        print(\"-\" * 60)\n",
    "        for token in special_tokens[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥\n",
    "            token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "            print(f\"   {token} â†’ ID: {token_id}\")\n",
    "        if len(special_tokens) > 5:\n",
    "            print(f\"   ... ì™¸ {len(special_tokens) - 5}ê°œ\")\n",
    "        \n",
    "        print('-' * 10, 'Load tokenizer & model complete', '-' * 10)\n",
    "    \n",
    "    return generate_model, tokenizer\n",
    "\n",
    "\n",
    "def load_tokenizer_and_model_for_inference(config, checkpoint_path, device, verbose=True):\n",
    "    \"\"\"\n",
    "    ì¶”ë¡ ì„ ìœ„í•œ tokenizerì™€ fine-tuned ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "    \n",
    "    3-1 ê°œì„ ì‚¬í•­:\n",
    "    - checkpointì—ì„œ ë¡œë“œ ì‹œì—ë„ special tokens ì¼ê´€ì„± ìœ ì§€\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print('-' * 10, 'Load tokenizer & model for inference', '-' * 10)\n",
    "        print('-' * 10, f'Checkpoint: {checkpoint_path}', '-' * 10)\n",
    "    \n",
    "    # Checkpointì—ì„œ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "    \n",
    "    # Special tokens í™•ì¸ ë° ì¶”ê°€ (checkpointì— ì—†ì„ ê²½ìš°)\n",
    "    special_tokens = config['tokenizer']['special_tokens']\n",
    "    existing_special = tokenizer.additional_special_tokens or []\n",
    "    \n",
    "    missing_tokens = [t for t in special_tokens if t not in existing_special]\n",
    "    if missing_tokens:\n",
    "        if verbose:\n",
    "            print(f\"âš ï¸ ëˆ„ë½ëœ special tokens ì¶”ê°€: {missing_tokens}\")\n",
    "        special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "        tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    model = BartForConditionalGeneration.from_pretrained(checkpoint_path)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "        print(f\"   í† í¬ë‚˜ì´ì € vocab size: {len(tokenizer)}\")\n",
    "        print(f\"   ëª¨ë¸ ì„ë² ë”© ì‚¬ì´ì¦ˆ: {model.config.vocab_size}\")\n",
    "        print('-' * 10, 'Load complete', '-' * 10)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvutzKQYvQgl"
   },
   "source": [
    "## 3. ëª¨ë¸ í•™ìŠµí•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImZUb-BC42J-"
   },
   "source": [
    "- ì•ì—ì„œ êµ¬ì¶•í•œ í´ë˜ìŠ¤ ë° í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµ ì§„í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "qnA96wmR44is"
   },
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ í•™ìŠµ ë©”ì¸ í•¨ìˆ˜\n",
    "    3-1, 3-2, 4, 5-1, 6-1, 6-2 ê°œì„ : \n",
    "    - Special tokens + í”„ë¡¬í”„íŠ¸ + êµ¬ì–´ì²´ ì „ì²˜ë¦¬ + TF-IDF í‚¤ì›Œë“œ\n",
    "    - í™”ì ìˆ˜ ëª…ì‹œ + PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ\n",
    "    \"\"\"\n",
    "    # ì‚¬ìš©í•  deviceë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print('-' * 10, f'device : {device}', '-' * 10)\n",
    "    print(torch.__version__)\n",
    "\n",
    "    # ì‚¬ìš©í•  ëª¨ë¸ê³¼ tokenizerë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "    generate_model, tokenizer = load_tokenizer_and_model_for_train(config, device)\n",
    "    print('-' * 10, \"tokenizer special tokens : \", tokenizer.special_tokens_map, '-' * 10)\n",
    "\n",
    "    # 3-2. í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "    prompt_config = config.get('prompt', {\n",
    "        \"use_prompt\": False,\n",
    "        \"prompt_style\": \"none\",\n",
    "        \"use_turn_separator\": False,\n",
    "        \"turn_separator\": \"[SEP]\",\n",
    "        \"tfidf_top_k\": 5,\n",
    "    })\n",
    "    \n",
    "    # 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •\n",
    "    preprocess_config = config.get('preprocess', {\n",
    "        \"normalize_slang\": False,\n",
    "    })\n",
    "    \n",
    "    # 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš© ì„¤ì • (NEW)\n",
    "    regex_pattern_config = config.get('regex_pattern', {\n",
    "        \"use_speaker_count\": True,       # 6-1. í™”ì ìˆ˜ ëª…ì‹œ\n",
    "        \"use_pii_instruction\": True,     # 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ\n",
    "        \"show_pii_examples\": True,       # ë§ˆìŠ¤í‚¹ ì˜ˆì‹œ í‘œì‹œ\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ í”„ë¡¬í”„íŠ¸ ì„¤ì •:\")\n",
    "    print(f\"   - í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {prompt_config.get('use_prompt', False)}\")\n",
    "    print(f\"   - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: {prompt_config.get('prompt_style', 'none')}\")\n",
    "    print(f\"   - ë°œí™” êµ¬ë¶„ì ì‚¬ìš©: {prompt_config.get('use_turn_separator', False)}\")\n",
    "    if prompt_config.get('prompt_style') in ['keyword', 'keyword_aware']:\n",
    "        print(f\"   - TF-IDF í‚¤ì›Œë“œ ìˆ˜: {prompt_config.get('tfidf_top_k', 5)}ê°œ\")\n",
    "    \n",
    "    print(f\"\\nğŸ—£ï¸ êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •:\")\n",
    "    print(f\"   - ìŠ¬ë­ ì •ê·œí™”: {preprocess_config.get('normalize_slang', False)}\")\n",
    "    \n",
    "    print(f\"\\nğŸ” 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš©:\")\n",
    "    print(f\"   - 6-1. í™”ì ìˆ˜ ëª…ì‹œ: {regex_pattern_config.get('use_speaker_count', True)}\")\n",
    "    print(f\"   - 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: {regex_pattern_config.get('use_pii_instruction', True)}\")\n",
    "    \n",
    "    # 5-1. TF-IDF extractor ì‚¬ìš© ì—¬ë¶€ í™•ì¸\n",
    "    use_tfidf = prompt_config.get('prompt_style') in ['keyword', 'keyword_aware']\n",
    "    tfidf_ext = tfidf_extractor if use_tfidf else None\n",
    "    \n",
    "    if use_tfidf:\n",
    "        print(f\"\\nğŸ“Š TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ í™œì„±í™”\")\n",
    "    \n",
    "    preprocessor = Preprocess(\n",
    "        config['tokenizer']['bos_token'], \n",
    "        config['tokenizer']['eos_token'],\n",
    "        prompt_config=prompt_config,\n",
    "        preprocess_config=preprocess_config,\n",
    "        tfidf_extractor=tfidf_ext,           # 5-1. TF-IDF extractor ì „ë‹¬\n",
    "        regex_pattern_config=regex_pattern_config  # 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ ì„¤ì • ì „ë‹¬\n",
    "    )\n",
    "    \n",
    "    data_path = config['general']['data_path']\n",
    "    train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(config, preprocessor, data_path, tokenizer)\n",
    "\n",
    "    # Trainer í´ë˜ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "    trainer = load_trainer_for_train(config, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset)\n",
    "    trainer.train()   # ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
    "\n",
    "    # (ì„ íƒ) wandb ì‚¬ìš© ì‹œì—ë§Œ ì¢…ë£Œ\n",
    "    if config['training']['report_to'] == 'wandb':\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1DMS60wL-Dhv",
    "outputId": "cbb6aba7-18ff-4d12-b9e7-2a2ef31d94d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- device : cuda:0 ----------\n",
      "2.1.0\n",
      "---------- Load tokenizer & model ----------\n",
      "---------- Model Name : digit82/kobart-summarization ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Œ ë“±ë¡í•  Special Tokens (22ê°œ):\n",
      "   ['#Address#', '#Alex#', '#Bob#', '#CarNumber#', '#CardNumber#', '#DateOfBirth#', '#Email#', '#Kristin#', '#Liliana#', '#Name#', '#PassportNumber#', '#Person1#', '#Person2#', '#Person3#', '#Person4#', '#Person5#', '#Person6#', '#Person7#', '#PersonName#', '#PhoneNumber#', '#Price#', '#SSN#']\n",
      "\n",
      "âœ… ìƒˆë¡œ ì¶”ê°€ëœ í† í° ìˆ˜: 22\n",
      "   í† í¬ë‚˜ì´ì € vocab size: 30022\n",
      "   ëª¨ë¸ ì„ë² ë”© ì‚¬ì´ì¦ˆ ì¡°ì • ì™„ë£Œ: 30022\n",
      "\n",
      "------------------------------------------------------------\n",
      "ğŸ” Special Token í™•ì¸:\n",
      "------------------------------------------------------------\n",
      "   #Address# â†’ ID: 30002\n",
      "   #Alex# â†’ ID: 30007\n",
      "   #Bob# â†’ ID: 30008\n",
      "   #CarNumber# â†’ ID: 30006\n",
      "   #CardNumber# â†’ ID: 30020\n",
      "   ... ì™¸ 17ê°œ\n",
      "---------- Load tokenizer & model complete ----------\n",
      "---------- tokenizer special tokens :  {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>', 'additional_special_tokens': ['#Email#', '#Name#', '#Address#', '#Person7#', '#Person3#', '#PhoneNumber#', '#CarNumber#', '#Alex#', '#Bob#', '#PersonName#', '#Price#', '#DateOfBirth#', '#PassportNumber#', '#Person2#', '#Person1#', '#Person6#', '#Kristin#', '#SSN#', '#Person5#', '#Liliana#', '#CardNumber#', '#Person4#']} ----------\n",
      "\n",
      "ğŸ“‹ í”„ë¡¬í”„íŠ¸ ì„¤ì •:\n",
      "   - í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: True\n",
      "   - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: balanced\n",
      "   - ë°œí™” êµ¬ë¶„ì ì‚¬ìš©: False\n",
      "\n",
      "ğŸ—£ï¸ êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •:\n",
      "   - ìŠ¬ë­ ì •ê·œí™”: True\n",
      "\n",
      "ğŸ” 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš©:\n",
      "   - 6-1. í™”ì ìˆ˜ ëª…ì‹œ: True\n",
      "   - 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: True\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[ì›ë³¸] train_data dialogue:\n",
      " #Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? \n",
      "#Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. \n",
      "#Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. \n",
      "#Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”. \n",
      "#Person...\n",
      "train_label:\n",
      " Mr. SmithëŠ” Dr. Hawkinsì—ê²Œ ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ëŸ¬ ì™€ì„œ, ë§¤ë…„ ê²€ì§„ í•„ìš”ì„±ì„ ì•ˆë‚´ë°›ê³  í¡ì—° ìŠµê´€ ê°œì„ ì„ ìœ„í•œ ë„ì›€ì„ ì œì•ˆë°›ì•˜ìŠµë‹ˆë‹¤.\n",
      "\n",
      "------------------------------------------------------------\n",
      "ğŸ” Special Token í™•ì¸:\n",
      "------------------------------------------------------------\n",
      "   #Address# â†’ ID: 30002\n",
      "   #Alex# â†’ ID: 30007\n",
      "   #Bob# â†’ ID: 30008\n",
      "   #CarNumber# â†’ ID: 30006\n",
      "   #CardNumber# â†’ ID: 30020\n",
      "   ... ì™¸ 17ê°œ\n",
      "---------- Load tokenizer & model complete ----------\n",
      "---------- tokenizer special tokens :  {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>', 'additional_special_tokens': ['#Email#', '#Name#', '#Address#', '#Person7#', '#Person3#', '#PhoneNumber#', '#CarNumber#', '#Alex#', '#Bob#', '#PersonName#', '#Price#', '#DateOfBirth#', '#PassportNumber#', '#Person2#', '#Person1#', '#Person6#', '#Kristin#', '#SSN#', '#Person5#', '#Liliana#', '#CardNumber#', '#Person4#']} ----------\n",
      "\n",
      "ğŸ“‹ í”„ë¡¬í”„íŠ¸ ì„¤ì •:\n",
      "   - í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: True\n",
      "   - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: balanced\n",
      "   - ë°œí™” êµ¬ë¶„ì ì‚¬ìš©: False\n",
      "\n",
      "ğŸ—£ï¸ êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •:\n",
      "   - ìŠ¬ë­ ì •ê·œí™”: True\n",
      "\n",
      "ğŸ” 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš©:\n",
      "   - 6-1. í™”ì ìˆ˜ ëª…ì‹œ: True\n",
      "   - 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: True\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[ì›ë³¸] train_data dialogue:\n",
      " #Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? \n",
      "#Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. \n",
      "#Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. \n",
      "#Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”. \n",
      "#Person...\n",
      "train_label:\n",
      " Mr. SmithëŠ” Dr. Hawkinsì—ê²Œ ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ëŸ¬ ì™€ì„œ, ë§¤ë…„ ê²€ì§„ í•„ìš”ì„±ì„ ì•ˆë‚´ë°›ê³  í¡ì—° ìŠµê´€ ê°œì„ ì„ ìœ„í•œ ë„ì›€ì„ ì œì•ˆë°›ì•˜ìŠµë‹ˆë‹¤.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[í”„ë¡¬í”„íŠ¸ ì ìš© í›„] encoder_input_train[0]:\n",
      " ë‹¤ìŒì€ 2ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ë‘ ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \n",
      "\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? #Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. #Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. #Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”. #Person1#: ìŒ, ì‹¬ê°í•œ ì§ˆë³‘ì„ í”¼í•˜ë ¤ë©´ ë¯¸ë¦¬ ë°œê²¬í•˜ëŠ” ê²Œ ì œì¼ ì¢‹ê±°ë“ ìš”. ë³¸ì¸ì„ ìœ„í•´ì„œë¼ë„ ë§¤ë…„ í•œ ...\n",
      "---------- Load data complete ----------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[í”„ë¡¬í”„íŠ¸ ì ìš© í›„] encoder_input_train[0]:\n",
      " ë‹¤ìŒì€ 2ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ë‘ ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \n",
      "\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? #Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. #Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. #Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”. #Person1#: ìŒ, ì‹¬ê°í•œ ì§ˆë³‘ì„ í”¼í•˜ë ¤ë©´ ë¯¸ë¦¬ ë°œê²¬í•˜ëŠ” ê²Œ ì œì¼ ì¢‹ê±°ë“ ìš”. ë³¸ì¸ì„ ìœ„í•´ì„œë¼ë„ ë§¤ë…„ í•œ ...\n",
      "---------- Load data complete ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Make dataset complete ----------\n",
      "\n",
      "ğŸ“Š í† í°í™” ê²°ê³¼ í™•ì¸:\n",
      "   ì²« 50 í† í° ë””ì½”ë”©: ë‹¤ìŒì€ 2ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ë‘ ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \n",
      "\n",
      "#Person1# : ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? ...\n",
      "---------- Make training arguments ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mquriquri7\u001b[0m (\u001b[33mfc_bootcamp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/wandb/run-20251204_172350-ydsutslq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fc_bootcamp/kobart_v1/runs/ydsutslq' target=\"_blank\">baseline_v0</a></strong> to <a href='https://wandb.ai/fc_bootcamp/kobart_v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fc_bootcamp/kobart_v1' target=\"_blank\">https://wandb.ai/fc_bootcamp/kobart_v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fc_bootcamp/kobart_v1/runs/ydsutslq' target=\"_blank\">https://wandb.ai/fc_bootcamp/kobart_v1/runs/ydsutslq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Make training arguments complete ----------\n",
      "---------- Make trainer ----------\n",
      "---------- Make trainer complete ----------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5453' max='15580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5453/15580 28:19 < 52:36, 3.21 it/s, Epoch 7/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.825000</td>\n",
       "      <td>0.566371</td>\n",
       "      <td>0.344045</td>\n",
       "      <td>0.109752</td>\n",
       "      <td>0.325590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.570300</td>\n",
       "      <td>0.525475</td>\n",
       "      <td>0.363801</td>\n",
       "      <td>0.128972</td>\n",
       "      <td>0.340726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.495200</td>\n",
       "      <td>0.510620</td>\n",
       "      <td>0.366163</td>\n",
       "      <td>0.130008</td>\n",
       "      <td>0.344374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.430900</td>\n",
       "      <td>0.509380</td>\n",
       "      <td>0.369940</td>\n",
       "      <td>0.139470</td>\n",
       "      <td>0.345888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.378400</td>\n",
       "      <td>0.517669</td>\n",
       "      <td>0.366900</td>\n",
       "      <td>0.136874</td>\n",
       "      <td>0.344868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.334300</td>\n",
       "      <td>0.528289</td>\n",
       "      <td>0.373542</td>\n",
       "      <td>0.140957</td>\n",
       "      <td>0.351235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.295500</td>\n",
       "      <td>0.543136</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.140288</td>\n",
       "      <td>0.347263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ #Person2# ì—ê²Œ ê°ê¸°ì— ê±¸ë ¸ìœ¼ë©°, ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ë¥¼ ë°©ë¬¸í•˜ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                                            \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ ì•¼ Jimmyì—ê²Œ ìš´ë™í•˜ëŸ¬ ê°€ìê³  ì œì•ˆí•©ë‹ˆë‹¤. ì•¼ëŠ” í† ìš”ì¼ì— ìš´ë™í•  ì˜ˆì •ì…ë‹ˆë‹¤.                                                                             \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ #Person2# ì—ê²Œ ê³¼ì¼ê³¼ ì±„ì†Œ, ë‹­ê³ ê¸°ë¥¼ ë¨¹ìœ¼ë¼ê³  ê¶Œí•©ë‹ˆë‹¤.                                                                                 \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ê°ê¸°ì— ê±¸ë ¸ì§€ë§Œ, #Person1# ì€ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                                      \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ Jimmyì—ê²Œ ìš´ë™ ë‚ ì§œë¥¼ ë³€ê²½í•´ ë‹¬ë¼ê³  ìš”ì²­í•˜ê³ , JimmyëŠ” ê¸ˆìš”ì¼ì— í•˜ê¸°ë¡œ ì•½ì†í•œë‹¤.                                                                     \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ #Person2# ì—ê²Œ ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ìœ¼ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                                             \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ê°ê¸°ì— ê±¸ë ¸ì§€ë§Œ, #Person1# ì€ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                                      \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ Jimmyì—ê²Œ ìš´ë™ ë‚ ì§œë¥¼ ë³€ê²½í•´ ë‹¬ë¼ê³  ìš”ì²­í•˜ê³ , JimmyëŠ” ê¸ˆìš”ì¼ì— í•˜ê¸°ë¡œ ì•½ì†í•œë‹¤.                                                                     \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ #Person2# ì—ê²Œ ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ìœ¼ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                                             \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ìˆ¨ì‰¬ê¸° í˜ë“¤ë‹¤ê³  í˜¸ì†Œí•©ë‹ˆë‹¤. #Person1# ì€ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ì¡°ì–¸í•©ë‹ˆë‹¤.                                                           \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ Jimmyì—ê²Œ ìš´ë™ ì‹œê°„ì„ ë‹¬ë¼ê³  ìš”ì²­í•˜ê³ , JimmyëŠ” ë™ì˜í•œë‹¤.                                                                 \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ì¤„ì´ê³  ì‹¶ì–´ í•œë‹¤. #Person2# ëŠ” ê³¼ì¼ê³¼ ì±„ì†Œë¥¼ ì¢‹ì•„í•œë‹¤ê³  ë§í•œë‹¤.                                                                \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ìˆ¨ì‰¬ê¸° í˜ë“¤ë‹¤ê³  í˜¸ì†Œí•©ë‹ˆë‹¤. #Person1# ì€ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ì¡°ì–¸í•©ë‹ˆë‹¤.                                                           \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ Jimmyì—ê²Œ ìš´ë™ ì‹œê°„ì„ ë‹¬ë¼ê³  ìš”ì²­í•˜ê³ , JimmyëŠ” ë™ì˜í•œë‹¤.                                                                 \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ì¤„ì´ê³  ì‹¶ì–´ í•œë‹¤. #Person2# ëŠ” ê³¼ì¼ê³¼ ì±„ì†Œë¥¼ ì¢‹ì•„í•œë‹¤ê³  ë§í•œë‹¤.                                                                \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ìˆ¨ì‰¬ê¸° í˜ë“¤ë‹¤ê³  í˜¸ì†Œí•˜ë©°, #Person1# ì€ #Person2# ì—ê²Œ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ê¶Œí•©ë‹ˆë‹¤.                                                          \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì´ Jimmyì—ê²Œ ìš´ë™ ì‹œê°„ì„ ë‹¬ë¼ê³  ìš”ì²­í•˜ê³ , JimmyëŠ” ê¸ˆìš”ì¼ì— í•˜ê¸°ë¡œ ì•½ì†í•œë‹¤.                                                                 \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ì¤„ì´ê³  ì‹¶ì–´ í•©ë‹ˆë‹¤. #Person1# ì€ #Person2# ì—ê²Œ ê³¼ì¼, ì±„ì†Œ, ë‹­ê³ ê¸°ë¥¼ êµ¬ì›Œ ë¨¹ìœ¼ë©´ ê±´ê°•ì— ì¢‹ë‹¤ê³  ë§í•©ë‹ˆë‹¤.                                                         \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ìˆ¨ì‰¬ê¸° í˜ë“¤ë‹¤ê³  í˜¸ì†Œí•˜ë©°, #Person1# ì€ #Person2# ì—ê²Œ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ê¶Œí•©ë‹ˆë‹¤.                                                          \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì´ Jimmyì—ê²Œ ìš´ë™ ì‹œê°„ì„ ë‹¬ë¼ê³  ìš”ì²­í•˜ê³ , JimmyëŠ” ê¸ˆìš”ì¼ì— í•˜ê¸°ë¡œ ì•½ì†í•œë‹¤.                                                                 \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ì¤„ì´ê³  ì‹¶ì–´ í•©ë‹ˆë‹¤. #Person1# ì€ #Person2# ì—ê²Œ ê³¼ì¼, ì±„ì†Œ, ë‹­ê³ ê¸°ë¥¼ êµ¬ì›Œ ë¨¹ìœ¼ë©´ ê±´ê°•ì— ì¢‹ë‹¤ê³  ë§í•©ë‹ˆë‹¤.                                                         \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›€ì„ í˜¸ì†Œí•˜ë©°, #Person1# ì€ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                    \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ Jimmyì—ê²Œ ìš´ë™ ë‚ ì§œë¥¼ ì•Œë ¤ì£¼ê³ , JimmyëŠ” í† ìš”ì¼ì— í•˜ê¸°ë¡œ ì•½ì†í•œë‹¤.                                                      \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ì¤„ì´ê³  ì‹¶ì–´ í•œë‹¤. #Person2# ëŠ” #Person1# ì—ê²Œ êµ¬ìš´ ë‹­ê³ ê¸°ë¥¼ ì¶”ì²œí•œë‹¤.                                                      \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›€ì„ í˜¸ì†Œí•˜ë©°, #Person1# ì€ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                    \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ Jimmyì—ê²Œ ìš´ë™ ë‚ ì§œë¥¼ ì•Œë ¤ì£¼ê³ , JimmyëŠ” í† ìš”ì¼ì— í•˜ê¸°ë¡œ ì•½ì†í•œë‹¤.                                                      \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì€ ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ì¤„ì´ê³  ì‹¶ì–´ í•œë‹¤. #Person2# ëŠ” #Person1# ì—ê²Œ êµ¬ìš´ ë‹­ê³ ê¸°ë¥¼ ì¶”ì²œí•œë‹¤.                                                      \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›€ì„ í˜¸ì†Œí•˜ë©°, #Person1# ì€ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                     \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì´ Jimmyì—ê²Œ ìš´ë™ ë‚ ì§œë¥¼ ì•Œë ¤ì£¼ê³ , JimmyëŠ” ë™ì˜í•©ë‹ˆë‹¤.                                                          \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ì¤„ì´ê³  ì‹¶ì–´ í•©ë‹ˆë‹¤. #Person1# ì€ #Person2# ì—ê²Œ ë‹¤ë¥¸ ê²ƒì€ ë¨¹ì§€ ë§ë¼ê³  ê¶Œí•©ë‹ˆë‹¤.                                                      \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›€ì„ í˜¸ì†Œí•˜ë©°, #Person1# ì€ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                     \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person1# ì´ Jimmyì—ê²Œ ìš´ë™ ë‚ ì§œë¥¼ ì•Œë ¤ì£¼ê³ , JimmyëŠ” ë™ì˜í•©ë‹ˆë‹¤.                                                          \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ì¤„ì´ê³  ì‹¶ì–´ í•©ë‹ˆë‹¤. #Person1# ì€ #Person2# ì—ê²Œ ë‹¤ë¥¸ ê²ƒì€ ë¨¹ì§€ ë§ë¼ê³  ê¶Œí•©ë‹ˆë‹¤.                                                      \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›€ì„ í˜¸ì†Œí•˜ë©°, #Person1# ì€ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                                          \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:   JimmyëŠ” ë‹¤ë¦¬ ìš´ë™ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë ¤ í•˜ì§€ë§Œ #Person1# ì€ ì£¼ê°„ ì¼ì •ì— ë³€ë™ì´ ìˆì–´ ê±°ì ˆí•œë‹¤.                                                                         \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ì¤„ì´ê³  ì‹¶ì–´ í•©ë‹ˆë‹¤. #Person1# ì€ #Person2# ì—ê²Œ êµ¬ìš´ ë‹­ê³ ê¸°ë¥¼ ê¶Œí•©ë‹ˆë‹¤.                                                                            \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›€ì„ í˜¸ì†Œí•˜ë©°, #Person1# ì€ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ê¶Œì¥í•©ë‹ˆë‹¤.                                                                          \n",
      "GOLD: #Person2# ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2# ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:   JimmyëŠ” ë‹¤ë¦¬ ìš´ë™ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë ¤ í•˜ì§€ë§Œ #Person1# ì€ ì£¼ê°„ ì¼ì •ì— ë³€ë™ì´ ìˆì–´ ê±°ì ˆí•œë‹¤.                                                                         \n",
      "GOLD: #Person1# ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PRED:  #Person2# ëŠ” ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ìŒì‹ì„ ì¤„ì´ê³  ì‹¶ì–´ í•©ë‹ˆë‹¤. #Person1# ì€ #Person2# ì—ê²Œ êµ¬ìš´ ë‹­ê³ ê¸°ë¥¼ ê¶Œí•©ë‹ˆë‹¤.                                                                            \n",
      "GOLD: #Person1# ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2# ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1# ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d57ff742ad14e2c95ae336c8cea9a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='472.707 MB of 472.707 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>â–ˆâ–ƒâ–â–â–‚â–ƒâ–…</td></tr><tr><td>eval/rouge-1</td><td>â–â–†â–†â–‡â–†â–ˆâ–‡</td></tr><tr><td>eval/rouge-2</td><td>â–â–…â–†â–ˆâ–‡â–ˆâ–ˆ</td></tr><tr><td>eval/rouge-l</td><td>â–â–…â–†â–‡â–†â–ˆâ–‡</td></tr><tr><td>eval/runtime</td><td>â–„â–â–„â–„â–‚â–ƒâ–ˆ</td></tr><tr><td>eval/samples_per_second</td><td>â–…â–ˆâ–…â–…â–‡â–†â–</td></tr><tr><td>eval/steps_per_second</td><td>â–…â–ˆâ–…â–…â–‡â–†â–</td></tr><tr><td>train/epoch</td><td>â–â–â–‚â–‚â–ƒâ–ƒâ–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–‚â–‚â–ƒâ–ƒâ–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/learning_rate</td><td>â–â–ˆâ–ˆâ–ˆâ–‡â–†â–†</td></tr><tr><td>train/loss</td><td>â–ˆâ–‚â–‚â–â–â–â–</td></tr><tr><td>train/total_flos</td><td>â–</td></tr><tr><td>train/train_loss</td><td>â–</td></tr><tr><td>train/train_runtime</td><td>â–</td></tr><tr><td>train/train_samples_per_second</td><td>â–</td></tr><tr><td>train/train_steps_per_second</td><td>â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.54314</td></tr><tr><td>eval/rouge-1</td><td>0.371</td></tr><tr><td>eval/rouge-2</td><td>0.14029</td></tr><tr><td>eval/rouge-l</td><td>0.34726</td></tr><tr><td>eval/runtime</td><td>10.5327</td></tr><tr><td>eval/samples_per_second</td><td>47.376</td></tr><tr><td>eval/steps_per_second</td><td>1.519</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>5453</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>0.2955</td></tr><tr><td>train/total_flos</td><td>4.927423881010176e+16</td></tr><tr><td>train/train_loss</td><td>0.76137</td></tr><tr><td>train/train_runtime</td><td>1697.3706</td></tr><tr><td>train/train_samples_per_second</td><td>146.78</td></tr><tr><td>train/train_steps_per_second</td><td>9.179</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">baseline_v0</strong> at: <a href='https://wandb.ai/fc_bootcamp/kobart_v1/runs/ydsutslq' target=\"_blank\">https://wandb.ai/fc_bootcamp/kobart_v1/runs/ydsutslq</a><br/>Synced 6 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251204_172350-ydsutslq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(loaded_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFtWqowCGzEc"
   },
   "source": [
    "## 4. ëª¨ë¸ ì¶”ë¡ í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ê³³ì— ë‚´ê°€ ì‚¬ìš©í•  wandb config ì„¤ì •\n",
    "loaded_config['inference']['ckt_path'] = \"./prediction_kobart_v1/checkpoint-4674\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFGul3-rSscf"
   },
   "source": [
    "- test dataë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "lV1Do7nlTylG"
   },
   "outputs": [],
   "source": [
    "# tokenization ê³¼ì •ê¹Œì§€ ì§„í–‰ëœ ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ì— ì…ë ¥ë  ë°ì´í„°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "def prepare_test_dataset(config, preprocessor, tokenizer):\n",
    "    \"\"\"\n",
    "    3-2 ê°œì„ : í”„ë¡¬í”„íŠ¸ ì„¤ì •ì´ ì ìš©ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "    \"\"\"\n",
    "    test_file_path = os.path.join(config['general']['data_path'],'test.csv')\n",
    "\n",
    "    test_data = preprocessor.make_set_as_df(test_file_path, is_train=False)\n",
    "    test_id = test_data['fname']\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f'[ì›ë³¸] test_data dialogue:\\n{test_data[\"dialogue\"][0][:200]}...')\n",
    "    print('-'*150)\n",
    "\n",
    "    # 3-2. í”„ë¡¬í”„íŠ¸ ì ìš©ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë³€í™˜\n",
    "    encoder_input_test, decoder_input_test = preprocessor.make_input(\n",
    "        test_data, \n",
    "        is_test=True,\n",
    "        apply_dynamic_prompt=True  # í”„ë¡¬í”„íŠ¸ ì„¤ì • ì‚¬ìš©\n",
    "    )\n",
    "    \n",
    "    # í”„ë¡¬í”„íŠ¸ ì ìš© í™•ì¸\n",
    "    print(f'[í”„ë¡¬í”„íŠ¸ ì ìš© í›„] encoder_input_test[0]:\\n{encoder_input_test[0][:300]}...')\n",
    "    print('-'*10, 'Load data complete', '-'*10)\n",
    "\n",
    "    test_tokenized_encoder_inputs = tokenizer(encoder_input_test, return_tensors=\"pt\", padding=True,\n",
    "                    add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False,)\n",
    "    test_tokenized_decoder_inputs = tokenizer(decoder_input_test, return_tensors=\"pt\", padding=True,\n",
    "                    add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False,)\n",
    "\n",
    "    test_encoder_inputs_dataset = DatasetForInference(test_tokenized_encoder_inputs, test_id, len(encoder_input_test))\n",
    "    print('-'*10, 'Make dataset complete', '-'*10)\n",
    "\n",
    "    return test_data, test_encoder_inputs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "eb49bLULT3aS"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ì¶”ë¡ ì„ ìœ„í•œ tokenizerì™€ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "# 3-1 ê°œì„ : ë™ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ special tokens ì‚¬ìš©\n",
    "# ============================================================================\n",
    "\n",
    "def load_tokenizer_and_model_for_test(config, device, verbose=True):\n",
    "    \"\"\"\n",
    "    ì¶”ë¡ ìš© í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ\n",
    "    3-1: ë™ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ special tokens ë“±ë¡\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print('-' * 10, 'Load tokenizer & model', '-' * 10)\n",
    "\n",
    "    model_name = config['general']['model_name']\n",
    "    ckt_path = config['inference']['ckt_path']\n",
    "    \n",
    "    if verbose:\n",
    "        print('-' * 10, f'Model Name : {model_name}', '-' * 10)\n",
    "        print('-' * 10, f'Checkpoint : {ckt_path}', '-' * 10)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # 3-1. ë™ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ special tokens ë“±ë¡\n",
    "    special_tokens = config['tokenizer']['special_tokens']\n",
    "    special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "    num_added = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nğŸ“Œ ë“±ë¡ëœ Special Tokens ({len(special_tokens)}ê°œ)\")\n",
    "        print(f\"   ìƒˆë¡œ ì¶”ê°€ëœ í† í°: {num_added}ê°œ\")\n",
    "\n",
    "    generate_model = BartForConditionalGeneration.from_pretrained(ckt_path)\n",
    "    generate_model.resize_token_embeddings(len(tokenizer))\n",
    "    generate_model.to(device)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   í† í¬ë‚˜ì´ì € vocab size: {len(tokenizer)}\")\n",
    "        print(f\"   ëª¨ë¸ ì„ë² ë”© ì‚¬ì´ì¦ˆ: {generate_model.config.vocab_size}\")\n",
    "        print('-' * 10, 'Load tokenizer & model complete', '-' * 10)\n",
    "\n",
    "    return generate_model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Axzu9rsoGLgJ"
   },
   "outputs": [],
   "source": [
    "# í•™ìŠµëœ ëª¨ë¸ì´ ìƒì„±í•œ ìš”ì•½ë¬¸ì˜ ì¶œë ¥ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "def inference(config):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ ì¶”ë¡  í•¨ìˆ˜\n",
    "    3-1, 3-2, 4, 5-1, 5-2, 6-1, 6-2 ê°œì„ : \n",
    "    - Special tokens ë™ì  ë“±ë¡\n",
    "    - í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "    - êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì ìš©\n",
    "    - TF-IDF í‚¤ì›Œë“œ í”„ë¡¬í”„íŠ¸\n",
    "    - ë™ì  ì¶”ë¡  ì„¤ì • (ê¸¸ì´ ê¸°ë°˜)\n",
    "    - í™”ì ìˆ˜ ëª…ì‹œ (6-1)\n",
    "    - PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ (6-2)\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print('-' * 10, f'device : {device}', '-' * 10)\n",
    "    print(torch.__version__)\n",
    "\n",
    "    generate_model, tokenizer = load_tokenizer_and_model_for_test(config, device)\n",
    "\n",
    "    # 3-2. í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "    prompt_config = config.get('prompt', {\n",
    "        \"use_prompt\": False,\n",
    "        \"prompt_style\": \"none\",\n",
    "        \"use_turn_separator\": False,\n",
    "        \"turn_separator\": \"[SEP]\",\n",
    "        \"tfidf_top_k\": 5,\n",
    "    })\n",
    "    \n",
    "    # 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •\n",
    "    preprocess_config = config.get('preprocess', {\n",
    "        \"normalize_slang\": False,\n",
    "    })\n",
    "    \n",
    "    # 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš© ì„¤ì • (NEW)\n",
    "    regex_pattern_config = config.get('regex_pattern', {\n",
    "        \"use_speaker_count\": True,       # 6-1. í™”ì ìˆ˜ ëª…ì‹œ\n",
    "        \"use_pii_instruction\": True,     # 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ\n",
    "        \"show_pii_examples\": True,       # ë§ˆìŠ¤í‚¹ ì˜ˆì‹œ í‘œì‹œ\n",
    "    })\n",
    "    \n",
    "    # 5-2. ë™ì  ì¶”ë¡  ì„¤ì • (ê¸¸ì´ ê¸°ë°˜)\n",
    "    dynamic_inference_config = config.get('dynamic_inference', {})\n",
    "    use_dynamic = dynamic_inference_config.get('use_length_based', False) or dynamic_inference_config.get('use_dynamic_settings', False)\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ ì¶”ë¡  ì‹œ í”„ë¡¬í”„íŠ¸ ì„¤ì •:\")\n",
    "    print(f\"   - í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {prompt_config.get('use_prompt', False)}\")\n",
    "    print(f\"   - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: {prompt_config.get('prompt_style', 'none')}\")\n",
    "    if prompt_config.get('prompt_style') in ['keyword', 'keyword_aware']:\n",
    "        print(f\"   - TF-IDF í‚¤ì›Œë“œ ìˆ˜: {prompt_config.get('tfidf_top_k', 5)}ê°œ\")\n",
    "    \n",
    "    print(f\"\\nğŸ—£ï¸ ì¶”ë¡  ì‹œ êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •:\")\n",
    "    print(f\"   - ìŠ¬ë­ ì •ê·œí™”: {preprocess_config.get('normalize_slang', False)}\")\n",
    "    \n",
    "    print(f\"\\nğŸ” 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš©:\")\n",
    "    print(f\"   - 6-1. í™”ì ìˆ˜ ëª…ì‹œ: {regex_pattern_config.get('use_speaker_count', True)}\")\n",
    "    print(f\"   - 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: {regex_pattern_config.get('use_pii_instruction', True)}\")\n",
    "    \n",
    "    print(f\"\\nâš™ï¸ ë™ì  ì¶”ë¡  ì„¤ì •:\")\n",
    "    print(f\"   - ë™ì  ì„¤ì • ì‚¬ìš©: {use_dynamic}\")\n",
    "    if use_dynamic:\n",
    "        print(f\"   - ê¸¸ì´ ê¸°ë°˜: {dynamic_inference_config.get('use_length_based', False)}\")\n",
    "        print(f\"   - ì£¼ì œ ê¸°ë°˜: {dynamic_inference_config.get('use_topic_based', False)}\")\n",
    "    \n",
    "    # 5-1. TF-IDF extractor ì‚¬ìš© ì—¬ë¶€ í™•ì¸\n",
    "    use_tfidf = prompt_config.get('prompt_style') in ['keyword', 'keyword_aware']\n",
    "    tfidf_ext = tfidf_extractor if use_tfidf else None\n",
    "    \n",
    "    if use_tfidf:\n",
    "        print(f\"\\nğŸ“Š TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ í™œì„±í™”\")\n",
    "    \n",
    "    preprocessor = Preprocess(\n",
    "        config['tokenizer']['bos_token'], \n",
    "        config['tokenizer']['eos_token'],\n",
    "        prompt_config=prompt_config,               # 3-2. í”„ë¡¬í”„íŠ¸ ì„¤ì • ì „ë‹¬\n",
    "        preprocess_config=preprocess_config,       # 4. êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì • ì „ë‹¬\n",
    "        tfidf_extractor=tfidf_ext,                 # 5-1. TF-IDF extractor ì „ë‹¬\n",
    "        regex_pattern_config=regex_pattern_config  # 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ ì„¤ì • ì „ë‹¬\n",
    "    )\n",
    "\n",
    "    data_path = config['general']['data_path']\n",
    "    test_data, test_encoder_inputs_dataset = prepare_test_dataset(config, preprocessor, tokenizer)\n",
    "    \n",
    "    # 5-2. ë™ì  ì¶”ë¡ ì„ ìœ„í•´ ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´ ì •ë³´ í•„ìš”\n",
    "    dialogue_lengths = []\n",
    "    if use_dynamic:\n",
    "        # test_dataì—ì„œ ëŒ€í™”ë¬¸ ê¸¸ì´ ê³„ì‚°\n",
    "        for idx in range(len(test_data)):\n",
    "            dialogue = test_data.iloc[idx]['dialogue']\n",
    "            dialogue_lengths.append(len(dialogue))\n",
    "        print(f\"\\nğŸ“ ëŒ€í™”ë¬¸ ê¸¸ì´ ë¶„í¬: min={min(dialogue_lengths)}, max={max(dialogue_lengths)}, avg={sum(dialogue_lengths)/len(dialogue_lengths):.0f}\")\n",
    "    \n",
    "    dataloader = DataLoader(test_encoder_inputs_dataset, batch_size=config['inference']['batch_size'])\n",
    "\n",
    "    summary = []\n",
    "    text_ids = []\n",
    "    \n",
    "    # 5-2. ë™ì  ì¶”ë¡  ì‹œ ë°°ì¹˜ ë‹¨ìœ„ê°€ ì•„ë‹Œ ê°œë³„ ìƒ˜í”Œ ì²˜ë¦¬ í•„ìš”\n",
    "    if use_dynamic:\n",
    "        print(\"\\nğŸš€ ë™ì  ì¶”ë¡  ëª¨ë“œë¡œ ìƒì„± ì‹œì‘...\")\n",
    "        sample_idx = 0\n",
    "        use_topic = dynamic_inference_config.get('use_topic_based', True)\n",
    "        use_length = dynamic_inference_config.get('use_length_based', True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for item in tqdm(dataloader):\n",
    "                batch_size = len(item['ID'])\n",
    "                text_ids.extend(item['ID'])\n",
    "                \n",
    "                for i in range(batch_size):\n",
    "                    # ê°œë³„ ìƒ˜í”Œì— ëŒ€í•œ ë™ì  ì„¤ì • ê³„ì‚° (ì›ë³¸ ëŒ€í™”ë¬¸ ì‚¬ìš©)\n",
    "                    dialogue_text = test_data.iloc[sample_idx]['dialogue']\n",
    "                    dynamic_cfg = get_dynamic_inference_config(dialogue_text, use_topic=use_topic, use_length=use_length)\n",
    "                    \n",
    "                    # ë‹¨ì¼ ìƒ˜í”Œ ìƒì„±\n",
    "                    single_input_ids = item['input_ids'][i:i+1].to(device)\n",
    "                    generated_ids = generate_model.generate(\n",
    "                        input_ids=single_input_ids,\n",
    "                        no_repeat_ngram_size=dynamic_cfg['no_repeat_ngram_size'],\n",
    "                        early_stopping=config['inference']['early_stopping'],\n",
    "                        max_length=dynamic_cfg['max_new_tokens'],\n",
    "                        num_beams=dynamic_cfg['num_beams'],\n",
    "                        length_penalty=dynamic_cfg['length_penalty'],\n",
    "                    )\n",
    "                    \n",
    "                    # 3-1 ê°œì„ : í™”ì íƒœê·¸ ë³´ì¡´í•˜ë©´ì„œ ë””ì½”ë”©\n",
    "                    result = decode_with_speaker_tags(tokenizer, generated_ids[0])\n",
    "                    summary.append(result)\n",
    "                    sample_idx += 1\n",
    "    else:\n",
    "        # ê¸°ì¡´ ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹\n",
    "        with torch.no_grad():\n",
    "            for item in tqdm(dataloader):\n",
    "                text_ids.extend(item['ID'])\n",
    "                generated_ids = generate_model.generate(\n",
    "                    input_ids=item['input_ids'].to(device),\n",
    "                    no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],\n",
    "                    early_stopping=config['inference']['early_stopping'],\n",
    "                    max_length=config['inference']['generate_max_length'],\n",
    "                    num_beams=config['inference']['num_beams'],\n",
    "                )\n",
    "                for ids in generated_ids:\n",
    "                    # 3-1 ê°œì„ : í™”ì íƒœê·¸ ë³´ì¡´í•˜ë©´ì„œ ë””ì½”ë”©\n",
    "                    result = decode_with_speaker_tags(tokenizer, ids)\n",
    "                    summary.append(result)\n",
    "\n",
    "    # í›„ì²˜ë¦¬ ì ìš© (ë„ì–´ì“°ê¸° êµì •)\n",
    "    preprocessed_summary = [postprocess_summary(s) for s in summary]\n",
    "\n",
    "    output = pd.DataFrame(\n",
    "        {\n",
    "            \"fname\": test_data['fname'],\n",
    "            \"summary\": preprocessed_summary,\n",
    "        }\n",
    "    )\n",
    "    result_path = config['inference']['result_path']\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    output.to_csv(os.path.join(result_path, \"output.csv\"), index=False)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "-pJ1ZXf-5V50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- device : cuda:0 ----------\n",
      "2.1.0\n",
      "---------- Load tokenizer & model ----------\n",
      "---------- Model Name : digit82/kobart-summarization ----------\n",
      "---------- Checkpoint : ./prediction_kobart_v1/checkpoint-4674 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Œ ë“±ë¡ëœ Special Tokens (22ê°œ)\n",
      "   ìƒˆë¡œ ì¶”ê°€ëœ í† í°: 22ê°œ\n",
      "   í† í¬ë‚˜ì´ì € vocab size: 30022\n",
      "   ëª¨ë¸ ì„ë² ë”© ì‚¬ì´ì¦ˆ: 30022\n",
      "---------- Load tokenizer & model complete ----------\n",
      "\n",
      "ğŸ“‹ ì¶”ë¡  ì‹œ í”„ë¡¬í”„íŠ¸ ì„¤ì •:\n",
      "   - í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: True\n",
      "   - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: balanced\n",
      "\n",
      "ğŸ—£ï¸ ì¶”ë¡  ì‹œ êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •:\n",
      "   - ìŠ¬ë­ ì •ê·œí™”: True\n",
      "\n",
      "ğŸ” 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš©:\n",
      "   - 6-1. í™”ì ìˆ˜ ëª…ì‹œ: True\n",
      "   - 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: True\n",
      "\n",
      "âš™ï¸ ë™ì  ì¶”ë¡  ì„¤ì •:\n",
      "   - ë™ì  ì„¤ì • ì‚¬ìš©: True\n",
      "   - ê¸¸ì´ ê¸°ë°˜: True\n",
      "   - ì£¼ì œ ê¸°ë°˜: True\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[ì›ë³¸] test_data dialogue:\n",
      "#Person1#: Ms. Dawson, ë°›ì•„ì“°ê¸° ì¢€ ë¶€íƒë“œë ¤ì•¼ê² ì–´ìš”. \n",
      "#Person2#: ë„¤, ë§ì”€í•˜ì„¸ìš”... \n",
      "#Person1#: ì´ê±¸ ì˜¤ëŠ˜ ì˜¤í›„ê¹Œì§€ ëª¨ë“  ì§ì›ë“¤ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¡œ ë³´ë‚´ì•¼ í•´ìš”. ì¤€ë¹„ëë‚˜ìš”? \n",
      "#Person2#: ë„¤, ë§ì”€í•˜ì„¸ìš”. \n",
      "#Person1#: ëª¨ë“  ì§ì›ì—ê²Œ ì•Œë¦½ë‹ˆë‹¤... ì¦‰ì‹œ ë°œíš¨ë˜ì–´ ëª¨ë“  ì‚¬ë‚´ í†µì‹ ì€ ì´ë©”ì¼ê³¼ ê³µì‹ ë©”ëª¨ë¡œë§Œ ì œ...\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[í”„ë¡¬í”„íŠ¸ ì ìš© í›„] encoder_input_test[0]:\n",
      "ë‹¤ìŒì€ 2ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ë‘ ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \n",
      "\n",
      "#Person1#: Ms. Dawson, ë°›ì•„ì“°ê¸° ì¢€ ë¶€íƒë“œë ¤ì•¼ê² ì–´ìš”. #Person2#: ë„¤, ë§ì”€í•˜ì„¸ìš”... #Person1#: ì´ê±¸ ì˜¤ëŠ˜ ì˜¤í›„ê¹Œì§€ ëª¨ë“  ì§ì›ë“¤ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¡œ ë³´ë‚´ì•¼ í•´ìš”. ì¤€ë¹„ëë‚˜ìš”? #Person2#: ë„¤, ë§ì”€í•˜ì„¸ìš”. #Person1#: ëª¨ë“  ì§ì›ì—ê²Œ ì•Œë¦½ë‹ˆë‹¤... ì¦‰ì‹œ ë°œíš¨ë˜ì–´ ëª¨ë“  ì‚¬ë‚´ í†µì‹ ì€ ì´ë©”ì¼ê³¼ ê³µì‹ ë©”ëª¨ë¡œë§Œ ì œí•œë©ë‹ˆë‹¤. ê·¼ë¬´ ì‹œê°„ ë™ì•ˆ ì¦‰ì‹œ ë©”ì‹œì§€ í”„ë¡œê·¸ë¨ ì‚¬ ìš” ì€ ê¸ˆì§€ë©ë‹ˆë‹¤. #Person2#: ì´ ì •...\n",
      "---------- Load data complete ----------\n",
      "---------- Make dataset complete ----------\n",
      "\n",
      "ğŸ“ ëŒ€í™”ë¬¸ ê¸¸ì´ ë¶„í¬: min=111, max=2275, avg=422\n",
      "\n",
      "ğŸš€ ë™ì  ì¶”ë¡  ëª¨ë“œë¡œ ìƒì„± ì‹œì‘...\n",
      "   í† í¬ë‚˜ì´ì € vocab size: 30022\n",
      "   ëª¨ë¸ ì„ë² ë”© ì‚¬ì´ì¦ˆ: 30022\n",
      "---------- Load tokenizer & model complete ----------\n",
      "\n",
      "ğŸ“‹ ì¶”ë¡  ì‹œ í”„ë¡¬í”„íŠ¸ ì„¤ì •:\n",
      "   - í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: True\n",
      "   - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: balanced\n",
      "\n",
      "ğŸ—£ï¸ ì¶”ë¡  ì‹œ êµ¬ì–´ì²´ ì „ì²˜ë¦¬ ì„¤ì •:\n",
      "   - ìŠ¬ë­ ì •ê·œí™”: True\n",
      "\n",
      "ğŸ” 6. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™œìš©:\n",
      "   - 6-1. í™”ì ìˆ˜ ëª…ì‹œ: True\n",
      "   - 6-2. PII ë§ˆìŠ¤í‚¹ ìœ ì§€ ì§€ì‹œ: True\n",
      "\n",
      "âš™ï¸ ë™ì  ì¶”ë¡  ì„¤ì •:\n",
      "   - ë™ì  ì„¤ì • ì‚¬ìš©: True\n",
      "   - ê¸¸ì´ ê¸°ë°˜: True\n",
      "   - ì£¼ì œ ê¸°ë°˜: True\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[ì›ë³¸] test_data dialogue:\n",
      "#Person1#: Ms. Dawson, ë°›ì•„ì“°ê¸° ì¢€ ë¶€íƒë“œë ¤ì•¼ê² ì–´ìš”. \n",
      "#Person2#: ë„¤, ë§ì”€í•˜ì„¸ìš”... \n",
      "#Person1#: ì´ê±¸ ì˜¤ëŠ˜ ì˜¤í›„ê¹Œì§€ ëª¨ë“  ì§ì›ë“¤ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¡œ ë³´ë‚´ì•¼ í•´ìš”. ì¤€ë¹„ëë‚˜ìš”? \n",
      "#Person2#: ë„¤, ë§ì”€í•˜ì„¸ìš”. \n",
      "#Person1#: ëª¨ë“  ì§ì›ì—ê²Œ ì•Œë¦½ë‹ˆë‹¤... ì¦‰ì‹œ ë°œíš¨ë˜ì–´ ëª¨ë“  ì‚¬ë‚´ í†µì‹ ì€ ì´ë©”ì¼ê³¼ ê³µì‹ ë©”ëª¨ë¡œë§Œ ì œ...\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[í”„ë¡¬í”„íŠ¸ ì ìš© í›„] encoder_input_test[0]:\n",
      "ë‹¤ìŒì€ 2ëª…ì´ ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤. ë‘ ì‚¬ëŒì˜ ì…ì¥ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•´ì„œ ìš”ì•½í•´ ì£¼ì„¸ìš”. \n",
      "\n",
      "#Person1#: Ms. Dawson, ë°›ì•„ì“°ê¸° ì¢€ ë¶€íƒë“œë ¤ì•¼ê² ì–´ìš”. #Person2#: ë„¤, ë§ì”€í•˜ì„¸ìš”... #Person1#: ì´ê±¸ ì˜¤ëŠ˜ ì˜¤í›„ê¹Œì§€ ëª¨ë“  ì§ì›ë“¤ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¡œ ë³´ë‚´ì•¼ í•´ìš”. ì¤€ë¹„ëë‚˜ìš”? #Person2#: ë„¤, ë§ì”€í•˜ì„¸ìš”. #Person1#: ëª¨ë“  ì§ì›ì—ê²Œ ì•Œë¦½ë‹ˆë‹¤... ì¦‰ì‹œ ë°œíš¨ë˜ì–´ ëª¨ë“  ì‚¬ë‚´ í†µì‹ ì€ ì´ë©”ì¼ê³¼ ê³µì‹ ë©”ëª¨ë¡œë§Œ ì œí•œë©ë‹ˆë‹¤. ê·¼ë¬´ ì‹œê°„ ë™ì•ˆ ì¦‰ì‹œ ë©”ì‹œì§€ í”„ë¡œê·¸ë¨ ì‚¬ ìš” ì€ ê¸ˆì§€ë©ë‹ˆë‹¤. #Person2#: ì´ ì •...\n",
      "---------- Load data complete ----------\n",
      "---------- Make dataset complete ----------\n",
      "\n",
      "ğŸ“ ëŒ€í™”ë¬¸ ê¸¸ì´ ë¶„í¬: min=111, max=2275, avg=422\n",
      "\n",
      "ğŸš€ ë™ì  ì¶”ë¡  ëª¨ë“œë¡œ ìƒì„± ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:24<00:00,  5.29s/it]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµëœ ëª¨ë¸ì˜ testë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "if __name__ == \"__main__\":\n",
    "    output = inference(loaded_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "OsPmLfhbzZqS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>#Person1# ì€ Ms. Dawsonì—ê²Œ ì´ë©”ì¼ê³¼ ê³µì‹ ë©”ëª¨ë¡œë§Œ ì œí•œë˜ëŠ” ì‚¬ë‚´ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>#Person1# ê³¼ #Person2# ëŠ” êµí†µì²´ì¦ìœ¼ë¡œ ì¸í•´ ì¶œí‡´ê·¼ì— ì–´ë ¤ì›€ì„ ê²ªê³ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>#Person1# ì€ Kateì—ê²Œ Mashaì™€ Heroê°€ ë‘ ë‹¬ ë™ì•ˆ ë³„ê±°í•˜ë‹¤ê°€ ê²°...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>#Person1# ì€ Brianì—ê²Œ ìƒì¼ íŒŒí‹°ì—ì„œ í•¨ê»˜ ì¶¤ì„ ì¶”ìê³  ì´ˆëŒ€í•©ë‹ˆë‹¤.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>#Person1# ê³¼ #Person2# ëŠ” ì˜¬ë¦¼í”½ ê³µì›ì˜ í¬ê¸°ì™€ ì‹œì„¤ì— ëŒ€í•´ ì´ì•¼ê¸°...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>test_495</td>\n",
       "      <td>Jackì€ Charlieì—ê²Œ í•™êµ í›„ ì§‘ì—ì„œ ê·¸ë…€ì™€ ë¹„ë””ì˜¤ ê²Œì„ì„ í•˜ìê³  ì œì•ˆí•˜ê³ ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>test_496</td>\n",
       "      <td>#Person2# ëŠ” ì‹œê³¨ ìŒì•…ì— ê´€ì‹¬ì„ ê°€ì§€ê²Œ ëœ ê³„ê¸°ì™€ ë¼ë””ì˜¤ ë°©ì†¡êµ­ì—ì„œ ì¼í•˜ê²Œ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>test_497</td>\n",
       "      <td>AliceëŠ” #Person1# ì—ê²Œ ì„¸íƒê¸°ì— ë¹„ëˆ„ê°€ ë“¤ì–´ ìˆì§€ ì•Šì•„ ë”°ë¡œ ë„£ì–´ì•¼ í•œ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>test_498</td>\n",
       "      <td>MatthewëŠ” ì„ëŒ€ ê³„ì•½ì„ ê°±ì‹ í•˜ê³  ì‹¶ì§€ ì•Šì•„ ì§‘ì„ ì°¾ê³  ìˆìœ¼ë©°, Mrs. Tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>test_499</td>\n",
       "      <td>FrankëŠ” Betsyì—ê²Œ ìŠ¹ì§„ íŒŒí‹°ì— ì°¸ì„í•  150ëª…ì„ ì´ˆëŒ€í–ˆë‹¤ê³  ì•Œë¦½ë‹ˆë‹¤. íŒŒí‹°...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        fname                                            summary\n",
       "0      test_0  #Person1# ì€ Ms. Dawsonì—ê²Œ ì´ë©”ì¼ê³¼ ê³µì‹ ë©”ëª¨ë¡œë§Œ ì œí•œë˜ëŠ” ì‚¬ë‚´ ...\n",
       "1      test_1  #Person1# ê³¼ #Person2# ëŠ” êµí†µì²´ì¦ìœ¼ë¡œ ì¸í•´ ì¶œí‡´ê·¼ì— ì–´ë ¤ì›€ì„ ê²ªê³ ...\n",
       "2      test_2  #Person1# ì€ Kateì—ê²Œ Mashaì™€ Heroê°€ ë‘ ë‹¬ ë™ì•ˆ ë³„ê±°í•˜ë‹¤ê°€ ê²°...\n",
       "3      test_3       #Person1# ì€ Brianì—ê²Œ ìƒì¼ íŒŒí‹°ì—ì„œ í•¨ê»˜ ì¶¤ì„ ì¶”ìê³  ì´ˆëŒ€í•©ë‹ˆë‹¤.\n",
       "4      test_4  #Person1# ê³¼ #Person2# ëŠ” ì˜¬ë¦¼í”½ ê³µì›ì˜ í¬ê¸°ì™€ ì‹œì„¤ì— ëŒ€í•´ ì´ì•¼ê¸°...\n",
       "..        ...                                                ...\n",
       "494  test_495  Jackì€ Charlieì—ê²Œ í•™êµ í›„ ì§‘ì—ì„œ ê·¸ë…€ì™€ ë¹„ë””ì˜¤ ê²Œì„ì„ í•˜ìê³  ì œì•ˆí•˜ê³ ,...\n",
       "495  test_496  #Person2# ëŠ” ì‹œê³¨ ìŒì•…ì— ê´€ì‹¬ì„ ê°€ì§€ê²Œ ëœ ê³„ê¸°ì™€ ë¼ë””ì˜¤ ë°©ì†¡êµ­ì—ì„œ ì¼í•˜ê²Œ...\n",
       "496  test_497  AliceëŠ” #Person1# ì—ê²Œ ì„¸íƒê¸°ì— ë¹„ëˆ„ê°€ ë“¤ì–´ ìˆì§€ ì•Šì•„ ë”°ë¡œ ë„£ì–´ì•¼ í•œ...\n",
       "497  test_498  MatthewëŠ” ì„ëŒ€ ê³„ì•½ì„ ê°±ì‹ í•˜ê³  ì‹¶ì§€ ì•Šì•„ ì§‘ì„ ì°¾ê³  ìˆìœ¼ë©°, Mrs. Tho...\n",
       "498  test_499  FrankëŠ” Betsyì—ê²Œ ìŠ¹ì§„ íŒŒí‹°ì— ì°¸ì„í•  150ëª…ì„ ì´ˆëŒ€í–ˆë‹¤ê³  ì•Œë¦½ë‹ˆë‹¤. íŒŒí‹°...\n",
       "\n",
       "[499 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output  # ê° ëŒ€í™”ë¬¸ì— ëŒ€í•œ ìš”ì•½ë¬¸ì´ ì¶œë ¥ë¨ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "083ea69907bb48d4a8fff919bac51aad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08d05bc20a96432badd459e1ffaf868e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "13651c09564a4337b8274c1cb436faa5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14f6c91d6c634379b498586c51e606e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21d2e54b5a0a4f79973a512105da43eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2307c6dcbe0141acb5e61baae19cade7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "285007b45236478ca147c6df752c8da4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a190bda0b72407e9a953cd2104dd3b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2fd3d7bbcd6948d8904d33001f95ea03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3645438ace1f4596a8dbc157b48c1521": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14f6c91d6c634379b498586c51e606e0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_08d05bc20a96432badd459e1ffaf868e",
      "value": " 295/295 [00:00&lt;00:00, 21.3kB/s]"
     }
    },
    "3a04e871b74b45d7bf02fd33bb103577": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3bcd6b6b956347b29e1efa20a1d00542": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c284a826f6843f6aa47eacad478ac30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_affff1d8a89e4c14955d1b2aa39ff1ab",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_13651c09564a4337b8274c1cb436faa5",
      "value": "tokenizer.json: 100%"
     }
    },
    "45187decb58b4ad39ad532259c6277e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4747b668e2fa4ab58a449446f80030f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "52095cc7087243ac916055e569fd22f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c18f0e3bc35e44d9915c3f84cd282a26",
      "max": 109,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3a04e871b74b45d7bf02fd33bb103577",
      "value": 109
     }
    },
    "58001a60eacc44d5b38a68648adccde4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58c794fb7ce543a39fdf66d757f6eeab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f5fde5b0ac840a18bd5cc380e564ff6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_45187decb58b4ad39ad532259c6277e5",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "5dfcf310ca9e4e2794076098a5d69cea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3c284a826f6843f6aa47eacad478ac30",
       "IPY_MODEL_6caedd60c6b747469c82930be1f95d6d",
       "IPY_MODEL_64f2218f899d446393cfea44f206f0a6"
      ],
      "layout": "IPY_MODEL_d068f541df3f438dbd5138863e64b2f2"
     }
    },
    "64f2218f899d446393cfea44f206f0a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d22fbc2c5dbf422399e496c9b500025a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_775d8bbeceac4e2da4f21ab6235c89ed",
      "value": " 682k/682k [00:00&lt;00:00, 5.40MB/s]"
     }
    },
    "6caedd60c6b747469c82930be1f95d6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3bcd6b6b956347b29e1efa20a1d00542",
      "max": 682133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2fd3d7bbcd6948d8904d33001f95ea03",
      "value": 682133
     }
    },
    "6f5fde5b0ac840a18bd5cc380e564ff6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "775d8bbeceac4e2da4f21ab6235c89ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8a6464a355f7464c989033965d418a8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2307c6dcbe0141acb5e61baae19cade7",
      "max": 295,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4747b668e2fa4ab58a449446f80030f5",
      "value": 295
     }
    },
    "a15af9e8158f4903b9189f3d322a5ef3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac00d6c2cf974b33a628acb3f1471316",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_285007b45236478ca147c6df752c8da4",
      "value": " 109/109 [00:00&lt;00:00, 9.44kB/s]"
     }
    },
    "ac00d6c2cf974b33a628acb3f1471316": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "affff1d8a89e4c14955d1b2aa39ff1ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c18f0e3bc35e44d9915c3f84cd282a26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d068f541df3f438dbd5138863e64b2f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d22fbc2c5dbf422399e496c9b500025a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de1a3f7701c243839fe03b930a9b9e30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ebc22683058a4f229c5588e52fc93536",
       "IPY_MODEL_52095cc7087243ac916055e569fd22f3",
       "IPY_MODEL_a15af9e8158f4903b9189f3d322a5ef3"
      ],
      "layout": "IPY_MODEL_21d2e54b5a0a4f79973a512105da43eb"
     }
    },
    "e920dbc173c045d1a32143349f1dff8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_58c794fb7ce543a39fdf66d757f6eeab",
       "IPY_MODEL_8a6464a355f7464c989033965d418a8a",
       "IPY_MODEL_3645438ace1f4596a8dbc157b48c1521"
      ],
      "layout": "IPY_MODEL_58001a60eacc44d5b38a68648adccde4"
     }
    },
    "ebc22683058a4f229c5588e52fc93536": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_083ea69907bb48d4a8fff919bac51aad",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2a190bda0b72407e9a953cd2104dd3b2",
      "value": "special_tokens_map.json: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
