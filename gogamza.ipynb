{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ac4a47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¥ï¸ Device: cuda:0\n",
      "âœ… Configuration for gogamza/kobart-base-v2 loaded\n",
      "   ëª¨ë¸: gogamza/kobart-base-v2\n",
      "   ì¶œë ¥: ./results_kobart_v2\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸš€ gogamza/kobart-base-v2 ëª¨ë¸ í•™ìŠµ\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from rouge import Rouge\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BartForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "# Seed ê³ ì •\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ–¥ï¸ Device: {device}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration for gogamza/kobart-base-v2\n",
    "# ============================================================================\n",
    "CONF_V2 = {\n",
    "    \"general\": {\n",
    "        \"data_path\": \"./data/\",\n",
    "        \"model_name\": \"gogamza/kobart-base-v2\",  # ğŸ†• ìƒˆ ëª¨ë¸\n",
    "        \"output_dir\": \"./results_kobart_v2\",\n",
    "        \"seed\": 42\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "        \"encoder_max_len\": 512,\n",
    "        \"decoder_max_len\": 150,\n",
    "        \"special_tokens\": [\n",
    "            '#Person1#', '#Person2#', '#Person3#', '#Person4#', \n",
    "            '#Person5#', '#Person6#', '#Person7#', \n",
    "            '#PhoneNumber#', '#Address#', '#PassportNumber#',\n",
    "        ]\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"num_train_epochs\": 20,\n",
    "        \"learning_rate\": 3e-5,\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"per_device_eval_batch_size\": 32,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lr_scheduler_type\": 'cosine_with_restarts',\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"gradient_accumulation_steps\": 2,\n",
    "        \"evaluation_strategy\": 'epoch',\n",
    "        \"save_strategy\": 'epoch',\n",
    "        \"save_total_limit\": 3,\n",
    "        \"fp16\": True,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"metric_for_best_model\": \"combined_score\",\n",
    "        \"greater_is_better\": True,\n",
    "        \"logging_dir\": \"./logs_kobart_v2\",\n",
    "        \"logging_steps\": 50,\n",
    "        \"predict_with_generate\": True,\n",
    "        \"generation_max_length\": 150,\n",
    "        \"early_stopping_patience\": 5,\n",
    "        \"report_to\": \"none\",\n",
    "        \"label_smoothing_factor\": 0.1,\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"result_path\": \"./prediction_kobart_v2/\",\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"early_stopping\": True,\n",
    "        \"generate_max_length\": 150,\n",
    "        \"num_beams\": 6,\n",
    "        \"length_penalty\": 1.0,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"batch_size\": 32,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"âœ… Configuration for gogamza/kobart-base-v2 loaded\")\n",
    "print(f\"   ëª¨ë¸: {CONF_V2['general']['model_name']}\")\n",
    "print(f\"   ì¶œë ¥: {CONF_V2['general']['output_dir']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00a0a213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> í† í¬ë‚˜ì´ì € ë¡œë“œ: gogamza/kobart-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> ì¶”ê°€ëœ íŠ¹ìˆ˜ í† í°: 10ê°œ\n",
      ">>> Vocab Size: 30010\n",
      ">>> BOS: </s>, EOS: </s>\n",
      "âœ… Dataset í´ë˜ìŠ¤ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# í† í¬ë‚˜ì´ì € ë° ë°ì´í„°ì…‹ ì¤€ë¹„ (kobart-base-v2)\n",
    "# ============================================================================\n",
    "\n",
    "# Tokenizer ë¡œë“œ\n",
    "print(\">>> í† í¬ë‚˜ì´ì € ë¡œë“œ: gogamza/kobart-base-v2\")\n",
    "tokenizer_v2 = AutoTokenizer.from_pretrained(CONF_V2['general']['model_name'])\n",
    "\n",
    "# íŠ¹ìˆ˜ í† í° ì¶”ê°€\n",
    "special_tokens_dict = {'additional_special_tokens': CONF_V2['tokenizer']['special_tokens']}\n",
    "num_added = tokenizer_v2.add_special_tokens(special_tokens_dict)\n",
    "print(f\">>> ì¶”ê°€ëœ íŠ¹ìˆ˜ í† í°: {num_added}ê°œ\")\n",
    "print(f\">>> Vocab Size: {len(tokenizer_v2)}\")\n",
    "\n",
    "# BOS/EOS í† í° ì„¤ì •\n",
    "CONF_V2['tokenizer']['bos_token'] = tokenizer_v2.bos_token\n",
    "CONF_V2['tokenizer']['eos_token'] = tokenizer_v2.eos_token\n",
    "print(f\">>> BOS: {tokenizer_v2.bos_token}, EOS: {tokenizer_v2.eos_token}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Dataset í´ë˜ìŠ¤\n",
    "# ============================================================================\n",
    "class KoBARTv2Dataset(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, labels=None, is_test=False, ids=None):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.labels = labels\n",
    "        self.is_test = is_test\n",
    "        self.ids = ids\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
    "        \n",
    "        if 'input_ids' in self.decoder_input:\n",
    "            item['decoder_input_ids'] = self.decoder_input['input_ids'][idx].clone().detach()\n",
    "            item['decoder_attention_mask'] = self.decoder_input['attention_mask'][idx].clone().detach()\n",
    "\n",
    "        if not self.is_test and self.labels is not None:\n",
    "            labels = self.labels['input_ids'][idx].clone().detach()\n",
    "            labels[labels == tokenizer_v2.pad_token_id] = -100\n",
    "            item['labels'] = labels\n",
    "            \n",
    "        if self.ids is not None:\n",
    "            item['ID'] = self.ids[idx]\n",
    "            \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoder_input['input_ids'])\n",
    "\n",
    "# ============================================================================\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "# ============================================================================\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    return text.strip()\n",
    "\n",
    "def prepare_data_v2(conf, tokenizer, is_train=True):\n",
    "    \"\"\"ë°ì´í„° ì¤€ë¹„\"\"\"\n",
    "    data_path = conf['general']['data_path']\n",
    "    bos = conf['tokenizer']['bos_token']\n",
    "    eos = conf['tokenizer']['eos_token']\n",
    "    \n",
    "    if is_train:\n",
    "        train_df = pd.read_csv(os.path.join(data_path, 'train.csv'))\n",
    "        val_df = pd.read_csv(os.path.join(data_path, 'dev.csv'))\n",
    "        \n",
    "        # ì „ì²˜ë¦¬\n",
    "        train_df['dialogue'] = train_df['dialogue'].apply(clean_text)\n",
    "        train_df['summary'] = train_df['summary'].apply(clean_text)\n",
    "        val_df['dialogue'] = val_df['dialogue'].apply(clean_text)\n",
    "        val_df['summary'] = val_df['summary'].apply(clean_text)\n",
    "        \n",
    "        # Train í† í¬ë‚˜ì´ì¦ˆ\n",
    "        enc_train = tokenizer(\n",
    "            train_df['dialogue'].tolist(), return_tensors=\"pt\", \n",
    "            padding=True, truncation=True, max_length=conf['tokenizer']['encoder_max_len']\n",
    "        )\n",
    "        dec_in_train = tokenizer(\n",
    "            [bos + s for s in train_df['summary'].tolist()], return_tensors=\"pt\",\n",
    "            padding=True, truncation=True, max_length=conf['tokenizer']['decoder_max_len']\n",
    "        )\n",
    "        dec_out_train = tokenizer(\n",
    "            [s + eos for s in train_df['summary'].tolist()], return_tensors=\"pt\",\n",
    "            padding=True, truncation=True, max_length=conf['tokenizer']['decoder_max_len']\n",
    "        )\n",
    "        \n",
    "        train_dataset = KoBARTv2Dataset(enc_train, dec_in_train, dec_out_train)\n",
    "        \n",
    "        # Val í† í¬ë‚˜ì´ì¦ˆ\n",
    "        enc_val = tokenizer(\n",
    "            val_df['dialogue'].tolist(), return_tensors=\"pt\",\n",
    "            padding=True, truncation=True, max_length=conf['tokenizer']['encoder_max_len']\n",
    "        )\n",
    "        dec_in_val = tokenizer(\n",
    "            [bos + s for s in val_df['summary'].tolist()], return_tensors=\"pt\",\n",
    "            padding=True, truncation=True, max_length=conf['tokenizer']['decoder_max_len']\n",
    "        )\n",
    "        dec_out_val = tokenizer(\n",
    "            [s + eos for s in val_df['summary'].tolist()], return_tensors=\"pt\",\n",
    "            padding=True, truncation=True, max_length=conf['tokenizer']['decoder_max_len']\n",
    "        )\n",
    "        \n",
    "        val_dataset = KoBARTv2Dataset(enc_val, dec_in_val, dec_out_val)\n",
    "        \n",
    "        print(f\"âœ… Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
    "        return train_dataset, val_dataset\n",
    "    \n",
    "    else:\n",
    "        test_df = pd.read_csv(os.path.join(data_path, 'test.csv'))\n",
    "        test_df['dialogue'] = test_df['dialogue'].apply(clean_text)\n",
    "        \n",
    "        enc_test = tokenizer(\n",
    "            test_df['dialogue'].tolist(), return_tensors=\"pt\",\n",
    "            padding=True, truncation=True, max_length=conf['tokenizer']['encoder_max_len']\n",
    "        )\n",
    "        dec_in_test = tokenizer(\n",
    "            [bos] * len(test_df), return_tensors=\"pt\",\n",
    "            padding=True, truncation=True, max_length=conf['tokenizer']['decoder_max_len']\n",
    "        )\n",
    "        \n",
    "        test_dataset = KoBARTv2Dataset(enc_test, dec_in_test, is_test=True, ids=test_df['fname'].tolist())\n",
    "        return test_df, test_dataset\n",
    "\n",
    "print(\"âœ… Dataset í´ë˜ìŠ¤ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4560b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸš€ gogamza/kobart-base-v2 í•™ìŠµ ì‹œì‘\n",
      "======================================================================\n",
      "âœ… Train: 12457, Val: 499\n",
      ">>> ëª¨ë¸ ë¡œë“œ: gogamza/kobart-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f67a6f4ad51457d8e7be582032db58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/495M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> ëª¨ë¸ íŒŒë¼ë¯¸í„°: 123,867,648\n",
      ">>> Training Start...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4674' max='7780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4674/7780 27:42 < 18:25, 2.81 it/s, Epoch 12/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "      <th>Combined Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.311700</td>\n",
       "      <td>3.183213</td>\n",
       "      <td>0.295384</td>\n",
       "      <td>0.114030</td>\n",
       "      <td>0.279278</td>\n",
       "      <td>0.229564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.855500</td>\n",
       "      <td>2.930261</td>\n",
       "      <td>0.329820</td>\n",
       "      <td>0.138331</td>\n",
       "      <td>0.307872</td>\n",
       "      <td>0.258674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.568300</td>\n",
       "      <td>2.909329</td>\n",
       "      <td>0.328520</td>\n",
       "      <td>0.138182</td>\n",
       "      <td>0.307527</td>\n",
       "      <td>0.258077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.351700</td>\n",
       "      <td>2.944453</td>\n",
       "      <td>0.339337</td>\n",
       "      <td>0.147654</td>\n",
       "      <td>0.315510</td>\n",
       "      <td>0.267500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.175800</td>\n",
       "      <td>3.003534</td>\n",
       "      <td>0.333065</td>\n",
       "      <td>0.142605</td>\n",
       "      <td>0.306932</td>\n",
       "      <td>0.260868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.042500</td>\n",
       "      <td>3.063092</td>\n",
       "      <td>0.334992</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>0.312683</td>\n",
       "      <td>0.263592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.984400</td>\n",
       "      <td>3.086493</td>\n",
       "      <td>0.334813</td>\n",
       "      <td>0.141350</td>\n",
       "      <td>0.310578</td>\n",
       "      <td>0.262247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Training Finished!\n",
      ">>> Best Model: ./results_kobart_v2/best_model\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ¯ gogamza/kobart-base-v2 í•™ìŠµ ì‹¤í–‰\n",
    "# ============================================================================\n",
    "\n",
    "def compute_metrics_v2(eval_pred):\n",
    "    \"\"\"ROUGE ë©”íŠ¸ë¦­ ê³„ì‚°\"\"\"\n",
    "    rouge = Rouge()\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    \n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer_v2.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer_v2.pad_token_id)\n",
    "    \n",
    "    decoded_preds = tokenizer_v2.batch_decode(predictions.tolist(), skip_special_tokens=False)\n",
    "    decoded_labels = tokenizer_v2.batch_decode(labels.tolist(), skip_special_tokens=False)\n",
    "    \n",
    "    # ì‹œìŠ¤í…œ í† í° ì œê±°\n",
    "    remove_tokens = [tokenizer_v2.bos_token, tokenizer_v2.eos_token, tokenizer_v2.pad_token, '<usr>']\n",
    "    \n",
    "    def clean_for_rouge(text_list):\n",
    "        cleaned = []\n",
    "        for text in text_list:\n",
    "            for token in remove_tokens:\n",
    "                if token:\n",
    "                    text = text.replace(token, \"\").strip()\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            cleaned.append(text if text else \"empty\")\n",
    "        return cleaned\n",
    "\n",
    "    decoded_preds = clean_for_rouge(decoded_preds)\n",
    "    decoded_labels = clean_for_rouge(decoded_labels)\n",
    "    \n",
    "    try:\n",
    "        results = rouge.get_scores(decoded_preds, decoded_labels, avg=True)\n",
    "        r1 = results[\"rouge-1\"][\"f\"]\n",
    "        r2 = results[\"rouge-2\"][\"f\"]\n",
    "        rl = results[\"rouge-l\"][\"f\"]\n",
    "        combined_score = (r1 + r2 + rl) / 3\n",
    "        \n",
    "        return {\n",
    "            \"rouge-1\": r1,\n",
    "            \"rouge-2\": r2,\n",
    "            \"rouge-l\": rl,\n",
    "            \"combined_score\": combined_score\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Metrics Error: {e}\")\n",
    "        return {\"combined_score\": 0.0}\n",
    "\n",
    "\n",
    "def train_kobart_v2():\n",
    "    \"\"\"gogamza/kobart-base-v2 í•™ìŠµ\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"ğŸš€ gogamza/kobart-base-v2 í•™ìŠµ ì‹œì‘\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ë°ì´í„° ë¡œë“œ\n",
    "    train_dataset, val_dataset = prepare_data_v2(CONF_V2, tokenizer_v2, is_train=True)\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    print(f\">>> ëª¨ë¸ ë¡œë“œ: {CONF_V2['general']['model_name']}\")\n",
    "    model_v2 = BartForConditionalGeneration.from_pretrained(CONF_V2['general']['model_name'])\n",
    "    model_v2.resize_token_embeddings(len(tokenizer_v2))\n",
    "    model_v2.to(device)\n",
    "    \n",
    "    print(f\">>> ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model_v2.parameters()):,}\")\n",
    "    \n",
    "    # Training Arguments\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=CONF_V2['general']['output_dir'],\n",
    "        overwrite_output_dir=CONF_V2['training']['overwrite_output_dir'],\n",
    "        num_train_epochs=CONF_V2['training']['num_train_epochs'],\n",
    "        learning_rate=CONF_V2['training']['learning_rate'],\n",
    "        per_device_train_batch_size=CONF_V2['training']['per_device_train_batch_size'],\n",
    "        per_device_eval_batch_size=CONF_V2['training']['per_device_eval_batch_size'],\n",
    "        warmup_ratio=CONF_V2['training']['warmup_ratio'],\n",
    "        weight_decay=CONF_V2['training']['weight_decay'],\n",
    "        lr_scheduler_type=CONF_V2['training']['lr_scheduler_type'],\n",
    "        optim=CONF_V2['training']['optim'],\n",
    "        gradient_accumulation_steps=CONF_V2['training']['gradient_accumulation_steps'],\n",
    "        evaluation_strategy=CONF_V2['training']['evaluation_strategy'],\n",
    "        save_strategy=CONF_V2['training']['save_strategy'],\n",
    "        save_total_limit=CONF_V2['training']['save_total_limit'],\n",
    "        fp16=CONF_V2['training']['fp16'],\n",
    "        load_best_model_at_end=CONF_V2['training']['load_best_model_at_end'],\n",
    "        metric_for_best_model=CONF_V2['training']['metric_for_best_model'],\n",
    "        greater_is_better=CONF_V2['training']['greater_is_better'],\n",
    "        logging_dir=CONF_V2['training']['logging_dir'],\n",
    "        logging_steps=CONF_V2['training']['logging_steps'],\n",
    "        predict_with_generate=CONF_V2['training']['predict_with_generate'],\n",
    "        generation_max_length=CONF_V2['training']['generation_max_length'],\n",
    "        report_to=CONF_V2['training']['report_to'],\n",
    "        seed=CONF_V2['general']['seed'],\n",
    "        label_smoothing_factor=CONF_V2['training']['label_smoothing_factor'],\n",
    "    )\n",
    "    \n",
    "    # Early Stopping\n",
    "    early_stopping = EarlyStoppingCallback(\n",
    "        early_stopping_patience=CONF_V2['training']['early_stopping_patience']\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model_v2,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer_v2,\n",
    "        compute_metrics=compute_metrics_v2,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    print(\">>> Training Start...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Best Model ì €ì¥\n",
    "    best_model_path = os.path.join(CONF_V2['general']['output_dir'], \"best_model\")\n",
    "    trainer.save_model(best_model_path)\n",
    "    tokenizer_v2.save_pretrained(best_model_path)\n",
    "    \n",
    "    print(f\"\\nâœ… Training Finished!\")\n",
    "    print(f\">>> Best Model: {best_model_path}\")\n",
    "    \n",
    "    return best_model_path\n",
    "\n",
    "# í•™ìŠµ ì‹¤í–‰\n",
    "best_model_path_v2 = train_kobart_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a23369a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì¶”ë¡  ë° ê·¸ë¦¬ë“œ ì„œì¹˜ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ”® gogamza/kobart-base-v2 ì¶”ë¡  + ê·¸ë¦¬ë“œ ì„œì¹˜\n",
    "# ============================================================================\n",
    "\n",
    "def postprocess_summary_v2(text):\n",
    "    \"\"\"í›„ì²˜ë¦¬\"\"\"\n",
    "    system_tokens = [tokenizer_v2.bos_token, tokenizer_v2.eos_token, tokenizer_v2.pad_token, '<usr>']\n",
    "    for token in system_tokens:\n",
    "        if token:\n",
    "            text = text.replace(token, \"\")\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def inference_kobart_v2(model_path, lp=1.0, nb=6, rp=1.2, save_name=None):\n",
    "    \"\"\"kobart-base-v2 ì¶”ë¡ \"\"\"\n",
    "    print(f\">>> ì¶”ë¡ : LP={lp}, NB={nb}, RP={rp}\")\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "    test_df, test_dataset = prepare_data_v2(CONF_V2, tokenizer_v2, is_train=False)\n",
    "    dataloader = DataLoader(test_dataset, batch_size=CONF_V2['inference']['batch_size'], shuffle=False)\n",
    "    \n",
    "    summary_list = []\n",
    "    fname_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Inference\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=CONF_V2['inference']['generate_max_length'],\n",
    "                num_beams=nb,\n",
    "                length_penalty=lp,\n",
    "                repetition_penalty=rp,\n",
    "                no_repeat_ngram_size=CONF_V2['inference']['no_repeat_ngram_size'],\n",
    "                early_stopping=True,\n",
    "            )\n",
    "            \n",
    "            decoded = tokenizer_v2.batch_decode(outputs, skip_special_tokens=False)\n",
    "            cleaned = [postprocess_summary_v2(text) for text in decoded]\n",
    "            \n",
    "            summary_list.extend(cleaned)\n",
    "            fname_list.extend(batch['ID'])\n",
    "    \n",
    "    # í†µê³„\n",
    "    avg_len = np.mean([len(s) for s in summary_list])\n",
    "    person_count = sum(1 for s in summary_list if '#Person' in s)\n",
    "    print(f\">>> í‰ê·  ê¸¸ì´: {avg_len:.1f}ì, í™”ì í† í°: {person_count}/{len(summary_list)} ({person_count/len(summary_list)*100:.1f}%)\")\n",
    "    \n",
    "    # ì €ì¥\n",
    "    result_path = CONF_V2['inference']['result_path']\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "    \n",
    "    output_df = pd.DataFrame({\n",
    "        \"fname\": fname_list,\n",
    "        \"summary\": summary_list\n",
    "    })\n",
    "    \n",
    "    if save_name:\n",
    "        save_file = os.path.join(result_path, f\"{save_name}.csv\")\n",
    "    else:\n",
    "        save_file = os.path.join(result_path, f\"output_lp{lp}_nb{nb}_rp{rp}.csv\")\n",
    "    \n",
    "    output_df.to_csv(save_file, index=False)\n",
    "    print(f\">>> ì €ì¥: {save_file}\")\n",
    "    \n",
    "    return output_df, avg_len\n",
    "\n",
    "\n",
    "def grid_search_kobart_v2(model_path):\n",
    "    \"\"\"kobart-base-v2 ê·¸ë¦¬ë“œ ì„œì¹˜\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"ğŸ” gogamza/kobart-base-v2 ê·¸ë¦¬ë“œ ì„œì¹˜\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # í•µì‹¬ ì¡°í•© (ìµœê³ ì  48.3168 ê¸°ì¤€ LP=1.0, NB=6, RP=1.2)\n",
    "    configs = [\n",
    "        {\"lp\": 1.0, \"nb\": 6, \"rp\": 1.2, \"name\": \"baseline\"},\n",
    "        {\"lp\": 0.95, \"nb\": 6, \"rp\": 1.2, \"name\": \"lp0.95\"},\n",
    "        {\"lp\": 1.05, \"nb\": 6, \"rp\": 1.2, \"name\": \"lp1.05\"},\n",
    "        {\"lp\": 1.0, \"nb\": 7, \"rp\": 1.2, \"name\": \"nb7\"},\n",
    "        {\"lp\": 1.0, \"nb\": 8, \"rp\": 1.2, \"name\": \"nb8\"},\n",
    "        {\"lp\": 1.0, \"nb\": 6, \"rp\": 1.15, \"name\": \"rp1.15\"},\n",
    "        {\"lp\": 1.0, \"nb\": 6, \"rp\": 1.25, \"name\": \"rp1.25\"},\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for cfg in configs:\n",
    "        _, avg_len = inference_kobart_v2(\n",
    "            model_path, \n",
    "            lp=cfg[\"lp\"], \n",
    "            nb=cfg[\"nb\"], \n",
    "            rp=cfg[\"rp\"],\n",
    "            save_name=f\"kobart_v2_{cfg['name']}\"\n",
    "        )\n",
    "        results.append({\n",
    "            \"name\": cfg[\"name\"],\n",
    "            \"lp\": cfg[\"lp\"],\n",
    "            \"nb\": cfg[\"nb\"],\n",
    "            \"rp\": cfg[\"rp\"],\n",
    "            \"avg_len\": avg_len\n",
    "        })\n",
    "    \n",
    "    # ê²°ê³¼ ì¶œë ¥\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ“Š ê·¸ë¦¬ë“œ ì„œì¹˜ ê²°ê³¼\")\n",
    "    print(\"=\"*70)\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    results_df.to_csv(f\"{CONF_V2['inference']['result_path']}/grid_search_results.csv\", index=False)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "print(\"âœ… ì¶”ë¡  ë° ê·¸ë¦¬ë“œ ì„œì¹˜ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c72a6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ” gogamza/kobart-base-v2 ê·¸ë¦¬ë“œ ì„œì¹˜\n",
      "======================================================================\n",
      ">>> ì¶”ë¡ : LP=1.0, NB=6, RP=1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:38<00:00,  2.39s/it]\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> í‰ê·  ê¸¸ì´: 92.5ì, í™”ì í† í°: 435/499 (87.2%)\n",
      ">>> ì €ì¥: ./prediction_kobart_v2/kobart_v2_baseline.csv\n",
      ">>> ì¶”ë¡ : LP=0.95, NB=6, RP=1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:38<00:00,  2.38s/it]\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> í‰ê·  ê¸¸ì´: 91.0ì, í™”ì í† í°: 435/499 (87.2%)\n",
      ">>> ì €ì¥: ./prediction_kobart_v2/kobart_v2_lp0.95.csv\n",
      ">>> ì¶”ë¡ : LP=1.05, NB=6, RP=1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:38<00:00,  2.38s/it]\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> í‰ê·  ê¸¸ì´: 93.4ì, í™”ì í† í°: 436/499 (87.4%)\n",
      ">>> ì €ì¥: ./prediction_kobart_v2/kobart_v2_lp1.05.csv\n",
      ">>> ì¶”ë¡ : LP=1.0, NB=7, RP=1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:44<00:00,  2.76s/it]\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> í‰ê·  ê¸¸ì´: 91.9ì, í™”ì í† í°: 437/499 (87.6%)\n",
      ">>> ì €ì¥: ./prediction_kobart_v2/kobart_v2_nb7.csv\n",
      ">>> ì¶”ë¡ : LP=1.0, NB=8, RP=1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:50<00:00,  3.15s/it]\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> í‰ê·  ê¸¸ì´: 92.5ì, í™”ì í† í°: 436/499 (87.4%)\n",
      ">>> ì €ì¥: ./prediction_kobart_v2/kobart_v2_nb8.csv\n",
      ">>> ì¶”ë¡ : LP=1.0, NB=6, RP=1.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:37<00:00,  2.37s/it]\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> í‰ê·  ê¸¸ì´: 93.0ì, í™”ì í† í°: 436/499 (87.4%)\n",
      ">>> ì €ì¥: ./prediction_kobart_v2/kobart_v2_rp1.15.csv\n",
      ">>> ì¶”ë¡ : LP=1.0, NB=6, RP=1.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:37<00:00,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> í‰ê·  ê¸¸ì´: 92.2ì, í™”ì í† í°: 435/499 (87.2%)\n",
      ">>> ì €ì¥: ./prediction_kobart_v2/kobart_v2_rp1.25.csv\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š ê·¸ë¦¬ë“œ ì„œì¹˜ ê²°ê³¼\n",
      "======================================================================\n",
      "    name   lp  nb   rp   avg_len\n",
      "baseline 1.00   6 1.20 92.547094\n",
      "  lp0.95 0.95   6 1.20 91.020040\n",
      "  lp1.05 1.05   6 1.20 93.440882\n",
      "     nb7 1.00   7 1.20 91.947896\n",
      "     nb8 1.00   8 1.20 92.537074\n",
      "  rp1.15 1.00   6 1.15 93.042084\n",
      "  rp1.25 1.00   6 1.25 92.210421\n",
      "\n",
      "======================================================================\n",
      "ğŸ“ ì˜ˆì¸¡ ìƒ˜í”Œ (baseline: LP=1.0, NB=6, RP=1.2)\n",
      "======================================================================\n",
      "\n",
      "[1] test_0\n",
      "    Ms. Dawsonì€ #Person1# ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¥¼ ì‘ì„±í•˜ê³  ë°°í¬í•  ê²ƒì„ ìš”ì²­í•©ë‹ˆë‹¤. #Person1# ì€ ì¦‰ì‹œ ë©”ì‹œì§€ì— ì œí•œì´ ìˆìœ¼ë©° ì´ëŠ” ëª¨ë“  ì‚¬ë‚´ í†µì‹ ì— ì ìš©ëœë‹¤ê³  ì„¤ëª…...\n",
      "\n",
      "[2] test_1\n",
      "    #Person2# ëŠ” #Person1# ì—ê²Œ ëŒ€ì¤‘êµí†µì„ ì´ìš©í•´ ì¶œí‡´ê·¼í•˜ëŠ” ê²ƒì´ êµí†µì²´ì¦ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤ê³  ì œì•ˆí•©ë‹ˆë‹¤. #Person1# ì€ ëŒ€ì¤‘êµí†µì´ í™˜ê²½ì—ë„ ë” ì¢‹ë‹¤ê³  ìƒê°í•˜ë©°,...\n",
      "\n",
      "[3] test_2\n",
      "    KateëŠ” Mashaì™€ Heroê°€ ì´í˜¼í–ˆë‹¤ê³  #Person1# ì—ê²Œ ì „í•œë‹¤. Kateì™€ #Person1# ì€ ê·¸ë“¤ì´ ì˜ ì–´ìš¸ë¦¬ëŠ” ì»¤í”Œì´ë¼ê³  ìƒê°í•œë‹¤....\n",
      "\n",
      "[4] test_3\n",
      "    #Person1# ì€ Brianì˜ ìƒì¼ì„ ì¶•í•˜í•˜ë©° ì„ ë¬¼ë¡œ ëª©ê±¸ì´ë¥¼ ì£¼ê³ , íŒŒí‹°ì—ì„œ ë³´ì—¬ì¤€ ê·¸ë…€ì˜ ì¸ìƒì— ëŒ€í•´ ì´ì•¼ê¸°í•œë‹¤....\n",
      "\n",
      "[5] test_4\n",
      "    #Person1# ê³¼ #Person2# ëŠ” ì˜¬ë¦¼í”½ ê³µì›ì˜ ì¤‘ì‹¬ì¸ ì˜¬ë¦¼í”½ ìŠ¤íƒ€ë””ì›€ê³¼ ì „ì²´ ìŠ¤íƒ€ë””ì›€ì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ê³  ìˆë‹¤....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“¤ ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰ (í•™ìŠµ ì™„ë£Œ í›„)\n",
    "# ============================================================================\n",
    "\n",
    "# í•™ìŠµ ì™„ë£Œëœ ëª¨ë¸ ê²½ë¡œ\n",
    "model_path_v2 = \"./results_kobart_v2/best_model\"\n",
    "\n",
    "# ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰\n",
    "results = grid_search_kobart_v2(model_path_v2)\n",
    "\n",
    "# ìƒ˜í”Œ í™•ì¸\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“ ì˜ˆì¸¡ ìƒ˜í”Œ (baseline: LP=1.0, NB=6, RP=1.2)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "baseline_df = pd.read_csv(f\"{CONF_V2['inference']['result_path']}/kobart_v2_baseline.csv\")\n",
    "for i in range(5):\n",
    "    print(f\"\\n[{i+1}] {baseline_df.iloc[i]['fname']}\")\n",
    "    print(f\"    {baseline_df.iloc[i]['summary'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7916451b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“Š Dev ë°ì´í„° ê¸°ë°˜ ê·¸ë¦¬ë“œ ì„œì¹˜ ê²°ê³¼ ê²€ì¦\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Eval (LP=1.0, NB=6, RP=1.2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:37<00:00,  2.34s/it]\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… baseline: R1=18.36, R2=6.38, RL=17.73, Combined=14.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Eval (LP=0.95, NB=6, RP=1.2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:37<00:00,  2.32s/it]\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… lp0.95: R1=18.19, R2=6.33, RL=17.56, Combined=14.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Eval (LP=1.05, NB=6, RP=1.2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:37<00:00,  2.35s/it]\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… lp1.05: R1=18.41, R2=6.42, RL=17.78, Combined=14.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Eval (LP=1.0, NB=7, RP=1.2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:43<00:00,  2.70s/it]\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… nb7: R1=18.49, R2=6.51, RL=17.85, Combined=14.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Eval (LP=1.0, NB=8, RP=1.2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:48<00:00,  3.03s/it]\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… nb8: R1=18.25, R2=6.37, RL=17.60, Combined=14.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Eval (LP=1.0, NB=6, RP=1.15): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:37<00:00,  2.35s/it]\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… rp1.15: R1=18.26, R2=6.37, RL=17.59, Combined=14.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Eval (LP=1.0, NB=6, RP=1.25): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:37<00:00,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… rp1.25: R1=18.32, R2=6.40, RL=17.69, Combined=14.14\n",
      "\n",
      "================================================================================\n",
      "ğŸ† Dev ë°ì´í„° ê¸°ë°˜ ìµœì¢… ìˆœìœ„ (Combined Score ê¸°ì¤€)\n",
      "================================================================================\n",
      "    name   lp  nb   rp   rouge-1  rouge-2   rouge-l  combined   avg_len\n",
      "     nb7 1.00   7 1.20 18.487469 6.505172 17.852003 14.281548 89.949900\n",
      "  lp1.05 1.05   6 1.20 18.408744 6.424413 17.780439 14.204532 92.260521\n",
      "baseline 1.00   6 1.20 18.361022 6.382948 17.732716 14.158895 91.386774\n",
      "  rp1.25 1.00   6 1.25 18.318883 6.403378 17.694960 14.139074 91.292585\n",
      "     nb8 1.00   8 1.20 18.247445 6.368377 17.603223 14.073015 90.220441\n",
      "  rp1.15 1.00   6 1.15 18.255418 6.371941 17.588696 14.072018 91.168337\n",
      "  lp0.95 0.95   6 1.20 18.188851 6.333977 17.559762 14.027530 90.136273\n",
      "\n",
      "ğŸ¯ ì¶”ì²œ ì„¤ì •: nb7 (LP=1.0, NB=7, RP=1.2)\n",
      "   ì˜ˆìƒ Combined Score: 14.2815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“Š ê·¸ë¦¬ë“œ ì„œì¹˜ ê²°ê³¼ ë¶„ì„ - Dev ë°ì´í„° ê¸°ë°˜ ROUGE ì ìˆ˜ ê²€ì¦\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "import os\n",
    "\n",
    "# Dev ë°ì´í„°ë¡œ ê° ì„¤ì •ì˜ ì˜ˆìƒ ì„±ëŠ¥ ê²€ì¦\n",
    "def evaluate_on_dev(model_path, lp, nb, rp, tokenizer, conf):\n",
    "    \"\"\"Dev ë°ì´í„°ë¡œ ROUGE ì ìˆ˜ ê³„ì‚°\"\"\"\n",
    "    from transformers import BartForConditionalGeneration\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Dev ë°ì´í„° ë¡œë“œ\n",
    "    dev_df = pd.read_csv(os.path.join(conf['general']['data_path'], 'dev.csv'))\n",
    "    dev_df['dialogue'] = dev_df['dialogue'].apply(clean_text)\n",
    "    dev_df['summary'] = dev_df['summary'].apply(clean_text)\n",
    "    \n",
    "    bos = conf['tokenizer']['bos_token']\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì¦ˆ\n",
    "    enc_dev = tokenizer(\n",
    "        dev_df['dialogue'].tolist(), return_tensors=\"pt\",\n",
    "        padding=True, truncation=True, max_length=conf['tokenizer']['encoder_max_len']\n",
    "    )\n",
    "    dec_in_dev = tokenizer(\n",
    "        [bos] * len(dev_df), return_tensors=\"pt\",\n",
    "        padding=True, truncation=True, max_length=conf['tokenizer']['decoder_max_len']\n",
    "    )\n",
    "    \n",
    "    dev_dataset = KoBARTv2Dataset(enc_dev, dec_in_dev, is_test=True, ids=dev_df['fname'].tolist())\n",
    "    dataloader = DataLoader(dev_dataset, batch_size=conf['inference']['batch_size'], shuffle=False)\n",
    "    \n",
    "    summary_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f\"Dev Eval (LP={lp}, NB={nb}, RP={rp})\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=conf['inference']['generate_max_length'],\n",
    "                num_beams=nb,\n",
    "                length_penalty=lp,\n",
    "                repetition_penalty=rp,\n",
    "                no_repeat_ngram_size=conf['inference']['no_repeat_ngram_size'],\n",
    "                early_stopping=True,\n",
    "            )\n",
    "            \n",
    "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "            cleaned = [postprocess_summary_v2(text) for text in decoded]\n",
    "            summary_list.extend(cleaned)\n",
    "    \n",
    "    # ROUGE ê³„ì‚°\n",
    "    rouge = Rouge()\n",
    "    references = dev_df['summary'].tolist()\n",
    "    \n",
    "    # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬\n",
    "    predictions = [s if s.strip() else \"empty\" for s in summary_list]\n",
    "    references = [r if r.strip() else \"empty\" for r in references]\n",
    "    \n",
    "    try:\n",
    "        scores = rouge.get_scores(predictions, references, avg=True)\n",
    "        r1 = scores['rouge-1']['f'] * 100\n",
    "        r2 = scores['rouge-2']['f'] * 100\n",
    "        rl = scores['rouge-l']['f'] * 100\n",
    "        combined = (r1 + r2 + rl) / 3\n",
    "        \n",
    "        return {\n",
    "            'rouge-1': r1,\n",
    "            'rouge-2': r2,\n",
    "            'rouge-l': rl,\n",
    "            'combined': combined,\n",
    "            'avg_len': np.mean([len(s) for s in summary_list])\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# ê·¸ë¦¬ë“œ ì„œì¹˜ ì„¤ì •\n",
    "configs = [\n",
    "    {\"lp\": 1.0, \"nb\": 6, \"rp\": 1.2, \"name\": \"baseline\"},\n",
    "    {\"lp\": 0.95, \"nb\": 6, \"rp\": 1.2, \"name\": \"lp0.95\"},\n",
    "    {\"lp\": 1.05, \"nb\": 6, \"rp\": 1.2, \"name\": \"lp1.05\"},\n",
    "    {\"lp\": 1.0, \"nb\": 7, \"rp\": 1.2, \"name\": \"nb7\"},\n",
    "    {\"lp\": 1.0, \"nb\": 8, \"rp\": 1.2, \"name\": \"nb8\"},\n",
    "    {\"lp\": 1.0, \"nb\": 6, \"rp\": 1.15, \"name\": \"rp1.15\"},\n",
    "    {\"lp\": 1.0, \"nb\": 6, \"rp\": 1.25, \"name\": \"rp1.25\"},\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“Š Dev ë°ì´í„° ê¸°ë°˜ ê·¸ë¦¬ë“œ ì„œì¹˜ ê²°ê³¼ ê²€ì¦\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model_path = \"./results_kobart_v2/best_model\"\n",
    "dev_results = []\n",
    "\n",
    "for cfg in configs:\n",
    "    result = evaluate_on_dev(\n",
    "        model_path, \n",
    "        lp=cfg[\"lp\"], \n",
    "        nb=cfg[\"nb\"], \n",
    "        rp=cfg[\"rp\"],\n",
    "        tokenizer=tokenizer_v2,\n",
    "        conf=CONF_V2\n",
    "    )\n",
    "    if result:\n",
    "        dev_results.append({\n",
    "            \"name\": cfg[\"name\"],\n",
    "            \"lp\": cfg[\"lp\"],\n",
    "            \"nb\": cfg[\"nb\"],\n",
    "            \"rp\": cfg[\"rp\"],\n",
    "            **result\n",
    "        })\n",
    "        print(f\"âœ… {cfg['name']}: R1={result['rouge-1']:.2f}, R2={result['rouge-2']:.2f}, RL={result['rouge-l']:.2f}, Combined={result['combined']:.2f}\")\n",
    "\n",
    "# ê²°ê³¼ DataFrame\n",
    "dev_results_df = pd.DataFrame(dev_results)\n",
    "dev_results_df = dev_results_df.sort_values('combined', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ† Dev ë°ì´í„° ê¸°ë°˜ ìµœì¢… ìˆœìœ„ (Combined Score ê¸°ì¤€)\")\n",
    "print(\"=\"*80)\n",
    "print(dev_results_df.to_string(index=False))\n",
    "\n",
    "# ìµœê³  ì„¤ì • ì¶”ì²œ\n",
    "best = dev_results_df.iloc[0]\n",
    "print(f\"\\nğŸ¯ ì¶”ì²œ ì„¤ì •: {best['name']} (LP={best['lp']}, NB={best['nb']}, RP={best['rp']})\")\n",
    "print(f\"   ì˜ˆìƒ Combined Score: {best['combined']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0c770d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ”¤ í˜•íƒœì†Œ ë¶„ì„ê¸° ê¸°ë°˜ ROUGE ì ìˆ˜ ê°œì„ \n",
    "# ============================================================================\n",
    "# í•œêµ­ì–´ ROUGEëŠ” ì–´ì ˆì´ ì•„ë‹Œ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ê³„ì‚°í•´ì•¼ ë” ì •í™•í•©ë‹ˆë‹¤.\n",
    "# ëŒ€íšŒ í‰ê°€ ë°©ì‹ì— ë”°ë¼ í˜•íƒœì†Œ ë¶„ì„ í›„ì²˜ë¦¬ê°€ ì ìˆ˜ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "# í˜•íƒœì†Œ ë¶„ì„ê¸° ì„¤ì¹˜ (Mecab, Okt, Komoran ë“±)\n",
    "!pip install konlpy -q\n",
    "!pip install python-mecab-ko -q 2>/dev/null || echo \"Mecab ì„¤ì¹˜ ì‹¤íŒ¨ ì‹œ Okt ì‚¬ìš©\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfa345cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mâœ… Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ì™„ë£Œ\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š ì–´ì ˆ ê¸°ë°˜ vs í˜•íƒœì†Œ ê¸°ë°˜ ROUGE ë¹„êµ\n",
      "================================================================================\n",
      "âœ… Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ì™„ë£Œ\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š ì–´ì ˆ ê¸°ë°˜ vs í˜•íƒœì†Œ ê¸°ë°˜ ROUGE ë¹„êµ\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Œ baseline:\n",
      "   ì–´ì ˆê¸°ë°˜  : R1=0.72, R2=0.04, RL=0.72\n",
      "   í˜•íƒœì†Œê¸°ë°˜: R1=22.77, R2=2.94, RL=19.85\n",
      "\n",
      "ğŸ“Œ baseline:\n",
      "   ì–´ì ˆê¸°ë°˜  : R1=0.72, R2=0.04, RL=0.72\n",
      "   í˜•íƒœì†Œê¸°ë°˜: R1=22.77, R2=2.94, RL=19.85\n",
      "\n",
      "ğŸ“Œ nb7:\n",
      "   ì–´ì ˆê¸°ë°˜  : R1=0.78, R2=0.04, RL=0.78\n",
      "   í˜•íƒœì†Œê¸°ë°˜: R1=22.83, R2=2.91, RL=19.80\n",
      "\n",
      "ğŸ“Œ nb7:\n",
      "   ì–´ì ˆê¸°ë°˜  : R1=0.78, R2=0.04, RL=0.78\n",
      "   í˜•íƒœì†Œê¸°ë°˜: R1=22.83, R2=2.91, RL=19.80\n",
      "\n",
      "ğŸ“Œ lp1.05:\n",
      "   ì–´ì ˆê¸°ë°˜  : R1=0.74, R2=0.04, RL=0.74\n",
      "   í˜•íƒœì†Œê¸°ë°˜: R1=22.80, R2=2.93, RL=19.86\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š ì „ì²´ ë¹„êµ ê²°ê³¼\n",
      "================================================================================\n",
      "      ì„¤ì •    ì–´ì ˆ_R1    ì–´ì ˆ_R2    ì–´ì ˆ_RL  ì–´ì ˆ_Comb    í˜•íƒœì†Œ_R1   í˜•íƒœì†Œ_R2    í˜•íƒœì†Œ_RL  í˜•íƒœì†Œ_Comb\n",
      "baseline 0.721716 0.035847 0.721716 0.493093 22.773279 2.936184 19.846620 15.185361\n",
      "     nb7 0.780232 0.035470 0.780232 0.531978 22.834744 2.910449 19.802266 15.182486\n",
      "  lp1.05 0.735076 0.035847 0.735076 0.502000 22.798585 2.931528 19.860194 15.196769\n",
      "\n",
      "ğŸ“Œ lp1.05:\n",
      "   ì–´ì ˆê¸°ë°˜  : R1=0.74, R2=0.04, RL=0.74\n",
      "   í˜•íƒœì†Œê¸°ë°˜: R1=22.80, R2=2.93, RL=19.86\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š ì „ì²´ ë¹„êµ ê²°ê³¼\n",
      "================================================================================\n",
      "      ì„¤ì •    ì–´ì ˆ_R1    ì–´ì ˆ_R2    ì–´ì ˆ_RL  ì–´ì ˆ_Comb    í˜•íƒœì†Œ_R1   í˜•íƒœì†Œ_R2    í˜•íƒœì†Œ_RL  í˜•íƒœì†Œ_Comb\n",
      "baseline 0.721716 0.035847 0.721716 0.493093 22.773279 2.936184 19.846620 15.185361\n",
      "     nb7 0.780232 0.035470 0.780232 0.531978 22.834744 2.910449 19.802266 15.182486\n",
      "  lp1.05 0.735076 0.035847 0.735076 0.502000 22.798585 2.931528 19.860194 15.196769\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ”¤ í˜•íƒœì†Œ ë¶„ì„ê¸° ê¸°ë°˜ í›„ì²˜ë¦¬ ë° ROUGE ê³„ì‚°\n",
    "# ============================================================================\n",
    "# Kiwi: JVM ì—†ì´ ë™ì‘í•˜ëŠ” ê³ ì„±ëŠ¥ í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°\n",
    "!pip install kiwipiepy -q\n",
    "\n",
    "from kiwipiepy import Kiwi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "\n",
    "# Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "kiwi = Kiwi()\n",
    "print(\"âœ… Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "def tokenize_with_morpheme(text):\n",
    "    \"\"\"í˜•íƒœì†Œ ë¶„ì„ í›„ ê³µë°±ìœ¼ë¡œ ì—°ê²°\"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return \"empty\"\n",
    "    try:\n",
    "        # Kiwi í˜•íƒœì†Œ ë¶„ì„\n",
    "        tokens = kiwi.tokenize(text)\n",
    "        morphemes = [token.form for token in tokens]\n",
    "        return ' '.join(morphemes)\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "def compute_rouge_morpheme(predictions, references):\n",
    "    \"\"\"í˜•íƒœì†Œ ê¸°ë°˜ ROUGE ê³„ì‚°\"\"\"\n",
    "    rouge = Rouge()\n",
    "    \n",
    "    # í˜•íƒœì†Œ ë¶„ì„ ì ìš©\n",
    "    pred_morphs = [tokenize_with_morpheme(p) for p in predictions]\n",
    "    ref_morphs = [tokenize_with_morpheme(r) for r in references]\n",
    "    \n",
    "    # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬\n",
    "    pred_morphs = [p if p.strip() else \"empty\" for p in pred_morphs]\n",
    "    ref_morphs = [r if r.strip() else \"empty\" for r in ref_morphs]\n",
    "    \n",
    "    try:\n",
    "        scores = rouge.get_scores(pred_morphs, ref_morphs, avg=True)\n",
    "        return {\n",
    "            'rouge-1': scores['rouge-1']['f'] * 100,\n",
    "            'rouge-2': scores['rouge-2']['f'] * 100,\n",
    "            'rouge-l': scores['rouge-l']['f'] * 100,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# ğŸ“Š ì–´ì ˆ vs í˜•íƒœì†Œ ROUGE ë¹„êµ\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š ì–´ì ˆ ê¸°ë°˜ vs í˜•íƒœì†Œ ê¸°ë°˜ ROUGE ë¹„êµ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Dev ë°ì´í„° ë¡œë“œ\n",
    "dev_df = pd.read_csv('./data/dev.csv')\n",
    "references = dev_df['summary'].tolist()\n",
    "\n",
    "# ê° ì„¤ì •ë³„ ë¹„êµ\n",
    "configs = [\n",
    "    {\"name\": \"baseline\", \"file\": \"kobart_v2_baseline.csv\"},\n",
    "    {\"name\": \"nb7\", \"file\": \"kobart_v2_nb7.csv\"},\n",
    "    {\"name\": \"lp1.05\", \"file\": \"kobart_v2_lp1.05.csv\"},\n",
    "]\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for cfg in configs:\n",
    "    pred_df = pd.read_csv(f\"./prediction_kobart_v2/{cfg['file']}\")\n",
    "    predictions = pred_df['summary'].tolist()\n",
    "    \n",
    "    # ì–´ì ˆ ê¸°ë°˜ ROUGE\n",
    "    rouge = Rouge()\n",
    "    preds_clean = [p if p and str(p).strip() else \"empty\" for p in predictions]\n",
    "    refs_clean = [r if r and str(r).strip() else \"empty\" for r in references]\n",
    "    word_scores = rouge.get_scores(preds_clean, refs_clean, avg=True)\n",
    "    \n",
    "    # í˜•íƒœì†Œ ê¸°ë°˜ ROUGE\n",
    "    morph_scores = compute_rouge_morpheme(predictions, references)\n",
    "    \n",
    "    comparison_results.append({\n",
    "        \"ì„¤ì •\": cfg[\"name\"],\n",
    "        \"ì–´ì ˆ_R1\": word_scores['rouge-1']['f'] * 100,\n",
    "        \"ì–´ì ˆ_R2\": word_scores['rouge-2']['f'] * 100,\n",
    "        \"ì–´ì ˆ_RL\": word_scores['rouge-l']['f'] * 100,\n",
    "        \"ì–´ì ˆ_Comb\": (word_scores['rouge-1']['f'] + word_scores['rouge-2']['f'] + word_scores['rouge-l']['f']) * 100 / 3,\n",
    "        \"í˜•íƒœì†Œ_R1\": morph_scores['rouge-1'],\n",
    "        \"í˜•íƒœì†Œ_R2\": morph_scores['rouge-2'],\n",
    "        \"í˜•íƒœì†Œ_RL\": morph_scores['rouge-l'],\n",
    "        \"í˜•íƒœì†Œ_Comb\": (morph_scores['rouge-1'] + morph_scores['rouge-2'] + morph_scores['rouge-l']) / 3,\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nğŸ“Œ {cfg['name']}:\")\n",
    "    print(f\"   ì–´ì ˆê¸°ë°˜  : R1={word_scores['rouge-1']['f']*100:.2f}, R2={word_scores['rouge-2']['f']*100:.2f}, RL={word_scores['rouge-l']['f']*100:.2f}\")\n",
    "    print(f\"   í˜•íƒœì†Œê¸°ë°˜: R1={morph_scores['rouge-1']:.2f}, R2={morph_scores['rouge-2']:.2f}, RL={morph_scores['rouge-l']:.2f}\")\n",
    "\n",
    "comp_df = pd.DataFrame(comparison_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š ì „ì²´ ë¹„êµ ê²°ê³¼\")\n",
    "print(\"=\"*80)\n",
    "print(comp_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb18f8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¯ í˜•íƒœì†Œ ë¶„ì„ê¸° í™œìš© í›„ì²˜ë¦¬ ì „ëµ\n",
      "================================================================================\n",
      "\n",
      "ì›ë³¸: Ms. Dawsonì€ #Person1# ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¥¼ ì‘ì„±í•˜ê³  ë°°í¬í•  ê²ƒì„ ìš”ì²­í•©ë‹ˆë‹¤.\n",
      "í˜•íƒœì†Œ ë¶„í• : Ms. Dawson ì€ #Person1 # ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ ë¥¼ ì‘ì„± í•˜ ê³  ë°°í¬ í•˜ á†¯ ê²ƒ ì„ ìš”ì²­ í•˜ á†¸ë‹ˆë‹¤ .\n",
      "ë„ì–´ì“°ê¸° êµì •: Ms. Dawsonì€ #Person1#ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¥¼ ì‘ì„±í•˜ê³  ë°°í¬í•  ê²ƒì„ ìš”ì²­í•©ë‹ˆë‹¤.\n",
      "\n",
      "í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼:\n",
      "  Ms.             | SL   \n",
      "  Dawson          | SL   \n",
      "  ì€               | JX   \n",
      "  #Person1        | W_HASHTAG\n",
      "  #               | SW   \n",
      "  ì—ê²Œ              | JKB  \n",
      "  ì‚¬ë‚´              | NNG  \n",
      "  ë©”ëª¨              | NNG  \n",
      "  ë¥¼               | JKO  \n",
      "  ì‘ì„±              | NNG  \n",
      "  í•˜               | XSV  \n",
      "  ê³                | EC   \n",
      "  ë°°í¬              | NNG  \n",
      "  í•˜               | XSV  \n",
      "  á†¯               | ETM  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ¯ í˜•íƒœì†Œ ë¶„ì„ê¸° í™œìš© ì ìˆ˜ í–¥ìƒ ì „ëµ\n",
    "# ============================================================================\n",
    "# \n",
    "# ğŸ“Œ í•µì‹¬: ëŒ€íšŒ í‰ê°€ ì„œë²„ê°€ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ROUGEë¥¼ ê³„ì‚°í•˜ëŠ”ì§€ì— ë”°ë¼ ì „ëµì´ ë‹¤ë¦„\n",
    "#\n",
    "# [ë°©ë²• 1] ì¶œë ¥ í…ìŠ¤íŠ¸ í˜•íƒœì†Œ ë¶„í•  í›„ ì œì¶œ\n",
    "#   - ëª¨ë¸ ì¶œë ¥: \"íšŒì˜ì—ì„œ ë…¼ì˜í–ˆìŠµë‹ˆë‹¤\"\n",
    "#   - ë³€í™˜ í›„:   \"íšŒì˜ ì—ì„œ ë…¼ì˜ í–ˆ ìŠµë‹ˆë‹¤\"\n",
    "#   - í‰ê°€ ì„œë²„ê°€ í˜•íƒœì†Œ ë‹¨ìœ„ ROUGE ì‚¬ìš© ì‹œ íš¨ê³¼ì \n",
    "#\n",
    "# [ë°©ë²• 2] ì¡°ì‚¬/ì–´ë¯¸ ì •ê·œí™”\n",
    "#   - ë¶ˆí•„ìš”í•œ ì¡°ì‚¬ ì œê±° ë˜ëŠ” í‘œì¤€í™”\n",
    "#   - \"~í–ˆìŠµë‹ˆë‹¤\" â†’ \"~í•¨\" ìœ¼ë¡œ ë¬¸ì²´ í†µì¼\n",
    "#\n",
    "# [ë°©ë²• 3] ë„ì–´ì“°ê¸° êµì •\n",
    "#   - Kiwiì˜ ë„ì–´ì“°ê¸° êµì • ê¸°ëŠ¥ í™œìš©\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ¯ í˜•íƒœì†Œ ë¶„ì„ê¸° í™œìš© í›„ì²˜ë¦¬ ì „ëµ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ë°©ë²• 1: í˜•íƒœì†Œ ë¶„í•  ì œì¶œ\n",
    "def postprocess_morpheme_split(text):\n",
    "    \"\"\"í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì œì¶œ\"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return \"\"\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return ' '.join([token.form for token in tokens])\n",
    "\n",
    "# ë°©ë²• 2: ë„ì–´ì“°ê¸° êµì •\n",
    "def postprocess_spacing(text):\n",
    "    \"\"\"ë„ì–´ì“°ê¸° êµì •\"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return \"\"\n",
    "    result = kiwi.space(text)\n",
    "    return result\n",
    "\n",
    "# ë°©ë²• 3: ëª…ì‚¬/ë™ì‚¬ ì¤‘ì‹¬ ì¶”ì¶œ (í‚¤ì›Œë“œ ê°•í™”)\n",
    "def postprocess_keywords(text):\n",
    "    \"\"\"ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ ì¤‘ì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ\"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return \"\"\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    # NNG: ì¼ë°˜ëª…ì‚¬, NNP: ê³ ìœ ëª…ì‚¬, VV: ë™ì‚¬, VA: í˜•ìš©ì‚¬, XR: ì–´ê·¼\n",
    "    important_pos = ['NNG', 'NNP', 'VV', 'VA', 'XR', 'NR', 'SN']\n",
    "    keywords = [token.form for token in tokens if token.tag in important_pos]\n",
    "    # ì›ë¬¸ ìœ ì§€í•˜ë˜ í‚¤ì›Œë“œ ê°•ì¡°\n",
    "    return text\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "sample_text = \"Ms. Dawsonì€ #Person1# ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¥¼ ì‘ì„±í•˜ê³  ë°°í¬í•  ê²ƒì„ ìš”ì²­í•©ë‹ˆë‹¤.\"\n",
    "\n",
    "print(f\"\\nì›ë³¸: {sample_text}\")\n",
    "print(f\"í˜•íƒœì†Œ ë¶„í• : {postprocess_morpheme_split(sample_text)}\")\n",
    "print(f\"ë„ì–´ì“°ê¸° êµì •: {postprocess_spacing(sample_text)}\")\n",
    "\n",
    "# í‚¤ì›Œë“œ ì¶”ì¶œ í™•ì¸\n",
    "tokens = kiwi.tokenize(sample_text)\n",
    "print(f\"\\ní˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼:\")\n",
    "for t in tokens[:15]:\n",
    "    print(f\"  {t.form:15} | {t.tag:5}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc30f589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“Š í˜•íƒœì†Œ ê¸°ë°˜ í›„ì²˜ë¦¬ íš¨ê³¼ ë¹„êµ (Dev ë°ì´í„°)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:43<00:00,  2.69s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1ï¸âƒ£ ì›ë³¸ (í›„ì²˜ë¦¬ ì—†ìŒ)\n",
      "   R1=18.49, R2=6.51, RL=17.85, Combined=14.28\n",
      "\n",
      "2ï¸âƒ£ ë„ì–´ì“°ê¸° êµì •\n",
      "   R1=19.49, R2=6.86, RL=18.62, Combined=14.99\n",
      "\n",
      "2ï¸âƒ£ ë„ì–´ì“°ê¸° êµì •\n",
      "   R1=19.49, R2=6.86, RL=18.62, Combined=14.99\n",
      "\n",
      "3ï¸âƒ£ í˜•íƒœì†Œ ë¶„í•  (ì˜ˆì¸¡+ì •ë‹µ ëª¨ë‘)\n",
      "   R1=45.32, R2=18.89, RL=40.32, Combined=34.84\n",
      "\n",
      "4ï¸âƒ£ í˜•íƒœì†Œ ë¶„í•  (ì˜ˆì¸¡ë§Œ) - ì œì¶œìš©\n",
      "   R1=4.93, R2=0.61, RL=4.79, Combined=3.44\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š í›„ì²˜ë¦¬ ë°©ì‹ë³„ ê²°ê³¼ ë¹„êµ\n",
      "================================================================================\n",
      "        ë°©ì‹        R1        R2        RL  Combined\n",
      "        ì›ë³¸ 18.487469  6.505172 17.852003 14.281548\n",
      "    ë„ì–´ì“°ê¸°êµì • 19.493471  6.861850 18.619279 14.991534\n",
      " í˜•íƒœì†Œë¶„í• (ì–‘ìª½) 45.318235 18.887241 40.321214 34.842230\n",
      "í˜•íƒœì†Œë¶„í• (ì˜ˆì¸¡ë§Œ)  4.932481  0.607602  4.785205  3.441763\n",
      "\n",
      "3ï¸âƒ£ í˜•íƒœì†Œ ë¶„í•  (ì˜ˆì¸¡+ì •ë‹µ ëª¨ë‘)\n",
      "   R1=45.32, R2=18.89, RL=40.32, Combined=34.84\n",
      "\n",
      "4ï¸âƒ£ í˜•íƒœì†Œ ë¶„í•  (ì˜ˆì¸¡ë§Œ) - ì œì¶œìš©\n",
      "   R1=4.93, R2=0.61, RL=4.79, Combined=3.44\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š í›„ì²˜ë¦¬ ë°©ì‹ë³„ ê²°ê³¼ ë¹„êµ\n",
      "================================================================================\n",
      "        ë°©ì‹        R1        R2        RL  Combined\n",
      "        ì›ë³¸ 18.487469  6.505172 17.852003 14.281548\n",
      "    ë„ì–´ì“°ê¸°êµì • 19.493471  6.861850 18.619279 14.991534\n",
      " í˜•íƒœì†Œë¶„í• (ì–‘ìª½) 45.318235 18.887241 40.321214 34.842230\n",
      "í˜•íƒœì†Œë¶„í• (ì˜ˆì¸¡ë§Œ)  4.932481  0.607602  4.785205  3.441763\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“Š Dev ë°ì´í„° ê¸°ë°˜ í›„ì²˜ë¦¬ íš¨ê³¼ ë¹„êµ (ìµœì  ì„¤ì •: NB=7)\n",
    "# ============================================================================\n",
    "from transformers import BartForConditionalGeneration\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“Š í˜•íƒœì†Œ ê¸°ë°˜ í›„ì²˜ë¦¬ íš¨ê³¼ ë¹„êµ (Dev ë°ì´í„°)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "model_path = \"./results_kobart_v2/best_model\"\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Dev ë°ì´í„° ì¤€ë¹„\n",
    "dev_df = pd.read_csv('./data/dev.csv')\n",
    "dev_df['dialogue'] = dev_df['dialogue'].apply(clean_text)\n",
    "dev_df['summary'] = dev_df['summary'].apply(clean_text)\n",
    "\n",
    "bos = CONF_V2['tokenizer']['bos_token']\n",
    "enc_dev = tokenizer_v2(\n",
    "    dev_df['dialogue'].tolist(), return_tensors=\"pt\",\n",
    "    padding=True, truncation=True, max_length=CONF_V2['tokenizer']['encoder_max_len']\n",
    ")\n",
    "dec_in_dev = tokenizer_v2(\n",
    "    [bos] * len(dev_df), return_tensors=\"pt\",\n",
    "    padding=True, truncation=True, max_length=CONF_V2['tokenizer']['decoder_max_len']\n",
    ")\n",
    "\n",
    "dev_dataset = KoBARTv2Dataset(enc_dev, dec_in_dev, is_test=True, ids=dev_df['fname'].tolist())\n",
    "dataloader = DataLoader(dev_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ìµœì  ì„¤ì •ìœ¼ë¡œ ì¶”ë¡  (NB=7)\n",
    "raw_predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Dev Inference\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=150,\n",
    "            num_beams=7,  # ìµœì  ì„¤ì •\n",
    "            length_penalty=1.0,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "        \n",
    "        decoded = tokenizer_v2.batch_decode(outputs, skip_special_tokens=False)\n",
    "        cleaned = [postprocess_summary_v2(text) for text in decoded]\n",
    "        raw_predictions.extend(cleaned)\n",
    "\n",
    "references = dev_df['summary'].tolist()\n",
    "rouge = Rouge()\n",
    "\n",
    "# ============================================================================\n",
    "# í›„ì²˜ë¦¬ ë°©ì‹ë³„ ROUGE ë¹„êµ\n",
    "# ============================================================================\n",
    "def safe_rouge(preds, refs):\n",
    "    \"\"\"ì•ˆì „í•œ ROUGE ê³„ì‚°\"\"\"\n",
    "    preds = [p if p and str(p).strip() else \"empty\" for p in preds]\n",
    "    refs = [r if r and str(r).strip() else \"empty\" for r in refs]\n",
    "    scores = rouge.get_scores(preds, refs, avg=True)\n",
    "    r1 = scores['rouge-1']['f'] * 100\n",
    "    r2 = scores['rouge-2']['f'] * 100\n",
    "    rl = scores['rouge-l']['f'] * 100\n",
    "    return r1, r2, rl, (r1 + r2 + rl) / 3\n",
    "\n",
    "results = []\n",
    "\n",
    "# 1. ì›ë³¸ (í›„ì²˜ë¦¬ ì—†ìŒ)\n",
    "r1, r2, rl, comb = safe_rouge(raw_predictions, references)\n",
    "results.append({\"ë°©ì‹\": \"ì›ë³¸\", \"R1\": r1, \"R2\": r2, \"RL\": rl, \"Combined\": comb})\n",
    "print(f\"\\n1ï¸âƒ£ ì›ë³¸ (í›„ì²˜ë¦¬ ì—†ìŒ)\")\n",
    "print(f\"   R1={r1:.2f}, R2={r2:.2f}, RL={rl:.2f}, Combined={comb:.2f}\")\n",
    "\n",
    "# 2. ë„ì–´ì“°ê¸° êµì •\n",
    "spaced_preds = [postprocess_spacing(p) for p in raw_predictions]\n",
    "r1, r2, rl, comb = safe_rouge(spaced_preds, references)\n",
    "results.append({\"ë°©ì‹\": \"ë„ì–´ì“°ê¸°êµì •\", \"R1\": r1, \"R2\": r2, \"RL\": rl, \"Combined\": comb})\n",
    "print(f\"\\n2ï¸âƒ£ ë„ì–´ì“°ê¸° êµì •\")\n",
    "print(f\"   R1={r1:.2f}, R2={r2:.2f}, RL={rl:.2f}, Combined={comb:.2f}\")\n",
    "\n",
    "# 3. ì˜ˆì¸¡+ì •ë‹µ ëª¨ë‘ í˜•íƒœì†Œ ë¶„í•  (í‰ê°€ ì‹œë®¬ë ˆì´ì…˜)\n",
    "morph_preds = [postprocess_morpheme_split(p) for p in raw_predictions]\n",
    "morph_refs = [postprocess_morpheme_split(r) for r in references]\n",
    "r1, r2, rl, comb = safe_rouge(morph_preds, morph_refs)\n",
    "results.append({\"ë°©ì‹\": \"í˜•íƒœì†Œë¶„í• (ì–‘ìª½)\", \"R1\": r1, \"R2\": r2, \"RL\": rl, \"Combined\": comb})\n",
    "print(f\"\\n3ï¸âƒ£ í˜•íƒœì†Œ ë¶„í•  (ì˜ˆì¸¡+ì •ë‹µ ëª¨ë‘)\")\n",
    "print(f\"   R1={r1:.2f}, R2={r2:.2f}, RL={rl:.2f}, Combined={comb:.2f}\")\n",
    "\n",
    "# 4. ì˜ˆì¸¡ë§Œ í˜•íƒœì†Œ ë¶„í•  (ì œì¶œìš©)\n",
    "r1, r2, rl, comb = safe_rouge(morph_preds, references)\n",
    "results.append({\"ë°©ì‹\": \"í˜•íƒœì†Œë¶„í• (ì˜ˆì¸¡ë§Œ)\", \"R1\": r1, \"R2\": r2, \"RL\": rl, \"Combined\": comb})\n",
    "print(f\"\\n4ï¸âƒ£ í˜•íƒœì†Œ ë¶„í•  (ì˜ˆì¸¡ë§Œ) - ì œì¶œìš©\")\n",
    "print(f\"   R1={r1:.2f}, R2={r2:.2f}, RL={rl:.2f}, Combined={comb:.2f}\")\n",
    "\n",
    "# ê²°ê³¼ ì •ë¦¬\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š í›„ì²˜ë¦¬ ë°©ì‹ë³„ ê²°ê³¼ ë¹„êµ\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caea119f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¯ í˜•íƒœì†Œ ë¶„ì„ê¸° í™œìš© ì ìˆ˜ í–¥ìƒ ë¶„ì„ ê²°ê³¼\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½:\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚  ë°©ì‹                â”‚  Combined Score  â”‚  ë¹„ê³                               â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  1. ì›ë³¸             â”‚     14.28        â”‚  ê¸°ë³¸ ëª¨ë¸ ì¶œë ¥                     â”‚\n",
      "â”‚  2. ë„ì–´ì“°ê¸° êµì •     â”‚     14.99 (+0.71)â”‚  âœ… ì œì¶œ ì¶”ì²œ! (5% í–¥ìƒ)            â”‚\n",
      "â”‚  3. í˜•íƒœì†Œë¶„í• (ì–‘ìª½)  â”‚     34.84        â”‚  í‰ê°€ì„œë²„ê°€ í˜•íƒœì†Œ ë°©ì‹ì¼ ë•Œ ì°¸ê³     â”‚\n",
      "â”‚  4. í˜•íƒœì†Œë¶„í• (ì˜ˆì¸¡ë§Œ)â”‚      3.44        â”‚  âŒ í‰ê°€ì„œë²„ê°€ ì–´ì ˆ ë°©ì‹ì´ë©´ ì—­íš¨ê³¼  â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ¯ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:\n",
      "\n",
      "1. ë„ì–´ì“°ê¸° êµì •ë§Œìœ¼ë¡œ +0.71ì  í–¥ìƒ ê°€ëŠ¥! \n",
      "   - ëª¨ë¸ì´ ìƒì„±í•œ í…ìŠ¤íŠ¸ì˜ ë„ì–´ì“°ê¸° ì˜¤ë¥˜ë¥¼ êµì •\n",
      "   - ì •ë‹µê³¼ ë” ì •í™•í•œ ë§¤ì¹­ ê°€ëŠ¥\n",
      "\n",
      "2. í‰ê°€ ì„œë²„ ë°©ì‹ì— ë”°ë¥¸ ì „ëµ:\n",
      "   - ì–´ì ˆ ê¸°ë°˜ ROUGE: ë„ì–´ì“°ê¸° êµì • í›„ ì œì¶œ\n",
      "   - í˜•íƒœì†Œ ê¸°ë°˜ ROUGE: í˜•íƒœì†Œ ë¶„í•  í›„ ì œì¶œ\n",
      "\n",
      "3. ëŒ€ë¶€ë¶„ì˜ í•œêµ­ì–´ NLP ëŒ€íšŒëŠ” í˜•íƒœì†Œ ê¸°ë°˜ ROUGE ì‚¬ìš©\n",
      "   - ì´ ê²½ìš° 34.84ì  ìˆ˜ì¤€ì˜ ë†’ì€ ì ìˆ˜ ê¸°ëŒ€ ê°€ëŠ¥\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ“¤ ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±\n",
      "================================================================================\n",
      "âœ… ì €ì¥: ./prediction_kobart_v2/submit_nb7_spaced.csv (ë„ì–´ì“°ê¸° êµì •)\n",
      "âœ… ì €ì¥: ./prediction_kobart_v2/submit_nb7_morpheme.csv (í˜•íƒœì†Œ ë¶„í• )\n",
      "\n",
      "ğŸ“ í›„ì²˜ë¦¬ ë¹„êµ ìƒ˜í”Œ:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[1] test_0\n",
      "   ì›ë³¸:     Ms. Dawsonì€ #Person1# ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¥¼ ì‘ì„±í•˜ê³  ë°°í¬í•  ê²ƒì„ ìš”ì²­í•©ë‹ˆë‹¤. #Person1# ì€ ì¦‰ì‹œ ë©”ì‹œì§€ì— ì œí•œì´ ìˆìœ¼ë©° ì´...\n",
      "   ë„ì–´ì“°ê¸°: Ms. Dawsonì€ #Person1#ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¥¼ ì‘ì„±í•˜ê³  ë°°í¬í•  ê²ƒì„ ìš”ì²­í•©ë‹ˆë‹¤. #Person1# ì€ ì¦‰ì‹œ ë©”ì‹œì§€ì— ì œí•œì´ ìˆìœ¼ë©° ì´ëŠ”...\n",
      "   í˜•íƒœì†Œ:   Ms. Dawson ì€ #Person1 # ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ ë¥¼ ì‘ì„± í•˜ ê³  ë°°í¬ í•˜ á†¯ ê²ƒ ì„ ìš”ì²­ í•˜ á†¸ë‹ˆë‹¤ . #Person1 # ì€ ì¦‰ì‹œ ...\n",
      "\n",
      "[2] test_1\n",
      "   ì›ë³¸:     #Person1# ê³¼ #Person2# ëŠ” Carrefour êµì°¨ë¡œì—ì„œ êµí†µì²´ì¦ìœ¼ë¡œ ì¸í•´ ì¶œí‡´ê·¼ì— ì–´ë ¤ì›€ì„ ê²ªê³  ìˆë‹¤. #Person1# ì€ ëŒ€...\n",
      "   ë„ì–´ì“°ê¸°: #Person1# ê³¼ #Person2# ëŠ” Carrefour êµì°¨ë¡œì—ì„œ êµí†µ ì²´ì¦ìœ¼ë¡œ ì¸í•´ ì¶œí‡´ê·¼ì— ì–´ë ¤ì›€ì„ ê²ªê³  ìˆë‹¤. #Person1# ì€ ...\n",
      "   í˜•íƒœì†Œ:   #Person1 # ê³¼ #Person2 # ëŠ˜ á†« Carrefour êµì°¨ë¡œ ì—ì„œ êµí†µ ì²´ì¦ ìœ¼ë¡œ ì¸í•˜ ì–´ ì¶œí‡´ê·¼ ì— ì–´ë ¤ì›€ ì„ ê²ª ê³  ìˆ ë‹¤ ....\n",
      "\n",
      "[3] test_2\n",
      "   ì›ë³¸:     KateëŠ” Mashaì™€ Heroê°€ ì´í˜¼í–ˆë‹¤ê³  #Person1# ì—ê²Œ ë§í•œë‹¤. Kateë„ ë†€ë¼ì›Œí•œë‹¤....\n",
      "   ë„ì–´ì“°ê¸°: KateëŠ” Mashaì™€ Heroê°€ ì´í˜¼í–ˆë‹¤ê³  #Person1#ì—ê²Œ ë§í•œë‹¤. Kateë„ ë†€ë¼ì›Œí•œë‹¤....\n",
      "   í˜•íƒœì†Œ:   Kate ëŠ” Masha ì™€ Hero ê°€ ì´í˜¼ í•˜ ì—ˆ ë‹¤ê³  #Person1 # ì—ê²Œ ë§ í•˜ á†«ë‹¤ . Kate ë„ ë†€ë¼ì›Œí•˜ á†«ë‹¤ ....\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ¯ ê²°ë¡  ë° ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ¯ í˜•íƒœì†Œ ë¶„ì„ê¸° í™œìš© ì ìˆ˜ í–¥ìƒ ë¶„ì„ ê²°ê³¼\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½:\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ë°©ì‹                â”‚  Combined Score  â”‚  ë¹„ê³                               â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  1. ì›ë³¸             â”‚     14.28        â”‚  ê¸°ë³¸ ëª¨ë¸ ì¶œë ¥                     â”‚\n",
    "â”‚  2. ë„ì–´ì“°ê¸° êµì •     â”‚     14.99 (+0.71)â”‚  âœ… ì œì¶œ ì¶”ì²œ! (5% í–¥ìƒ)            â”‚\n",
    "â”‚  3. í˜•íƒœì†Œë¶„í• (ì–‘ìª½)  â”‚     34.84        â”‚  í‰ê°€ì„œë²„ê°€ í˜•íƒœì†Œ ë°©ì‹ì¼ ë•Œ ì°¸ê³     â”‚\n",
    "â”‚  4. í˜•íƒœì†Œë¶„í• (ì˜ˆì¸¡ë§Œ)â”‚      3.44        â”‚  âŒ í‰ê°€ì„œë²„ê°€ ì–´ì ˆ ë°©ì‹ì´ë©´ ì—­íš¨ê³¼  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ğŸ¯ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:\n",
    "\n",
    "1. ë„ì–´ì“°ê¸° êµì •ë§Œìœ¼ë¡œ +0.71ì  í–¥ìƒ ê°€ëŠ¥! \n",
    "   - ëª¨ë¸ì´ ìƒì„±í•œ í…ìŠ¤íŠ¸ì˜ ë„ì–´ì“°ê¸° ì˜¤ë¥˜ë¥¼ êµì •\n",
    "   - ì •ë‹µê³¼ ë” ì •í™•í•œ ë§¤ì¹­ ê°€ëŠ¥\n",
    "\n",
    "2. í‰ê°€ ì„œë²„ ë°©ì‹ì— ë”°ë¥¸ ì „ëµ:\n",
    "   - ì–´ì ˆ ê¸°ë°˜ ROUGE: ë„ì–´ì“°ê¸° êµì • í›„ ì œì¶œ\n",
    "   - í˜•íƒœì†Œ ê¸°ë°˜ ROUGE: í˜•íƒœì†Œ ë¶„í•  í›„ ì œì¶œ\n",
    "\n",
    "3. ëŒ€ë¶€ë¶„ì˜ í•œêµ­ì–´ NLP ëŒ€íšŒëŠ” í˜•íƒœì†Œ ê¸°ë°˜ ROUGE ì‚¬ìš©\n",
    "   - ì´ ê²½ìš° 34.84ì  ìˆ˜ì¤€ì˜ ë†’ì€ ì ìˆ˜ ê¸°ëŒ€ ê°€ëŠ¥\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# ğŸ“¤ ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± (ë„ì–´ì“°ê¸° êµì • ì ìš©)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“¤ ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ìµœì  ì„¤ì •(NB=7) íŒŒì¼ì— ë„ì–´ì“°ê¸° êµì • ì ìš©\n",
    "best_pred_df = pd.read_csv(\"./prediction_kobart_v2/kobart_v2_nb7.csv\")\n",
    "\n",
    "# ë„ì–´ì“°ê¸° êµì • ì ìš©\n",
    "best_pred_df['summary_spaced'] = best_pred_df['summary'].apply(postprocess_spacing)\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "submit_df = pd.DataFrame({\n",
    "    \"fname\": best_pred_df['fname'],\n",
    "    \"summary\": best_pred_df['summary_spaced']\n",
    "})\n",
    "submit_df.to_csv(\"./prediction_kobart_v2/submit_nb7_spaced.csv\", index=False)\n",
    "print(\"âœ… ì €ì¥: ./prediction_kobart_v2/submit_nb7_spaced.csv (ë„ì–´ì“°ê¸° êµì •)\")\n",
    "\n",
    "# í˜•íƒœì†Œ ë¶„í•  ë²„ì „ë„ ìƒì„± (í‰ê°€ ì„œë²„ê°€ í˜•íƒœì†Œ ë°©ì‹ì¼ ê²½ìš° ëŒ€ë¹„)\n",
    "best_pred_df['summary_morph'] = best_pred_df['summary'].apply(postprocess_morpheme_split)\n",
    "submit_morph_df = pd.DataFrame({\n",
    "    \"fname\": best_pred_df['fname'],\n",
    "    \"summary\": best_pred_df['summary_morph']\n",
    "})\n",
    "submit_morph_df.to_csv(\"./prediction_kobart_v2/submit_nb7_morpheme.csv\", index=False)\n",
    "print(\"âœ… ì €ì¥: ./prediction_kobart_v2/submit_nb7_morpheme.csv (í˜•íƒœì†Œ ë¶„í• )\")\n",
    "\n",
    "# ìƒ˜í”Œ ë¹„êµ\n",
    "print(\"\\nğŸ“ í›„ì²˜ë¦¬ ë¹„êµ ìƒ˜í”Œ:\")\n",
    "print(\"-\"*80)\n",
    "for i in range(3):\n",
    "    print(f\"\\n[{i+1}] {best_pred_df.iloc[i]['fname']}\")\n",
    "    print(f\"   ì›ë³¸:     {best_pred_df.iloc[i]['summary'][:80]}...\")\n",
    "    print(f\"   ë„ì–´ì“°ê¸°: {best_pred_df.iloc[i]['summary_spaced'][:80]}...\")\n",
    "    print(f\"   í˜•íƒœì†Œ:   {best_pred_df.iloc[i]['summary_morph'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f78aada2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¶„ì„\n",
      "================================================================================\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚                        ğŸ† ë¦¬ë”ë³´ë“œ ì œì¶œ ê²°ê³¼ ë¹„êµ                                â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  ëª¨ë¸                      â”‚  ì œì¶œ íŒŒì¼               â”‚  ë¦¬ë”ë³´ë“œ ì ìˆ˜           â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  digit82/kobart-summarization  â”‚  (ì´ì „ ìµœê³ ì )        â”‚  48.3168 âœ… (ìµœê³ )       â”‚\n",
      "â”‚  gogamza/kobart-base-v2        â”‚  submit_nb7_spaced.csvâ”‚  47.4829 (-0.83)        â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ“Œ ë¶„ì„:\n",
      "\n",
      "1ï¸âƒ£ ì ìˆ˜ ì°¨ì´ ì›ì¸ ë¶„ì„:\n",
      "   â€¢ digit82/kobart-summarization: ìš”ì•½ íƒœìŠ¤í¬ì— íŠ¹í™”ëœ fine-tuned ëª¨ë¸\n",
      "   â€¢ gogamza/kobart-base-v2: ë²”ìš© ì‚¬ì „í•™ìŠµ ëª¨ë¸ (ìš”ì•½ íŠ¹í™” X)\n",
      "   \n",
      "   â†’ ìš”ì•½ íŠ¹í™” ëª¨ë¸ì´ ì•½ 0.83ì  ë” ë†’ì€ ì„±ëŠ¥\n",
      "\n",
      "2ï¸âƒ£ Dev ì ìˆ˜ vs ë¦¬ë”ë³´ë“œ ì ìˆ˜ ì°¨ì´:\n",
      "   â€¢ Dev Combined: 14.28 (ì›ë³¸), 14.99 (ë„ì–´ì“°ê¸° êµì •)\n",
      "   â€¢ ë¦¬ë”ë³´ë“œ: 47.4829\n",
      "   \n",
      "   â†’ í‰ê°€ ì„œë²„ëŠ” í˜•íƒœì†Œ ê¸°ë°˜ ROUGEë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒìœ¼ë¡œ ì¶”ì •!\n",
      "   â†’ ìš°ë¦¬ Dev ì ìˆ˜ëŠ” ì–´ì ˆ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°ë˜ì–´ ë‚®ê²Œ ë‚˜ì˜´\n",
      "\n",
      "3ï¸âƒ£ ë„ì–´ì“°ê¸° êµì • íš¨ê³¼:\n",
      "   â€¢ Devì—ì„œ +0.71ì  í–¥ìƒ í™•ì¸\n",
      "   â€¢ ì‹¤ì œ ë¦¬ë”ë³´ë“œì—ì„œë„ íš¨ê³¼ê°€ ìˆì—ˆì„ ê²ƒìœ¼ë¡œ ì¶”ì •\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ¯ gogamza/kobart-base-v2 ì ìˆ˜ í–¥ìƒ ì „ëµ\n",
      "================================================================================\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚                           ğŸ“ˆ ì ìˆ˜ í–¥ìƒ ì „ëµ                                      â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "[ì „ëµ 1] ğŸ”„ í•™ìŠµ ì„¤ì • ìµœì í™” (ì˜ˆìƒ í–¥ìƒ: +0.5~1.0ì )\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "   â€¢ Learning Rate: 3e-5 â†’ 5e-5 ë˜ëŠ” 2e-5 ì‹œë„\n",
      "   â€¢ Epochs: 20 â†’ 30ìœ¼ë¡œ ì¦ê°€ (Early Stopping ìœ ì§€)\n",
      "   â€¢ Batch Size: 16 â†’ 8 (ë” ì„¸ë°€í•œ í•™ìŠµ)\n",
      "   â€¢ Label Smoothing: 0.1 â†’ 0.05 ë˜ëŠ” 0.15 ì‹œë„\n",
      "   â€¢ Warmup Ratio: 0.1 â†’ 0.06 ì‹œë„\n",
      "\n",
      "[ì „ëµ 2] ğŸ“ ë°ì´í„° ì¦ê°• (ì˜ˆìƒ í–¥ìƒ: +0.5~1.5ì )\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "   â€¢ ì—­ë²ˆì—­ (Back Translation): í•œâ†’ì˜â†’í•œ\n",
      "   â€¢ ë™ì˜ì–´ ì¹˜í™˜\n",
      "   â€¢ Train + Dev ë°ì´í„° í•©ì³ì„œ í•™ìŠµ (K-Fold)\n",
      "\n",
      "[ì „ëµ 3] ğŸ›ï¸ ìƒì„± íŒŒë¼ë¯¸í„° ì¶”ê°€ íŠœë‹ (ì˜ˆìƒ í–¥ìƒ: +0.2~0.5ì )\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "   â€¢ Beam í¬ê¸°: 7 â†’ 5, 9, 10 ì‹œë„\n",
      "   â€¢ Length Penalty: 1.0 â†’ 0.8, 1.2 ì‹œë„\n",
      "   â€¢ Diversity Penalty ì¶”ê°€ (ë‹¤ì–‘í•œ í›„ë³´ ìƒì„±)\n",
      "   â€¢ Top-k / Top-p ìƒ˜í”Œë§ ì‹œë„\n",
      "\n",
      "[ì „ëµ 4] ğŸ¤ ì•™ìƒë¸” (ì˜ˆìƒ í–¥ìƒ: +1.0~2.0ì ) â­ ì¶”ì²œ\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "   â€¢ digit82 ëª¨ë¸ + gogamza ëª¨ë¸ ì•™ìƒë¸”\n",
      "   â€¢ ë‘ ëª¨ë¸ì˜ ì¶œë ¥ì„ ì¡°í•©í•˜ì—¬ ìµœì¢… ê²°ê³¼ ìƒì„±\n",
      "   â€¢ ë°©ë²•: ë‹¤ìˆ˜ê²°, ê¸¸ì´ ê¸°ë°˜ ì„ íƒ, ROUGE ì ìˆ˜ ê¸°ë°˜ ì„ íƒ\n",
      "\n",
      "[ì „ëµ 5] ğŸ“ ì¶œë ¥ ê¸¸ì´ ìµœì í™” (ì˜ˆìƒ í–¥ìƒ: +0.2~0.5ì )\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "   â€¢ ì •ë‹µ ìš”ì•½ë¬¸ ê¸¸ì´ ë¶„í¬ ë¶„ì„\n",
      "   â€¢ Min/Max Length ì œì•½ ì¡°ê±´ ì¶”ê°€\n",
      "   â€¢ ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ìš”ì•½ í•„í„°ë§\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š ì •ë‹µ ìš”ì•½ë¬¸ ê¸¸ì´ ë¶„ì„\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Œ Train ì •ë‹µ ìš”ì•½ë¬¸ ê¸¸ì´:\n",
      "   í‰ê· : 85.8ì\n",
      "   ì¤‘ì•™ê°’: 80.0ì\n",
      "   ìµœì†Œ: 13ì, ìµœëŒ€: 376ì\n",
      "   25%: 61.0ì, 75%: 104.0ì\n",
      "\n",
      "ğŸ“Œ Dev ì •ë‹µ ìš”ì•½ë¬¸ ê¸¸ì´:\n",
      "   í‰ê· : 81.2ì\n",
      "   ì¤‘ì•™ê°’: 74.0ì\n",
      "\n",
      "ğŸ“Œ ëª¨ë¸ ì˜ˆì¸¡ ìš”ì•½ë¬¸ ê¸¸ì´ (NB=7):\n",
      "   í‰ê· : 91.9ì\n",
      "   ì¤‘ì•™ê°’: 88.0ì\n",
      "\n",
      "âš ï¸ ê¸¸ì´ ì°¨ì´: ì˜ˆì¸¡(91.9) vs ì •ë‹µ(85.8) = +6.2ì\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“Š gogamza/kobart-base-v2 vs KoBART ëª¨ë¸ ë¹„êµ ë¶„ì„\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¶„ì„\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        ğŸ† ë¦¬ë”ë³´ë“œ ì œì¶œ ê²°ê³¼ ë¹„êµ                                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  ëª¨ë¸                      â”‚  ì œì¶œ íŒŒì¼               â”‚  ë¦¬ë”ë³´ë“œ ì ìˆ˜           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  digit82/kobart-summarization  â”‚  (ì´ì „ ìµœê³ ì )        â”‚  48.3168 âœ… (ìµœê³ )       â”‚\n",
    "â”‚  gogamza/kobart-base-v2        â”‚  submit_nb7_spaced.csvâ”‚  47.4829 (-0.83)        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ğŸ“Œ ë¶„ì„:\n",
    "\n",
    "1ï¸âƒ£ ì ìˆ˜ ì°¨ì´ ì›ì¸ ë¶„ì„:\n",
    "   â€¢ digit82/kobart-summarization: ìš”ì•½ íƒœìŠ¤í¬ì— íŠ¹í™”ëœ fine-tuned ëª¨ë¸\n",
    "   â€¢ gogamza/kobart-base-v2: ë²”ìš© ì‚¬ì „í•™ìŠµ ëª¨ë¸ (ìš”ì•½ íŠ¹í™” X)\n",
    "   \n",
    "   â†’ ìš”ì•½ íŠ¹í™” ëª¨ë¸ì´ ì•½ 0.83ì  ë” ë†’ì€ ì„±ëŠ¥\n",
    "\n",
    "2ï¸âƒ£ Dev ì ìˆ˜ vs ë¦¬ë”ë³´ë“œ ì ìˆ˜ ì°¨ì´:\n",
    "   â€¢ Dev Combined: 14.28 (ì›ë³¸), 14.99 (ë„ì–´ì“°ê¸° êµì •)\n",
    "   â€¢ ë¦¬ë”ë³´ë“œ: 47.4829\n",
    "   \n",
    "   â†’ í‰ê°€ ì„œë²„ëŠ” í˜•íƒœì†Œ ê¸°ë°˜ ROUGEë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒìœ¼ë¡œ ì¶”ì •!\n",
    "   â†’ ìš°ë¦¬ Dev ì ìˆ˜ëŠ” ì–´ì ˆ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°ë˜ì–´ ë‚®ê²Œ ë‚˜ì˜´\n",
    "\n",
    "3ï¸âƒ£ ë„ì–´ì“°ê¸° êµì • íš¨ê³¼:\n",
    "   â€¢ Devì—ì„œ +0.71ì  í–¥ìƒ í™•ì¸\n",
    "   â€¢ ì‹¤ì œ ë¦¬ë”ë³´ë“œì—ì„œë„ íš¨ê³¼ê°€ ìˆì—ˆì„ ê²ƒìœ¼ë¡œ ì¶”ì •\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# ğŸ¯ ì ìˆ˜ í–¥ìƒ ì „ëµ ì œì‹œ\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ¯ gogamza/kobart-base-v2 ì ìˆ˜ í–¥ìƒ ì „ëµ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                           ğŸ“ˆ ì ìˆ˜ í–¥ìƒ ì „ëµ                                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "[ì „ëµ 1] ğŸ”„ í•™ìŠµ ì„¤ì • ìµœì í™” (ì˜ˆìƒ í–¥ìƒ: +0.5~1.0ì )\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "   â€¢ Learning Rate: 3e-5 â†’ 5e-5 ë˜ëŠ” 2e-5 ì‹œë„\n",
    "   â€¢ Epochs: 20 â†’ 30ìœ¼ë¡œ ì¦ê°€ (Early Stopping ìœ ì§€)\n",
    "   â€¢ Batch Size: 16 â†’ 8 (ë” ì„¸ë°€í•œ í•™ìŠµ)\n",
    "   â€¢ Label Smoothing: 0.1 â†’ 0.05 ë˜ëŠ” 0.15 ì‹œë„\n",
    "   â€¢ Warmup Ratio: 0.1 â†’ 0.06 ì‹œë„\n",
    "\n",
    "[ì „ëµ 2] ğŸ“ ë°ì´í„° ì¦ê°• (ì˜ˆìƒ í–¥ìƒ: +0.5~1.5ì )\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "   â€¢ ì—­ë²ˆì—­ (Back Translation): í•œâ†’ì˜â†’í•œ\n",
    "   â€¢ ë™ì˜ì–´ ì¹˜í™˜\n",
    "   â€¢ Train + Dev ë°ì´í„° í•©ì³ì„œ í•™ìŠµ (K-Fold)\n",
    "\n",
    "[ì „ëµ 3] ğŸ›ï¸ ìƒì„± íŒŒë¼ë¯¸í„° ì¶”ê°€ íŠœë‹ (ì˜ˆìƒ í–¥ìƒ: +0.2~0.5ì )\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "   â€¢ Beam í¬ê¸°: 7 â†’ 5, 9, 10 ì‹œë„\n",
    "   â€¢ Length Penalty: 1.0 â†’ 0.8, 1.2 ì‹œë„\n",
    "   â€¢ Diversity Penalty ì¶”ê°€ (ë‹¤ì–‘í•œ í›„ë³´ ìƒì„±)\n",
    "   â€¢ Top-k / Top-p ìƒ˜í”Œë§ ì‹œë„\n",
    "\n",
    "[ì „ëµ 4] ğŸ¤ ì•™ìƒë¸” (ì˜ˆìƒ í–¥ìƒ: +1.0~2.0ì ) â­ ì¶”ì²œ\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "   â€¢ digit82 ëª¨ë¸ + gogamza ëª¨ë¸ ì•™ìƒë¸”\n",
    "   â€¢ ë‘ ëª¨ë¸ì˜ ì¶œë ¥ì„ ì¡°í•©í•˜ì—¬ ìµœì¢… ê²°ê³¼ ìƒì„±\n",
    "   â€¢ ë°©ë²•: ë‹¤ìˆ˜ê²°, ê¸¸ì´ ê¸°ë°˜ ì„ íƒ, ROUGE ì ìˆ˜ ê¸°ë°˜ ì„ íƒ\n",
    "\n",
    "[ì „ëµ 5] ğŸ“ ì¶œë ¥ ê¸¸ì´ ìµœì í™” (ì˜ˆìƒ í–¥ìƒ: +0.2~0.5ì )\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "   â€¢ ì •ë‹µ ìš”ì•½ë¬¸ ê¸¸ì´ ë¶„í¬ ë¶„ì„\n",
    "   â€¢ Min/Max Length ì œì•½ ì¡°ê±´ ì¶”ê°€\n",
    "   â€¢ ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ìš”ì•½ í•„í„°ë§\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# ğŸ“Š ì •ë‹µ ìš”ì•½ë¬¸ ê¸¸ì´ ë¶„ì„\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š ì •ë‹µ ìš”ì•½ë¬¸ ê¸¸ì´ ë¶„ì„\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "dev_df_full = pd.read_csv('./data/dev.csv')\n",
    "\n",
    "train_lens = train_df['summary'].str.len()\n",
    "dev_lens = dev_df_full['summary'].str.len()\n",
    "\n",
    "print(f\"\\nğŸ“Œ Train ì •ë‹µ ìš”ì•½ë¬¸ ê¸¸ì´:\")\n",
    "print(f\"   í‰ê· : {train_lens.mean():.1f}ì\")\n",
    "print(f\"   ì¤‘ì•™ê°’: {train_lens.median():.1f}ì\")\n",
    "print(f\"   ìµœì†Œ: {train_lens.min()}ì, ìµœëŒ€: {train_lens.max()}ì\")\n",
    "print(f\"   25%: {train_lens.quantile(0.25):.1f}ì, 75%: {train_lens.quantile(0.75):.1f}ì\")\n",
    "\n",
    "print(f\"\\nğŸ“Œ Dev ì •ë‹µ ìš”ì•½ë¬¸ ê¸¸ì´:\")\n",
    "print(f\"   í‰ê· : {dev_lens.mean():.1f}ì\")\n",
    "print(f\"   ì¤‘ì•™ê°’: {dev_lens.median():.1f}ì\")\n",
    "\n",
    "# ì˜ˆì¸¡ ê¸¸ì´ì™€ ë¹„êµ\n",
    "pred_df = pd.read_csv('./prediction_kobart_v2/kobart_v2_nb7.csv')\n",
    "pred_lens = pred_df['summary'].str.len()\n",
    "\n",
    "print(f\"\\nğŸ“Œ ëª¨ë¸ ì˜ˆì¸¡ ìš”ì•½ë¬¸ ê¸¸ì´ (NB=7):\")\n",
    "print(f\"   í‰ê· : {pred_lens.mean():.1f}ì\")\n",
    "print(f\"   ì¤‘ì•™ê°’: {pred_lens.median():.1f}ì\")\n",
    "\n",
    "print(f\"\\nâš ï¸ ê¸¸ì´ ì°¨ì´: ì˜ˆì¸¡({pred_lens.mean():.1f}) vs ì •ë‹µ({train_lens.mean():.1f}) = {pred_lens.mean() - train_lens.mean():+.1f}ì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab2bb142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ”¬ ìƒì„± íŒŒë¼ë¯¸í„° ì¶”ê°€ íŠœë‹ ì‹¤í—˜\n",
      "================================================================================\n",
      "\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„°: 499ê°œ\n",
      "\n",
      "ì¶”ê°€ ì„¤ì • í…ŒìŠ¤íŠ¸ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config Test:  17%|â–ˆâ–‹        | 1/6 [00:46<03:53, 46.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… short_output: í‰ê· ê¸¸ì´=121.2ì â†’ ./prediction_kobart_v2/submit_short_output_spaced.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config Test:  33%|â–ˆâ–ˆâ–ˆâ–      | 2/6 [01:36<03:13, 48.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… shorter: í‰ê· ê¸¸ì´=145.0ì â†’ ./prediction_kobart_v2/submit_shorter_spaced.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config Test:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [02:09<02:05, 41.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… nb5: í‰ê· ê¸¸ì´=105.5ì â†’ ./prediction_kobart_v2/submit_nb5_spaced.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config Test:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [02:50<01:22, 41.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… balanced: í‰ê· ê¸¸ì´=122.4ì â†’ ./prediction_kobart_v2/submit_balanced_spaced.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config Test:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5/6 [03:34<00:42, 42.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… high_rp: í‰ê· ê¸¸ì´=104.0ì â†’ ./prediction_kobart_v2/submit_high_rp_spaced.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config Test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [04:20<00:00, 43.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… low_rp: í‰ê· ê¸¸ì´=104.4ì â†’ ./prediction_kobart_v2/submit_low_rp_spaced.csv\n",
      "\n",
      "================================================================================\n",
      "ğŸ“¤ ìƒì„±ëœ ì œì¶œ íŒŒì¼ë“¤\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ”¬ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„  ì‹¤í—˜ (ì¬í•™ìŠµ ë¶ˆí•„ìš”)\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ”¬ ìƒì„± íŒŒë¼ë¯¸í„° ì¶”ê°€ íŠœë‹ ì‹¤í—˜\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from transformers import BartForConditionalGeneration\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ (ì´ë¯¸ ë¡œë“œë¨)\n",
    "model_path = \"./results_kobart_v2/best_model\"\n",
    "\n",
    "# ìƒˆë¡œìš´ ìƒì„± ì„¤ì • í…ŒìŠ¤íŠ¸ (ê¸¸ì´ ì¡°ì ˆ + ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°)\n",
    "new_configs = [\n",
    "    # ê¸¸ì´ ì¡°ì ˆ ì‹¤í—˜\n",
    "    {\"nb\": 7, \"lp\": 0.9, \"rp\": 1.2, \"min_len\": 40, \"max_len\": 120, \"name\": \"short_output\"},\n",
    "    {\"nb\": 7, \"lp\": 0.85, \"rp\": 1.2, \"min_len\": 50, \"max_len\": 130, \"name\": \"shorter\"},\n",
    "    \n",
    "    # Beam í¬ê¸° + Length Penalty ì¡°í•©\n",
    "    {\"nb\": 5, \"lp\": 1.0, \"rp\": 1.2, \"min_len\": 30, \"max_len\": 150, \"name\": \"nb5\"},\n",
    "    {\"nb\": 6, \"lp\": 0.9, \"rp\": 1.15, \"min_len\": 40, \"max_len\": 130, \"name\": \"balanced\"},\n",
    "    \n",
    "    # Repetition Penalty ì¡°ì ˆ\n",
    "    {\"nb\": 7, \"lp\": 1.0, \"rp\": 1.3, \"min_len\": 30, \"max_len\": 150, \"name\": \"high_rp\"},\n",
    "    {\"nb\": 7, \"lp\": 1.0, \"rp\": 1.1, \"min_len\": 30, \"max_len\": 150, \"name\": \"low_rp\"},\n",
    "]\n",
    "\n",
    "def inference_with_config(model, tokenizer, test_dataset, config):\n",
    "    \"\"\"ì„¤ì •ë³„ ì¶”ë¡ \"\"\"\n",
    "    dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    summary_list = []\n",
    "    fname_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=config.get('max_len', 150),\n",
    "                min_length=config.get('min_len', 30),\n",
    "                num_beams=config['nb'],\n",
    "                length_penalty=config['lp'],\n",
    "                repetition_penalty=config['rp'],\n",
    "                no_repeat_ngram_size=3,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "            \n",
    "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "            cleaned = [postprocess_summary_v2(text) for text in decoded]\n",
    "            summary_list.extend(cleaned)\n",
    "            fname_list.extend(batch['ID'])\n",
    "    \n",
    "    return summary_list, fname_list\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„\n",
    "test_df, test_dataset = prepare_data_v2(CONF_V2, tokenizer_v2, is_train=False)\n",
    "\n",
    "print(f\"\\ní…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_dataset)}ê°œ\")\n",
    "print(\"\\nì¶”ê°€ ì„¤ì • í…ŒìŠ¤íŠ¸ ì‹œì‘...\")\n",
    "\n",
    "for cfg in tqdm(new_configs, desc=\"Config Test\"):\n",
    "    summaries, fnames = inference_with_config(model, tokenizer_v2, test_dataset, cfg)\n",
    "    \n",
    "    # ë„ì–´ì“°ê¸° êµì • ì ìš©\n",
    "    summaries_spaced = [postprocess_spacing(s) for s in summaries]\n",
    "    \n",
    "    # í†µê³„\n",
    "    avg_len = np.mean([len(s) for s in summaries_spaced])\n",
    "    \n",
    "    # ì €ì¥\n",
    "    result_df = pd.DataFrame({\"fname\": fnames, \"summary\": summaries_spaced})\n",
    "    save_path = f\"./prediction_kobart_v2/submit_{cfg['name']}_spaced.csv\"\n",
    "    result_df.to_csv(save_path, index=False)\n",
    "    \n",
    "    print(f\"\\nâœ… {cfg['name']}: í‰ê· ê¸¸ì´={avg_len:.1f}ì â†’ {save_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“¤ ìƒì„±ëœ ì œì¶œ íŒŒì¼ë“¤\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00e5507a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¯ ì •ë‹µ ê¸¸ì´(~85ì)ì— ë§ì¶˜ ìµœì í™” ì‹¤í—˜\n",
      "================================================================================\n",
      "\n",
      "ì •ë‹µ í‰ê·  ê¸¸ì´: 85.8ì\n",
      "ì§§ì€ ì¶œë ¥ì„ ìœ ë„í•˜ëŠ” ì„¤ì • í…ŒìŠ¤íŠ¸...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Length Opt:  20%|â–ˆâ–ˆ        | 1/5 [00:45<03:01, 45.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… lp0.7: í‰ê· ê¸¸ì´=100.9ì (ëª©í‘œ: ~85ì)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Length Opt:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:30<02:15, 45.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… lp0.6: í‰ê· ê¸¸ì´=100.3ì (ëª©í‘œ: ~85ì)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Length Opt:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:16<01:30, 45.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… lp0.8: í‰ê· ê¸¸ì´=101.7ì (ëª©í‘œ: ~85ì)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Length Opt:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [03:01<00:45, 45.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… max90: í‰ê· ê¸¸ì´=104.2ì (ëª©í‘œ: ~85ì)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Length Opt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:46<00:00, 45.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… max100: í‰ê· ê¸¸ì´=104.2ì (ëª©í‘œ: ~85ì)\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š ìµœì¢… ì œì¶œ íŒŒì¼ ëª©ë¡ ë° ì¶”ì²œ\n",
      "================================================================================\n",
      "                            íŒŒì¼       í‰ê· ê¸¸ì´      ê¸¸ì´ì°¨ì´\n",
      "         submit_nb7_spaced.csv  91.541082  5.741082\n",
      "       submit_lp0.6_spaced.csv 100.268537 14.468537\n",
      "       submit_lp0.7_spaced.csv 100.925852 15.125852\n",
      "       submit_lp0.8_spaced.csv 101.709419 15.909419\n",
      "     submit_high_rp_spaced.csv 103.995992 18.195992\n",
      "      submit_max100_spaced.csv 104.202405 18.402405\n",
      "       submit_max90_spaced.csv 104.202405 18.402405\n",
      "      submit_low_rp_spaced.csv 104.428858 18.628858\n",
      "         submit_nb5_spaced.csv 105.456914 19.656914\n",
      "       submit_nb7_morpheme.csv 116.597194 30.797194\n",
      "submit_short_output_spaced.csv 121.248497 35.448497\n",
      "    submit_balanced_spaced.csv 122.368737 36.568737\n",
      "     submit_shorter_spaced.csv 145.042084 59.242084\n",
      "\n",
      "ğŸ¯ ì •ë‹µ ê¸¸ì´(85.8ì)ì— ê°€ì¥ ê°€ê¹Œìš´ íŒŒì¼:\n",
      "   ì¶”ì²œ: submit_nb7_spaced.csv (í‰ê·  91.5ì, ì°¨ì´ 5.7ì)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ¯ ì •ë‹µ ê¸¸ì´ì— ë§ì¶˜ ìµœì í™” ì‹¤í—˜\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ¯ ì •ë‹µ ê¸¸ì´(~85ì)ì— ë§ì¶˜ ìµœì í™” ì‹¤í—˜\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ì •ë‹µ í‰ê·  ê¸¸ì´: 85.8ì â†’ ë” ì§§ê²Œ ìƒì„±í•˜ë„ë¡ ì¡°ì ˆ\n",
    "length_optimized_configs = [\n",
    "    # ê°•í•œ Length Penalty (ì§§ì€ ì¶œë ¥ ìœ ë„)\n",
    "    {\"nb\": 7, \"lp\": 0.7, \"rp\": 1.2, \"min_len\": 30, \"max_len\": 100, \"name\": \"lp0.7\"},\n",
    "    {\"nb\": 7, \"lp\": 0.6, \"rp\": 1.2, \"min_len\": 30, \"max_len\": 100, \"name\": \"lp0.6\"},\n",
    "    {\"nb\": 7, \"lp\": 0.8, \"rp\": 1.2, \"min_len\": 30, \"max_len\": 110, \"name\": \"lp0.8\"},\n",
    "    \n",
    "    # Max Length ì œí•œ\n",
    "    {\"nb\": 7, \"lp\": 1.0, \"rp\": 1.2, \"min_len\": 30, \"max_len\": 90, \"name\": \"max90\"},\n",
    "    {\"nb\": 7, \"lp\": 1.0, \"rp\": 1.2, \"min_len\": 30, \"max_len\": 100, \"name\": \"max100\"},\n",
    "]\n",
    "\n",
    "print(f\"\\nì •ë‹µ í‰ê·  ê¸¸ì´: {train_lens.mean():.1f}ì\")\n",
    "print(\"ì§§ì€ ì¶œë ¥ì„ ìœ ë„í•˜ëŠ” ì„¤ì • í…ŒìŠ¤íŠ¸...\\n\")\n",
    "\n",
    "for cfg in tqdm(length_optimized_configs, desc=\"Length Opt\"):\n",
    "    summaries, fnames = inference_with_config(model, tokenizer_v2, test_dataset, cfg)\n",
    "    \n",
    "    # ë„ì–´ì“°ê¸° êµì • ì ìš©\n",
    "    summaries_spaced = [postprocess_spacing(s) for s in summaries]\n",
    "    \n",
    "    # í†µê³„\n",
    "    avg_len = np.mean([len(s) for s in summaries_spaced])\n",
    "    \n",
    "    # ì €ì¥\n",
    "    result_df = pd.DataFrame({\"fname\": fnames, \"summary\": summaries_spaced})\n",
    "    save_path = f\"./prediction_kobart_v2/submit_{cfg['name']}_spaced.csv\"\n",
    "    result_df.to_csv(save_path, index=False)\n",
    "    \n",
    "    print(f\"âœ… {cfg['name']}: í‰ê· ê¸¸ì´={avg_len:.1f}ì (ëª©í‘œ: ~85ì)\")\n",
    "\n",
    "# ============================================================================\n",
    "# ğŸ“Š ìµœì¢… ì œì¶œ íŒŒì¼ ëª©ë¡\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š ìµœì¢… ì œì¶œ íŒŒì¼ ëª©ë¡ ë° ì¶”ì²œ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import glob\n",
    "files = glob.glob(\"./prediction_kobart_v2/submit_*.csv\")\n",
    "\n",
    "results_summary = []\n",
    "for f in sorted(files):\n",
    "    df = pd.read_csv(f)\n",
    "    avg_len = df['summary'].str.len().mean()\n",
    "    results_summary.append({\n",
    "        \"íŒŒì¼\": os.path.basename(f),\n",
    "        \"í‰ê· ê¸¸ì´\": avg_len,\n",
    "        \"ê¸¸ì´ì°¨ì´\": abs(avg_len - 85.8)\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(results_summary).sort_values(\"ê¸¸ì´ì°¨ì´\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nğŸ¯ ì •ë‹µ ê¸¸ì´(85.8ì)ì— ê°€ì¥ ê°€ê¹Œìš´ íŒŒì¼:\")\n",
    "best_file = summary_df.iloc[0]\n",
    "print(f\"   ì¶”ì²œ: {best_file['íŒŒì¼']} (í‰ê·  {best_file['í‰ê· ê¸¸ì´']:.1f}ì, ì°¨ì´ {best_file['ê¸¸ì´ì°¨ì´']:.1f}ì)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ff1a12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ† gogamza/kobart-base-v2 ìµœì¢… ë¶„ì„ ë° ì „ëµ\n",
      "================================================================================\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚                         ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë¶„ì„                                    â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "1ï¸âƒ£ í˜„ì¬ ìƒí™©:\n",
      "   â€¢ gogamza/kobart-base-v2 + NB=7 + ë„ì–´ì“°ê¸° êµì • â†’ 47.4829ì \n",
      "   â€¢ digit82/kobart-summarization ìµœê³ ì  â†’ 48.3168ì \n",
      "   â€¢ ì°¨ì´: -0.83ì \n",
      "\n",
      "2ï¸âƒ£ ì‹¤í—˜ ê²°ê³¼:\n",
      "   â€¢ Length Penalty, Max Length ì¡°ì ˆ â†’ ì˜¤íˆë ¤ ê¸¸ì´ ì¦ê°€\n",
      "   â€¢ ê¸°ì¡´ NB=7 + ë„ì–´ì“°ê¸° êµì •ì´ ê°€ì¥ ì¢‹ì€ ê¸¸ì´ (91.5ì)\n",
      "   â€¢ ì •ë‹µ ê¸¸ì´(85.8ì)ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ë¦¬í•  ê°€ëŠ¥ì„±\n",
      "\n",
      "3ï¸âƒ£ í•µì‹¬ ë°œê²¬:\n",
      "   â€¢ gogamza ëª¨ë¸ì€ ë²”ìš© ëª¨ë¸ì´ë¼ ìš”ì•½ íŠ¹í™” digit82ë³´ë‹¤ ì•½ê°„ ë’¤ì²˜ì§\n",
      "   â€¢ ìƒì„± íŒŒë¼ë¯¸í„° íŠœë‹ìœ¼ë¡œëŠ” í•œê³„ê°€ ìˆìŒ (ìµœëŒ€ +0.3~0.5ì  ì˜ˆìƒ)\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚                      ğŸ¯ ì ìˆ˜ 48ì  ëŒíŒŒë¥¼ ìœ„í•œ ì „ëµ                                â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "[ë°©ë²• 1] â­ ì•™ìƒë¸” (ê°€ì¥ ì¶”ì²œ, ì˜ˆìƒ +0.5~1.5ì )\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "   â€¢ digit82 ëª¨ë¸(48.31) + gogamza ëª¨ë¸(47.48) ì¡°í•©\n",
      "   â€¢ ê° ìƒ˜í”Œë³„ë¡œ ë” ë‚˜ì€ ê²°ê³¼ ì„ íƒ\n",
      "   â€¢ ë°©ë²•: ê¸¸ì´, ROUGE, ë‹¤ìˆ˜ê²° ê¸°ë°˜ ì„ íƒ\n",
      "\n",
      "[ë°©ë²• 2] ì¬í•™ìŠµ (ì‹œê°„ í•„ìš”, ì˜ˆìƒ +0.3~1.0ì )\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "   â€¢ Train + Dev í•©ì³ì„œ í•™ìŠµ\n",
      "   â€¢ Learning Rate íŠœë‹: 2e-5, 5e-5\n",
      "   â€¢ ë” ë§ì€ Epoch (30+)\n",
      "\n",
      "[ë°©ë²• 3] ë‹¤ë¥¸ ëª¨ë¸ ì‹œë„\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "   â€¢ ainize/kobart-news\n",
      "   â€¢ psyche/KoT5-summarization\n",
      "   â€¢ KETI-AIR/ke-t5-base\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚                         ğŸ“¤ ì œì¶œ ì¶”ì²œ ìˆœìœ„                                        â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "1ìˆœìœ„: submit_nb7_spaced.csv (í˜„ì¬ ìµœê³ , 47.48ì )\n",
      "       â†’ ì´ë¯¸ ì œì¶œë¨, ê¸°ì¤€ì \n",
      "\n",
      "2ìˆœìœ„: submit_lp0.6_spaced.csv (ê¸¸ì´ 100.3ì)\n",
      "       â†’ Length Penalty ë‚®ì¶¤, ë‹¤ì–‘ì„± ì¦ê°€\n",
      "\n",
      "3ìˆœìœ„: submit_high_rp_spaced.csv (ê¸¸ì´ 104.0ì)  \n",
      "       â†’ ë°˜ë³µ ì–µì œ ê°•í™”\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ“ ì œì¶œ ê°€ëŠ¥ íŒŒì¼ ì •ë¦¬\n",
      "================================================================================\n",
      "                       íŒŒì¼                   ì„¤ì •    ê¸¸ì´              ìƒíƒœ\n",
      "    submit_nb7_spaced.csv NB=7, LP=1.0, RP=1.2  91.5 âœ… ì œì¶œì™„ë£Œ (47.48ì )\n",
      "  submit_lp0.6_spaced.csv NB=7, LP=0.6, RP=1.2 100.3           â³ ë¯¸ì œì¶œ\n",
      "  submit_lp0.7_spaced.csv NB=7, LP=0.7, RP=1.2 100.9           â³ ë¯¸ì œì¶œ\n",
      "submit_high_rp_spaced.csv NB=7, LP=1.0, RP=1.3 104.0           â³ ë¯¸ì œì¶œ\n",
      "    submit_nb5_spaced.csv NB=5, LP=1.0, RP=1.2 105.5           â³ ë¯¸ì œì¶œ\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ† ìµœì¢… ë¶„ì„ ë° ì ìˆ˜ í–¥ìƒ ì „ëµ ìš”ì•½\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ† gogamza/kobart-base-v2 ìµœì¢… ë¶„ì„ ë° ì „ëµ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë¶„ì„                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "1ï¸âƒ£ í˜„ì¬ ìƒí™©:\n",
    "   â€¢ gogamza/kobart-base-v2 + NB=7 + ë„ì–´ì“°ê¸° êµì • â†’ 47.4829ì \n",
    "   â€¢ digit82/kobart-summarization ìµœê³ ì  â†’ 48.3168ì \n",
    "   â€¢ ì°¨ì´: -0.83ì \n",
    "\n",
    "2ï¸âƒ£ ì‹¤í—˜ ê²°ê³¼:\n",
    "   â€¢ Length Penalty, Max Length ì¡°ì ˆ â†’ ì˜¤íˆë ¤ ê¸¸ì´ ì¦ê°€\n",
    "   â€¢ ê¸°ì¡´ NB=7 + ë„ì–´ì“°ê¸° êµì •ì´ ê°€ì¥ ì¢‹ì€ ê¸¸ì´ (91.5ì)\n",
    "   â€¢ ì •ë‹µ ê¸¸ì´(85.8ì)ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ë¦¬í•  ê°€ëŠ¥ì„±\n",
    "\n",
    "3ï¸âƒ£ í•µì‹¬ ë°œê²¬:\n",
    "   â€¢ gogamza ëª¨ë¸ì€ ë²”ìš© ëª¨ë¸ì´ë¼ ìš”ì•½ íŠ¹í™” digit82ë³´ë‹¤ ì•½ê°„ ë’¤ì²˜ì§\n",
    "   â€¢ ìƒì„± íŒŒë¼ë¯¸í„° íŠœë‹ìœ¼ë¡œëŠ” í•œê³„ê°€ ìˆìŒ (ìµœëŒ€ +0.3~0.5ì  ì˜ˆìƒ)\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      ğŸ¯ ì ìˆ˜ 48ì  ëŒíŒŒë¥¼ ìœ„í•œ ì „ëµ                                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "[ë°©ë²• 1] â­ ì•™ìƒë¸” (ê°€ì¥ ì¶”ì²œ, ì˜ˆìƒ +0.5~1.5ì )\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "   â€¢ digit82 ëª¨ë¸(48.31) + gogamza ëª¨ë¸(47.48) ì¡°í•©\n",
    "   â€¢ ê° ìƒ˜í”Œë³„ë¡œ ë” ë‚˜ì€ ê²°ê³¼ ì„ íƒ\n",
    "   â€¢ ë°©ë²•: ê¸¸ì´, ROUGE, ë‹¤ìˆ˜ê²° ê¸°ë°˜ ì„ íƒ\n",
    "\n",
    "[ë°©ë²• 2] ì¬í•™ìŠµ (ì‹œê°„ í•„ìš”, ì˜ˆìƒ +0.3~1.0ì )\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "   â€¢ Train + Dev í•©ì³ì„œ í•™ìŠµ\n",
    "   â€¢ Learning Rate íŠœë‹: 2e-5, 5e-5\n",
    "   â€¢ ë” ë§ì€ Epoch (30+)\n",
    "\n",
    "[ë°©ë²• 3] ë‹¤ë¥¸ ëª¨ë¸ ì‹œë„\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "   â€¢ ainize/kobart-news\n",
    "   â€¢ psyche/KoT5-summarization\n",
    "   â€¢ KETI-AIR/ke-t5-base\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         ğŸ“¤ ì œì¶œ ì¶”ì²œ ìˆœìœ„                                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "1ìˆœìœ„: submit_nb7_spaced.csv (í˜„ì¬ ìµœê³ , 47.48ì )\n",
    "       â†’ ì´ë¯¸ ì œì¶œë¨, ê¸°ì¤€ì \n",
    "\n",
    "2ìˆœìœ„: submit_lp0.6_spaced.csv (ê¸¸ì´ 100.3ì)\n",
    "       â†’ Length Penalty ë‚®ì¶¤, ë‹¤ì–‘ì„± ì¦ê°€\n",
    "\n",
    "3ìˆœìœ„: submit_high_rp_spaced.csv (ê¸¸ì´ 104.0ì)  \n",
    "       â†’ ë°˜ë³µ ì–µì œ ê°•í™”\n",
    "\"\"\")\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ì •ë¦¬\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“ ì œì¶œ ê°€ëŠ¥ íŒŒì¼ ì •ë¦¬\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "submit_files = [\n",
    "    {\"íŒŒì¼\": \"submit_nb7_spaced.csv\", \"ì„¤ì •\": \"NB=7, LP=1.0, RP=1.2\", \"ê¸¸ì´\": 91.5, \"ìƒíƒœ\": \"âœ… ì œì¶œì™„ë£Œ (47.48ì )\"},\n",
    "    {\"íŒŒì¼\": \"submit_lp0.6_spaced.csv\", \"ì„¤ì •\": \"NB=7, LP=0.6, RP=1.2\", \"ê¸¸ì´\": 100.3, \"ìƒíƒœ\": \"â³ ë¯¸ì œì¶œ\"},\n",
    "    {\"íŒŒì¼\": \"submit_lp0.7_spaced.csv\", \"ì„¤ì •\": \"NB=7, LP=0.7, RP=1.2\", \"ê¸¸ì´\": 100.9, \"ìƒíƒœ\": \"â³ ë¯¸ì œì¶œ\"},\n",
    "    {\"íŒŒì¼\": \"submit_high_rp_spaced.csv\", \"ì„¤ì •\": \"NB=7, LP=1.0, RP=1.3\", \"ê¸¸ì´\": 104.0, \"ìƒíƒœ\": \"â³ ë¯¸ì œì¶œ\"},\n",
    "    {\"íŒŒì¼\": \"submit_nb5_spaced.csv\", \"ì„¤ì •\": \"NB=5, LP=1.0, RP=1.2\", \"ê¸¸ì´\": 105.5, \"ìƒíƒœ\": \"â³ ë¯¸ì œì¶œ\"},\n",
    "]\n",
    "\n",
    "submit_df = pd.DataFrame(submit_files)\n",
    "print(submit_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46c9df51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“Š í˜•íƒœì†Œ ê¸°ë°˜ ROUGE í‰ê°€ - ë¦¬ë”ë³´ë“œ ì ìˆ˜ ì˜ˆì¸¡\n",
      "================================================================================\n",
      "\n",
      "ğŸ”„ Dev ë°ì´í„°ë¡œ ê° ì„¤ì •ë³„ ì¶”ë¡  ë° í˜•íƒœì†Œ ROUGE ê³„ì‚°...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  10%|â–ˆ         | 1/10 [00:44<06:40, 44.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  nb7_spaced: Combined=35.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  20%|â–ˆâ–ˆ        | 2/10 [01:23<05:28, 41.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  baseline: Combined=34.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [02:07<04:58, 42.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  lp0.6: Combined=35.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [02:52<04:20, 43.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  lp0.7: Combined=35.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [03:37<03:39, 43.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  lp0.8: Combined=35.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [04:22<02:58, 44.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  high_rp: Combined=35.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [05:07<02:13, 44.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  low_rp: Combined=35.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [05:41<01:22, 41.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  nb5: Combined=35.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [06:31<00:43, 43.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  nb8: Combined=34.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [07:09<00:00, 42.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  lp1.05: Combined=34.90\n",
      "\n",
      "================================================================================\n",
      "ğŸ† í˜•íƒœì†Œ ê¸°ë°˜ ROUGE í‰ê°€ ê²°ê³¼ (ë¦¬ë”ë³´ë“œ ì ìˆ˜ ì˜ˆì¸¡)\n",
      "================================================================================\n",
      "        ì„¤ì •  NB   LP  RP ë„ì–´ì“°ê¸°        R1        R2        RL  Combined      í‰ê· ê¸¸ì´\n",
      "nb7_spaced   7 1.00 1.2    âœ“ 46.411488 20.146193 41.208652 35.922111 89.507014\n",
      "   high_rp   7 1.00 1.3    âœ“ 46.371636 20.069741 41.235997 35.892458 89.328657\n",
      "       nb5   5 1.00 1.2    âœ“ 46.321245 19.971367 41.178579 35.823730 90.727455\n",
      "     lp0.8   7 0.80 1.2    âœ“ 46.260322 19.954152 40.989104 35.734526 85.723447\n",
      "    low_rp   7 1.00 1.1    âœ“ 46.218109 19.882945 41.039014 35.713356 89.611222\n",
      "     lp0.7   7 0.70 1.2    âœ“ 46.239279 19.954511 40.938671 35.710820 84.595190\n",
      "     lp0.6   7 0.60 1.2    âœ“ 46.104784 19.795224 40.818680 35.572896 83.242485\n",
      "    lp1.05   6 1.05 1.2      45.335445 18.906606 40.460190 34.900747 92.260521\n",
      "  baseline   6 1.00 1.2      45.266892 18.873988 40.342738 34.827872 91.386774\n",
      "       nb8   8 1.00 1.2      45.249505 18.664839 40.181341 34.698562 90.220441\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š ì˜ˆì¸¡ ì ìˆ˜ vs ì‹¤ì œ ë¦¬ë”ë³´ë“œ ì ìˆ˜\n",
      "================================================================================\n",
      "\n",
      "ì°¸ê³ : submit_nb7_spaced.csv ì‹¤ì œ ë¦¬ë”ë³´ë“œ ì ìˆ˜ = 47.4829\n",
      "\n",
      "í˜•íƒœì†Œ ê¸°ë°˜ ì˜ˆì¸¡ ì ìˆ˜ì™€ ì‹¤ì œ ì ìˆ˜ì˜ ìƒê´€ê´€ê³„ë¥¼ í†µí•´\n",
      "ë‹¤ë¥¸ íŒŒì¼ë“¤ì˜ ì˜ˆìƒ ë¦¬ë”ë³´ë“œ ì ìˆ˜ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“Š í˜•íƒœì†Œ ê¸°ë°˜ ROUGEë¡œ ëª¨ë“  ì œì¶œ íŒŒì¼ í‰ê°€ (ë¦¬ë”ë³´ë“œ ì ìˆ˜ ì˜ˆì¸¡)\n",
    "# ============================================================================\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“Š í˜•íƒœì†Œ ê¸°ë°˜ ROUGE í‰ê°€ - ë¦¬ë”ë³´ë“œ ì ìˆ˜ ì˜ˆì¸¡\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Dev ë°ì´í„°ì—ì„œ ëª¨ë¸ ì˜ˆì¸¡ í•„ìš” â†’ Test ë°ì´í„°ëŠ” ì •ë‹µ ì—†ìŒ\n",
    "# ë”°ë¼ì„œ Dev ë°ì´í„°ë¡œ ë™ì¼ ì„¤ì • ì¶”ë¡  í›„ í˜•íƒœì†Œ ê¸°ë°˜ ROUGE ê³„ì‚°\n",
    "\n",
    "# Dev ë°ì´í„° ë¡œë“œ\n",
    "dev_df = pd.read_csv('./data/dev.csv')\n",
    "dev_references = dev_df['summary'].tolist()\n",
    "\n",
    "# í˜•íƒœì†Œ ë¶„ì„ í•¨ìˆ˜ (kiwi ì‚¬ìš©)\n",
    "def tokenize_morpheme(text):\n",
    "    \"\"\"í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë¶„í• \"\"\"\n",
    "    if not text or not isinstance(text, str) or not text.strip():\n",
    "        return \"empty\"\n",
    "    try:\n",
    "        tokens = kiwi.tokenize(text)\n",
    "        return ' '.join([t.form for t in tokens])\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "def compute_morpheme_rouge(predictions, references):\n",
    "    \"\"\"í˜•íƒœì†Œ ê¸°ë°˜ ROUGE ì ìˆ˜ ê³„ì‚°\"\"\"\n",
    "    rouge = Rouge()\n",
    "    \n",
    "    # í˜•íƒœì†Œ ë¶„í• \n",
    "    pred_morph = [tokenize_morpheme(str(p)) for p in predictions]\n",
    "    ref_morph = [tokenize_morpheme(str(r)) for r in references]\n",
    "    \n",
    "    # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬\n",
    "    pred_morph = [p if p.strip() else \"empty\" for p in pred_morph]\n",
    "    ref_morph = [r if r.strip() else \"empty\" for r in ref_morph]\n",
    "    \n",
    "    try:\n",
    "        scores = rouge.get_scores(pred_morph, ref_morph, avg=True)\n",
    "        r1 = scores['rouge-1']['f'] * 100\n",
    "        r2 = scores['rouge-2']['f'] * 100\n",
    "        rl = scores['rouge-l']['f'] * 100\n",
    "        combined = (r1 + r2 + rl) / 3\n",
    "        return r1, r2, rl, combined\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return 0, 0, 0, 0\n",
    "\n",
    "# ============================================================================\n",
    "# Dev ë°ì´í„°ë¡œ ê° ì„¤ì •ë³„ ì¶”ë¡  í›„ í˜•íƒœì†Œ ROUGE ê³„ì‚°\n",
    "# ============================================================================\n",
    "print(\"\\nğŸ”„ Dev ë°ì´í„°ë¡œ ê° ì„¤ì •ë³„ ì¶”ë¡  ë° í˜•íƒœì†Œ ROUGE ê³„ì‚°...\")\n",
    "\n",
    "from transformers import BartForConditionalGeneration\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "model_path = \"./results_kobart_v2/best_model\"\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Dev ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "dev_df_clean = dev_df.copy()\n",
    "dev_df_clean['dialogue'] = dev_df_clean['dialogue'].apply(clean_text)\n",
    "dev_df_clean['summary'] = dev_df_clean['summary'].apply(clean_text)\n",
    "\n",
    "bos = CONF_V2['tokenizer']['bos_token']\n",
    "enc_dev = tokenizer_v2(\n",
    "    dev_df_clean['dialogue'].tolist(), return_tensors=\"pt\",\n",
    "    padding=True, truncation=True, max_length=CONF_V2['tokenizer']['encoder_max_len']\n",
    ")\n",
    "dec_in_dev = tokenizer_v2(\n",
    "    [bos] * len(dev_df_clean), return_tensors=\"pt\",\n",
    "    padding=True, truncation=True, max_length=CONF_V2['tokenizer']['decoder_max_len']\n",
    ")\n",
    "\n",
    "dev_dataset = KoBARTv2Dataset(enc_dev, dec_in_dev, is_test=True, ids=dev_df_clean['fname'].tolist())\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸í•  ì„¤ì •ë“¤ (ì œì¶œ íŒŒì¼ê³¼ ë™ì¼í•œ ì„¤ì •)\n",
    "eval_configs = [\n",
    "    {\"name\": \"nb7_spaced\", \"nb\": 7, \"lp\": 1.0, \"rp\": 1.2, \"spacing\": True},\n",
    "    {\"name\": \"baseline\", \"nb\": 6, \"lp\": 1.0, \"rp\": 1.2, \"spacing\": False},\n",
    "    {\"name\": \"lp0.6\", \"nb\": 7, \"lp\": 0.6, \"rp\": 1.2, \"spacing\": True},\n",
    "    {\"name\": \"lp0.7\", \"nb\": 7, \"lp\": 0.7, \"rp\": 1.2, \"spacing\": True},\n",
    "    {\"name\": \"lp0.8\", \"nb\": 7, \"lp\": 0.8, \"rp\": 1.2, \"spacing\": True},\n",
    "    {\"name\": \"high_rp\", \"nb\": 7, \"lp\": 1.0, \"rp\": 1.3, \"spacing\": True},\n",
    "    {\"name\": \"low_rp\", \"nb\": 7, \"lp\": 1.0, \"rp\": 1.1, \"spacing\": True},\n",
    "    {\"name\": \"nb5\", \"nb\": 5, \"lp\": 1.0, \"rp\": 1.2, \"spacing\": True},\n",
    "    {\"name\": \"nb8\", \"nb\": 8, \"lp\": 1.0, \"rp\": 1.2, \"spacing\": False},\n",
    "    {\"name\": \"lp1.05\", \"nb\": 6, \"lp\": 1.05, \"rp\": 1.2, \"spacing\": False},\n",
    "]\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for cfg in tqdm(eval_configs, desc=\"Evaluating\"):\n",
    "    # ì¶”ë¡ \n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=150,\n",
    "                num_beams=cfg['nb'],\n",
    "                length_penalty=cfg['lp'],\n",
    "                repetition_penalty=cfg['rp'],\n",
    "                no_repeat_ngram_size=3,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "            \n",
    "            decoded = tokenizer_v2.batch_decode(outputs, skip_special_tokens=False)\n",
    "            cleaned = [postprocess_summary_v2(text) for text in decoded]\n",
    "            predictions.extend(cleaned)\n",
    "    \n",
    "    # ë„ì–´ì“°ê¸° êµì • ì ìš© ì—¬ë¶€\n",
    "    if cfg.get('spacing', False):\n",
    "        predictions = [postprocess_spacing(p) for p in predictions]\n",
    "    \n",
    "    # í˜•íƒœì†Œ ê¸°ë°˜ ROUGE ê³„ì‚°\n",
    "    r1, r2, rl, combined = compute_morpheme_rouge(predictions, dev_references)\n",
    "    \n",
    "    # í‰ê·  ê¸¸ì´\n",
    "    avg_len = np.mean([len(str(p)) for p in predictions])\n",
    "    \n",
    "    evaluation_results.append({\n",
    "        \"ì„¤ì •\": cfg['name'],\n",
    "        \"NB\": cfg['nb'],\n",
    "        \"LP\": cfg['lp'],\n",
    "        \"RP\": cfg['rp'],\n",
    "        \"ë„ì–´ì“°ê¸°\": \"âœ“\" if cfg.get('spacing') else \"\",\n",
    "        \"R1\": r1,\n",
    "        \"R2\": r2,\n",
    "        \"RL\": rl,\n",
    "        \"Combined\": combined,\n",
    "        \"í‰ê· ê¸¸ì´\": avg_len\n",
    "    })\n",
    "    \n",
    "    print(f\"  {cfg['name']}: Combined={combined:.2f}\")\n",
    "\n",
    "# ê²°ê³¼ ì •ë¦¬\n",
    "eval_df = pd.DataFrame(evaluation_results)\n",
    "eval_df = eval_df.sort_values('Combined', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ† í˜•íƒœì†Œ ê¸°ë°˜ ROUGE í‰ê°€ ê²°ê³¼ (ë¦¬ë”ë³´ë“œ ì ìˆ˜ ì˜ˆì¸¡)\")\n",
    "print(\"=\"*80)\n",
    "print(eval_df.to_string(index=False))\n",
    "\n",
    "# ì‹¤ì œ ë¦¬ë”ë³´ë“œ ì ìˆ˜ì™€ ë¹„êµ\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š ì˜ˆì¸¡ ì ìˆ˜ vs ì‹¤ì œ ë¦¬ë”ë³´ë“œ ì ìˆ˜\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "ì°¸ê³ : submit_nb7_spaced.csv ì‹¤ì œ ë¦¬ë”ë³´ë“œ ì ìˆ˜ = 47.4829\n",
    "\n",
    "í˜•íƒœì†Œ ê¸°ë°˜ ì˜ˆì¸¡ ì ìˆ˜ì™€ ì‹¤ì œ ì ìˆ˜ì˜ ìƒê´€ê´€ê³„ë¥¼ í†µí•´\n",
    "ë‹¤ë¥¸ íŒŒì¼ë“¤ì˜ ì˜ˆìƒ ë¦¬ë”ë³´ë“œ ì ìˆ˜ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "483c0b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¯ ë¦¬ë”ë³´ë“œ ì ìˆ˜ ì˜ˆì¸¡ ë° ì œì¶œ ì¶”ì²œ\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Œ ìŠ¤ì¼€ì¼ë§ ë¹„ìœ¨: 47.4829 / 35.92 = 1.3219\n",
      "\n",
      "================================================================================\n",
      "ğŸ† ì˜ˆìƒ ë¦¬ë”ë³´ë“œ ì ìˆ˜ ìˆœìœ„\n",
      "================================================================================\n",
      "âœ… nb7_spaced   | í˜•íƒœì†Œ:35.92 | ì˜ˆìƒ:47.49 (+0.00) | ê¸¸ì´:89.5ì\n",
      "   high_rp      | í˜•íƒœì†Œ:35.89 | ì˜ˆìƒ:47.45 (-0.04) | ê¸¸ì´:89.3ì\n",
      "   nb5          | í˜•íƒœì†Œ:35.82 | ì˜ˆìƒ:47.36 (-0.13) | ê¸¸ì´:90.7ì\n",
      "   lp0.8        | í˜•íƒœì†Œ:35.73 | ì˜ˆìƒ:47.24 (-0.25) | ê¸¸ì´:85.7ì\n",
      "   low_rp       | í˜•íƒœì†Œ:35.71 | ì˜ˆìƒ:47.21 (-0.27) | ê¸¸ì´:89.6ì\n",
      "   lp0.7        | í˜•íƒœì†Œ:35.71 | ì˜ˆìƒ:47.21 (-0.28) | ê¸¸ì´:84.6ì\n",
      "   lp0.6        | í˜•íƒœì†Œ:35.57 | ì˜ˆìƒ:47.02 (-0.46) | ê¸¸ì´:83.2ì\n",
      "   lp1.05       | í˜•íƒœì†Œ:34.90 | ì˜ˆìƒ:46.14 (-1.35) | ê¸¸ì´:92.3ì\n",
      "   baseline     | í˜•íƒœì†Œ:34.83 | ì˜ˆìƒ:46.04 (-1.44) | ê¸¸ì´:91.4ì\n",
      "   nb8          | í˜•íƒœì†Œ:34.70 | ì˜ˆìƒ:45.87 (-1.61) | ê¸¸ì´:90.2ì\n",
      "\n",
      "================================================================================\n",
      "ğŸ“¤ ì œì¶œ ì¶”ì²œ\n",
      "================================================================================\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚                        ğŸ¯ ì œì¶œ ì¶”ì²œ ìˆœìœ„                                         â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "1ìˆœìœ„: âœ… submit_nb7_spaced.csv (í˜„ì¬ ìµœê³ , ì‹¤ì œ 47.48ì )\n",
      "       â†’ ì´ë¯¸ ì œì¶œë¨, ê¸°ì¤€ì \n",
      "\n",
      "2ìˆœìœ„: ğŸ”¥ submit_high_rp_spaced.csv (ì˜ˆìƒ 47.44ì )\n",
      "       â†’ RP=1.3, ë°˜ë³µ ì–µì œ ê°•í™”\n",
      "       â†’ ê±°ì˜ ë™ì¼í•˜ì§€ë§Œ ì•½ê°„ ë‚®ì„ ìˆ˜ ìˆìŒ\n",
      "\n",
      "3ìˆœìœ„: ğŸ”¥ submit_nb5_spaced.csv (ì˜ˆìƒ 47.35ì )\n",
      "       â†’ NB=5, ë” ë¹ ë¥¸ ì¶”ë¡ \n",
      "       â†’ ì•½ê°„ ë‚®ì„ ìˆ˜ ìˆìŒ\n",
      "\n",
      "4ìˆœìœ„: submit_lp0.8_spaced.csv (ì˜ˆìƒ 47.23ì )\n",
      "       â†’ LP=0.8, ë” ì§§ì€ ì¶œë ¥ (85.7ì - ì •ë‹µê¸¸ì´ì™€ ê°€ì¥ ê·¼ì ‘!)\n",
      "\n",
      "âš ï¸ ì°¸ê³ : í˜•íƒœì†Œ ROUGE ì ìˆ˜ ì°¨ì´ê°€ ì‘ì•„ì„œ (35.57~35.92)\n",
      "   ì‹¤ì œ ë¦¬ë”ë³´ë“œì—ì„œë„ í° ì°¨ì´ê°€ ì—†ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.\n",
      "   \n",
      "ğŸ¯ ê¸¸ì´ê°€ ì •ë‹µ(85.8ì)ì— ê°€ì¥ ê°€ê¹Œìš´ lp0.8ì´ ì˜ì™¸ë¡œ ì¢‹ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ“ ì‹¤ì œ ì œì¶œ íŒŒì¼ í™•ì¸\n",
      "================================================================================\n",
      "âœ“ submit_nb7_spaced.csv: 499ê°œ, í‰ê·  91.5ì\n",
      "âœ“ submit_high_rp_spaced.csv: 499ê°œ, í‰ê·  104.0ì\n",
      "âœ“ submit_nb5_spaced.csv: 499ê°œ, í‰ê·  105.5ì\n",
      "âœ“ submit_lp0.8_spaced.csv: 499ê°œ, í‰ê·  101.7ì\n",
      "âœ“ submit_lp0.7_spaced.csv: 499ê°œ, í‰ê·  100.9ì\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ¯ ë¦¬ë”ë³´ë“œ ì ìˆ˜ ì˜ˆì¸¡ ë° ì œì¶œ ì¶”ì²œ\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ¯ ë¦¬ë”ë³´ë“œ ì ìˆ˜ ì˜ˆì¸¡ ë° ì œì¶œ ì¶”ì²œ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ì‹¤ì œ ë¦¬ë”ë³´ë“œ ì ìˆ˜ (ê¸°ì¤€ì )\n",
    "actual_score = 47.4829  # submit_nb7_spaced.csv\n",
    "morpheme_score = 35.92  # nb7_spaced í˜•íƒœì†Œ ROUGE\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ë§ ë¹„ìœ¨ ê³„ì‚°\n",
    "scale_ratio = actual_score / morpheme_score\n",
    "print(f\"\\nğŸ“Œ ìŠ¤ì¼€ì¼ë§ ë¹„ìœ¨: {actual_score} / {morpheme_score:.2f} = {scale_ratio:.4f}\")\n",
    "\n",
    "# ì˜ˆìƒ ë¦¬ë”ë³´ë“œ ì ìˆ˜ ê³„ì‚°\n",
    "eval_df_sorted = eval_df.copy()\n",
    "eval_df_sorted['ì˜ˆìƒ_ë¦¬ë”ë³´ë“œì ìˆ˜'] = eval_df_sorted['Combined'] * scale_ratio\n",
    "eval_df_sorted['í˜„ì¬ìµœê³ ëŒ€ë¹„'] = eval_df_sorted['ì˜ˆìƒ_ë¦¬ë”ë³´ë“œì ìˆ˜'] - actual_score\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ† ì˜ˆìƒ ë¦¬ë”ë³´ë“œ ì ìˆ˜ ìˆœìœ„\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "result_display = eval_df_sorted[['ì„¤ì •', 'NB', 'LP', 'RP', 'ë„ì–´ì“°ê¸°', 'Combined', 'ì˜ˆìƒ_ë¦¬ë”ë³´ë“œì ìˆ˜', 'í˜„ì¬ìµœê³ ëŒ€ë¹„', 'í‰ê· ê¸¸ì´']].copy()\n",
    "result_display = result_display.sort_values('ì˜ˆìƒ_ë¦¬ë”ë³´ë“œì ìˆ˜', ascending=False)\n",
    "\n",
    "for idx, row in result_display.iterrows():\n",
    "    marker = \"âœ…\" if row['ì„¤ì •'] == 'nb7_spaced' else \"  \"\n",
    "    diff = f\"+{row['í˜„ì¬ìµœê³ ëŒ€ë¹„']:.2f}\" if row['í˜„ì¬ìµœê³ ëŒ€ë¹„'] > 0 else f\"{row['í˜„ì¬ìµœê³ ëŒ€ë¹„']:.2f}\"\n",
    "    print(f\"{marker} {row['ì„¤ì •']:12} | í˜•íƒœì†Œ:{row['Combined']:.2f} | ì˜ˆìƒ:{row['ì˜ˆìƒ_ë¦¬ë”ë³´ë“œì ìˆ˜']:.2f} ({diff}) | ê¸¸ì´:{row['í‰ê· ê¸¸ì´']:.1f}ì\")\n",
    "\n",
    "# ì œì¶œ ì¶”ì²œ\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“¤ ì œì¶œ ì¶”ì²œ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        ğŸ¯ ì œì¶œ ì¶”ì²œ ìˆœìœ„                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "1ìˆœìœ„: âœ… submit_nb7_spaced.csv (í˜„ì¬ ìµœê³ , ì‹¤ì œ 47.48ì )\n",
    "       â†’ ì´ë¯¸ ì œì¶œë¨, ê¸°ì¤€ì \n",
    "\n",
    "2ìˆœìœ„: ğŸ”¥ submit_high_rp_spaced.csv (ì˜ˆìƒ 47.44ì )\n",
    "       â†’ RP=1.3, ë°˜ë³µ ì–µì œ ê°•í™”\n",
    "       â†’ ê±°ì˜ ë™ì¼í•˜ì§€ë§Œ ì•½ê°„ ë‚®ì„ ìˆ˜ ìˆìŒ\n",
    "\n",
    "3ìˆœìœ„: ğŸ”¥ submit_nb5_spaced.csv (ì˜ˆìƒ 47.35ì )\n",
    "       â†’ NB=5, ë” ë¹ ë¥¸ ì¶”ë¡ \n",
    "       â†’ ì•½ê°„ ë‚®ì„ ìˆ˜ ìˆìŒ\n",
    "\n",
    "4ìˆœìœ„: submit_lp0.8_spaced.csv (ì˜ˆìƒ 47.23ì )\n",
    "       â†’ LP=0.8, ë” ì§§ì€ ì¶œë ¥ (85.7ì - ì •ë‹µê¸¸ì´ì™€ ê°€ì¥ ê·¼ì ‘!)\n",
    "\n",
    "âš ï¸ ì°¸ê³ : í˜•íƒœì†Œ ROUGE ì ìˆ˜ ì°¨ì´ê°€ ì‘ì•„ì„œ (35.57~35.92)\n",
    "   ì‹¤ì œ ë¦¬ë”ë³´ë“œì—ì„œë„ í° ì°¨ì´ê°€ ì—†ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.\n",
    "   \n",
    "ğŸ¯ ê¸¸ì´ê°€ ì •ë‹µ(85.8ì)ì— ê°€ì¥ ê°€ê¹Œìš´ lp0.8ì´ ì˜ì™¸ë¡œ ì¢‹ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
    "\"\"\")\n",
    "\n",
    "# ìµœì¢… ì¶”ì²œ íŒŒì¼\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“ ì‹¤ì œ ì œì¶œ íŒŒì¼ í™•ì¸\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "files_to_check = [\n",
    "    \"submit_nb7_spaced.csv\",\n",
    "    \"submit_high_rp_spaced.csv\", \n",
    "    \"submit_nb5_spaced.csv\",\n",
    "    \"submit_lp0.8_spaced.csv\",\n",
    "    \"submit_lp0.7_spaced.csv\",\n",
    "]\n",
    "\n",
    "for f in files_to_check:\n",
    "    path = f\"./prediction_kobart_v2/{f}\"\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        avg_len = df['summary'].str.len().mean()\n",
    "        print(f\"âœ“ {f}: {len(df)}ê°œ, í‰ê·  {avg_len:.1f}ì\")\n",
    "    else:\n",
    "        print(f\"âœ— {f}: íŒŒì¼ ì—†ìŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fe12894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ê°œì„  + ë°ì´í„° ì¦ê°• ì „ëµ\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š ì›ë³¸ ë°ì´í„°:\n",
      "   Train: 12457ê°œ\n",
      "   Dev: 499ê°œ\n",
      "\n",
      "================================================================================\n",
      "ğŸ”§ ë°ì´í„° ì¦ê°• ê¸°ë²• ì •ì˜\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Œ ë°ì´í„° ì¦ê°• ê¸°ë²•:\n",
      "   1. í™”ì ë²ˆí˜¸ ìŠ¤ì™‘ (Person1 â†” Person2)\n",
      "   2. ë™ì˜ì–´ ì¹˜í™˜\n",
      "   3. Train + Dev í•©ì¹˜ê¸° (ìµœì¢… í•™ìŠµìš©)\n",
      "   \n",
      "âš ï¸ ì£¼ì˜: ëŒ€í™” ìš”ì•½ íƒœìŠ¤í¬ì—ì„œ ê³¼ë„í•œ ì¦ê°•ì€ ì˜¤íˆë ¤ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥\n",
      "   â†’ ë³´ìˆ˜ì ì¸ ì¦ê°• ì „ëµ ì‚¬ìš©\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ê°œì„  + ë°ì´í„° ì¦ê°•ì„ í†µí•œ ì ìˆ˜ í–¥ìƒ ì „ëµ\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ê°œì„  + ë°ì´í„° ì¦ê°• ì „ëµ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ì›ë³¸ ë°ì´í„° ë¡œë“œ\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "dev_df = pd.read_csv('./data/dev.csv')\n",
    "\n",
    "print(f\"\\nğŸ“Š ì›ë³¸ ë°ì´í„°:\")\n",
    "print(f\"   Train: {len(train_df)}ê°œ\")\n",
    "print(f\"   Dev: {len(dev_df)}ê°œ\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1ï¸âƒ£ í–¥ìƒëœ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "# ============================================================================\n",
    "def advanced_clean_text(text):\n",
    "    \"\"\"í–¥ìƒëœ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. ë‹¤ì¤‘ ê³µë°± ì •ë¦¬\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 2. ë‹¤ì¤‘ ë§ˆì¹¨í‘œ ì •ë¦¬\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    \n",
    "    # 3. ë¬¸ì¥ ë¶€í˜¸ ì•ë’¤ ê³µë°± ì •ë¦¬\n",
    "    text = re.sub(r'\\s+([.,!?])', r'\\1', text)\n",
    "    \n",
    "    # 4. í™”ì í† í° í˜•ì‹ í†µì¼\n",
    "    text = re.sub(r'#\\s*Person\\s*(\\d+)\\s*#', r'#Person\\1#', text)\n",
    "    \n",
    "    # 5. ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬\n",
    "    text = re.sub(r'[\"\"'']', '\"', text)  # ë”°ì˜´í‘œ í†µì¼\n",
    "    \n",
    "    # 6. Kiwië¡œ ë„ì–´ì“°ê¸° êµì • (ì„ íƒì )\n",
    "    # text = kiwi.space(text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_dialogue(dialogue):\n",
    "    \"\"\"ëŒ€í™” ì „ì²˜ë¦¬ - êµ¬ì¡°í™”\"\"\"\n",
    "    if not dialogue:\n",
    "        return \"\"\n",
    "    \n",
    "    # í™”ìë³„ ë°œí™” ë¶„ë¦¬ ë° ì •ë¦¬\n",
    "    lines = dialogue.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            # í™”ì í† í° í˜•ì‹ í†µì¼\n",
    "            line = re.sub(r'#\\s*Person\\s*(\\d+)\\s*#', r'#Person\\1#', line)\n",
    "            cleaned_lines.append(line)\n",
    "    \n",
    "    return ' '.join(cleaned_lines)\n",
    "\n",
    "# ============================================================================\n",
    "# 2ï¸âƒ£ ë°ì´í„° ì¦ê°• í•¨ìˆ˜ë“¤\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ”§ ë°ì´í„° ì¦ê°• ê¸°ë²• ì •ì˜\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 2-1. í™”ì ë²ˆí˜¸ ìŠ¤ì™‘ (Person1 â†” Person2)\n",
    "def augment_swap_speakers(dialogue, summary):\n",
    "    \"\"\"í™”ì ë²ˆí˜¸ ìŠ¤ì™‘\"\"\"\n",
    "    swap_map = {\n",
    "        '#Person1#': '#Person_TEMP#',\n",
    "        '#Person2#': '#Person1#',\n",
    "        '#Person_TEMP#': '#Person2#',\n",
    "    }\n",
    "    \n",
    "    new_dialogue = dialogue\n",
    "    new_summary = summary\n",
    "    \n",
    "    for old, new in swap_map.items():\n",
    "        new_dialogue = new_dialogue.replace(old, new)\n",
    "        new_summary = new_summary.replace(old, new)\n",
    "    \n",
    "    return new_dialogue, new_summary\n",
    "\n",
    "# 2-2. ë¬¸ì¥ ìˆœì„œ ì…”í”Œ (ëŒ€í™” ë‚´ì—ì„œ)\n",
    "def augment_shuffle_sentences(dialogue, summary, shuffle_prob=0.3):\n",
    "    \"\"\"ëŒ€í™” ë‚´ ë¬¸ì¥ ìˆœì„œ ë¶€ë¶„ ì…”í”Œ (ì£¼ì˜: ì˜ë¯¸ ë³´ì¡´ í•„ìš”)\"\"\"\n",
    "    # ìš”ì•½ì€ ìœ ì§€, ëŒ€í™”ë§Œ ì•½ê°„ ë³€í˜•\n",
    "    lines = dialogue.split('\\n')\n",
    "    \n",
    "    if len(lines) > 3 and random.random() < shuffle_prob:\n",
    "        # ì—°ì†ëœ 2ê°œ ë¬¸ì¥ë§Œ ìŠ¤ì™‘ (ì˜ë¯¸ í¬ê²Œ í›¼ì† ì•ˆë¨)\n",
    "        idx = random.randint(1, len(lines) - 2)\n",
    "        lines[idx], lines[idx + 1] = lines[idx + 1], lines[idx]\n",
    "    \n",
    "    return '\\n'.join(lines), summary\n",
    "\n",
    "# 2-3. ë™ì˜ì–´ ì¹˜í™˜ (ê°„ë‹¨í•œ ë²„ì „)\n",
    "SYNONYM_MAP = {\n",
    "    'ì¢‹ì•„ìš”': ['ì•Œê² ì–´ìš”', 'ë„¤', 'ê·¸ë˜ìš”', 'ì¢‹ìŠµë‹ˆë‹¤'],\n",
    "    'ê°ì‚¬í•©ë‹ˆë‹¤': ['ê³ ë§™ìŠµë‹ˆë‹¤', 'ê³ ë§ˆì›Œìš”', 'ê°ì‚¬í•´ìš”'],\n",
    "    'ì•ˆë…•í•˜ì„¸ìš”': ['ì•ˆë…•í•˜ì„¸ìš”', 'ë°˜ê°‘ìŠµë‹ˆë‹¤'],\n",
    "    'ë„¤': ['ì˜ˆ', 'ê·¸ë ‡ìŠµë‹ˆë‹¤', 'ë§ì•„ìš”'],\n",
    "    'ì•„ë‹ˆìš”': ['ì•„ë‹ˆì˜¤', 'ê·¸ë ‡ì§€ ì•Šì•„ìš”'],\n",
    "}\n",
    "\n",
    "def augment_synonym_replacement(text, replace_prob=0.2):\n",
    "    \"\"\"ë™ì˜ì–´ ì¹˜í™˜\"\"\"\n",
    "    for word, synonyms in SYNONYM_MAP.items():\n",
    "        if word in text and random.random() < replace_prob:\n",
    "            text = text.replace(word, random.choice(synonyms), 1)\n",
    "    return text\n",
    "\n",
    "# 2-4. Train + Dev í•©ì¹˜ê¸°\n",
    "def merge_train_dev(train_df, dev_df):\n",
    "    \"\"\"Trainê³¼ Dev ë°ì´í„° í•©ì¹˜ê¸° (ìµœì¢… í•™ìŠµìš©)\"\"\"\n",
    "    merged = pd.concat([train_df, dev_df], ignore_index=True)\n",
    "    return merged\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“Œ ë°ì´í„° ì¦ê°• ê¸°ë²•:\n",
    "   1. í™”ì ë²ˆí˜¸ ìŠ¤ì™‘ (Person1 â†” Person2)\n",
    "   2. ë™ì˜ì–´ ì¹˜í™˜\n",
    "   3. Train + Dev í•©ì¹˜ê¸° (ìµœì¢… í•™ìŠµìš©)\n",
    "   \n",
    "âš ï¸ ì£¼ì˜: ëŒ€í™” ìš”ì•½ íƒœìŠ¤í¬ì—ì„œ ê³¼ë„í•œ ì¦ê°•ì€ ì˜¤íˆë ¤ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥\n",
    "   â†’ ë³´ìˆ˜ì ì¸ ì¦ê°• ì „ëµ ì‚¬ìš©\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d86269fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“¦ ì¦ê°•ëœ ë°ì´í„°ì…‹ ìƒì„±\n",
      "================================================================================\n",
      "\n",
      "ğŸ”„ ë°ì´í„° ì¦ê°• ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12457/12457 [00:01<00:00, 9034.76it/s]\n",
      "Processing Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12457/12457 [00:01<00:00, 9034.76it/s]\n",
      "Processing Dev: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [00:00<00:00, 9847.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ì¦ê°•ëœ ë°ì´í„°ì…‹:\n",
      "   ì „ì²´: 16625ê°œ\n",
      "   ì›ë³¸ Train: 12457ê°œ\n",
      "   í™”ì ìŠ¤ì™‘: 3669ê°œ\n",
      "   Dev ì¶”ê°€: 499ê°œ\n",
      "\n",
      "ğŸ“ˆ ë°ì´í„° ì¦ê°€ìœ¨: 12457 â†’ 16625 (33.5% ì¦ê°€)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3ï¸âƒ£ ì¦ê°•ëœ ë°ì´í„°ì…‹ ìƒì„±\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“¦ ì¦ê°•ëœ ë°ì´í„°ì…‹ ìƒì„±\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def create_augmented_dataset(train_df, dev_df, augment_ratio=0.3):\n",
    "    \"\"\"\n",
    "    ì¦ê°•ëœ ë°ì´í„°ì…‹ ìƒì„±\n",
    "    \n",
    "    ì „ëµ:\n",
    "    1. ì›ë³¸ Train ë°ì´í„° ìœ ì§€\n",
    "    2. í™”ì ìŠ¤ì™‘ ì¦ê°• (30%)\n",
    "    3. Train + Dev í•©ì¹˜ê¸° (ìµœì¢… í•™ìŠµ)\n",
    "    \"\"\"\n",
    "    augmented_data = []\n",
    "    \n",
    "    # ì›ë³¸ ë°ì´í„° ì¶”ê°€ (ì „ì²˜ë¦¬ ì ìš©)\n",
    "    for idx, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Processing Train\"):\n",
    "        dialogue = advanced_clean_text(row['dialogue'])\n",
    "        summary = advanced_clean_text(row['summary'])\n",
    "        \n",
    "        augmented_data.append({\n",
    "            'fname': row['fname'],\n",
    "            'dialogue': dialogue,\n",
    "            'summary': summary,\n",
    "            'topic': row.get('topic', ''),\n",
    "            'augment_type': 'original'\n",
    "        })\n",
    "        \n",
    "        # í™”ì ìŠ¤ì™‘ ì¦ê°• (ì¼ë¶€ë§Œ)\n",
    "        if random.random() < augment_ratio:\n",
    "            swap_dialogue, swap_summary = augment_swap_speakers(dialogue, summary)\n",
    "            augmented_data.append({\n",
    "                'fname': f\"{row['fname']}_swap\",\n",
    "                'dialogue': swap_dialogue,\n",
    "                'summary': swap_summary,\n",
    "                'topic': row.get('topic', ''),\n",
    "                'augment_type': 'speaker_swap'\n",
    "            })\n",
    "    \n",
    "    # Dev ë°ì´í„° ì¶”ê°€ (ì „ì²˜ë¦¬ ì ìš©)\n",
    "    for idx, row in tqdm(dev_df.iterrows(), total=len(dev_df), desc=\"Processing Dev\"):\n",
    "        dialogue = advanced_clean_text(row['dialogue'])\n",
    "        summary = advanced_clean_text(row['summary'])\n",
    "        \n",
    "        augmented_data.append({\n",
    "            'fname': row['fname'],\n",
    "            'dialogue': dialogue,\n",
    "            'summary': summary,\n",
    "            'topic': row.get('topic', ''),\n",
    "            'augment_type': 'dev_data'\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(augmented_data)\n",
    "\n",
    "# ì¦ê°• ë°ì´í„°ì…‹ ìƒì„±\n",
    "print(\"\\nğŸ”„ ë°ì´í„° ì¦ê°• ì¤‘...\")\n",
    "augmented_df = create_augmented_dataset(train_df, dev_df, augment_ratio=0.3)\n",
    "\n",
    "print(f\"\\nğŸ“Š ì¦ê°•ëœ ë°ì´í„°ì…‹:\")\n",
    "print(f\"   ì „ì²´: {len(augmented_df)}ê°œ\")\n",
    "print(f\"   ì›ë³¸ Train: {len(augmented_df[augmented_df['augment_type'] == 'original'])}ê°œ\")\n",
    "print(f\"   í™”ì ìŠ¤ì™‘: {len(augmented_df[augmented_df['augment_type'] == 'speaker_swap'])}ê°œ\")\n",
    "print(f\"   Dev ì¶”ê°€: {len(augmented_df[augmented_df['augment_type'] == 'dev_data'])}ê°œ\")\n",
    "\n",
    "# ì¦ê°• ë¹„ìœ¨\n",
    "original_size = len(train_df)\n",
    "augmented_size = len(augmented_df)\n",
    "print(f\"\\nğŸ“ˆ ë°ì´í„° ì¦ê°€ìœ¨: {original_size} â†’ {augmented_size} ({(augmented_size/original_size - 1)*100:.1f}% ì¦ê°€)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "613df9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸš€ ì¦ê°•ëœ ë°ì´í„°ë¡œ gogamza/kobart-base-v2 ì¬í•™ìŠµ\n",
      "================================================================================\n",
      "\n",
      ">>> í† í¬ë‚˜ì´ì € ë¡œë“œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> ì¶”ê°€ëœ íŠ¹ìˆ˜ í† í°: 10ê°œ\n",
      "âœ… Configuration ë° Dataset í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 4ï¸âƒ£ ì¦ê°•ëœ ë°ì´í„°ë¡œ ëª¨ë¸ ì¬í•™ìŠµ\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸš€ ì¦ê°•ëœ ë°ì´í„°ë¡œ gogamza/kobart-base-v2 ì¬í•™ìŠµ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BartForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "# ìƒˆë¡œìš´ Configuration\n",
    "CONF_AUG = {\n",
    "    \"general\": {\n",
    "        \"data_path\": \"./data/\",\n",
    "        \"model_name\": \"gogamza/kobart-base-v2\",\n",
    "        \"output_dir\": \"./results_kobart_v2_augmented\",\n",
    "        \"seed\": 42\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "        \"encoder_max_len\": 512,\n",
    "        \"decoder_max_len\": 150,\n",
    "        \"special_tokens\": [\n",
    "            '#Person1#', '#Person2#', '#Person3#', '#Person4#', \n",
    "            '#Person5#', '#Person6#', '#Person7#', \n",
    "            '#PhoneNumber#', '#Address#', '#PassportNumber#',\n",
    "        ]\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"num_train_epochs\": 15,  # ë°ì´í„°ê°€ ë” ë§ìœ¼ë¯€ë¡œ epoch ì¡°ì •\n",
    "        \"learning_rate\": 2e-5,   # ì•½ê°„ ë‚®ì¶¤\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"per_device_eval_batch_size\": 32,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lr_scheduler_type\": 'cosine_with_restarts',\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"gradient_accumulation_steps\": 2,\n",
    "        \"evaluation_strategy\": 'epoch',\n",
    "        \"save_strategy\": 'epoch',\n",
    "        \"save_total_limit\": 3,\n",
    "        \"fp16\": True,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"metric_for_best_model\": \"combined_score\",\n",
    "        \"greater_is_better\": True,\n",
    "        \"logging_dir\": \"./logs_kobart_v2_augmented\",\n",
    "        \"logging_steps\": 50,\n",
    "        \"predict_with_generate\": True,\n",
    "        \"generation_max_length\": 150,\n",
    "        \"early_stopping_patience\": 3,  # ë¹ ë¥¸ early stopping\n",
    "        \"report_to\": \"none\",\n",
    "        \"label_smoothing_factor\": 0.1,\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"result_path\": \"./prediction_kobart_v2_augmented/\",\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"early_stopping\": True,\n",
    "        \"generate_max_length\": 150,\n",
    "        \"num_beams\": 7,\n",
    "        \"length_penalty\": 1.0,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"batch_size\": 32,\n",
    "    },\n",
    "}\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "print(\"\\n>>> í† í¬ë‚˜ì´ì € ë¡œë“œ\")\n",
    "tokenizer_aug = AutoTokenizer.from_pretrained(CONF_AUG['general']['model_name'])\n",
    "\n",
    "# íŠ¹ìˆ˜ í† í° ì¶”ê°€\n",
    "special_tokens_dict = {'additional_special_tokens': CONF_AUG['tokenizer']['special_tokens']}\n",
    "num_added = tokenizer_aug.add_special_tokens(special_tokens_dict)\n",
    "print(f\">>> ì¶”ê°€ëœ íŠ¹ìˆ˜ í† í°: {num_added}ê°œ\")\n",
    "\n",
    "CONF_AUG['tokenizer']['bos_token'] = tokenizer_aug.bos_token\n",
    "CONF_AUG['tokenizer']['eos_token'] = tokenizer_aug.eos_token\n",
    "\n",
    "# ============================================================================\n",
    "# ì¦ê°•ëœ ë°ì´í„°ì…‹ í´ë˜ìŠ¤\n",
    "# ============================================================================\n",
    "class AugmentedDataset(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, labels=None, is_test=False, ids=None):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.labels = labels\n",
    "        self.is_test = is_test\n",
    "        self.ids = ids\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
    "        \n",
    "        if 'input_ids' in self.decoder_input:\n",
    "            item['decoder_input_ids'] = self.decoder_input['input_ids'][idx].clone().detach()\n",
    "            item['decoder_attention_mask'] = self.decoder_input['attention_mask'][idx].clone().detach()\n",
    "\n",
    "        if not self.is_test and self.labels is not None:\n",
    "            labels = self.labels['input_ids'][idx].clone().detach()\n",
    "            labels[labels == tokenizer_aug.pad_token_id] = -100\n",
    "            item['labels'] = labels\n",
    "            \n",
    "        if self.ids is not None:\n",
    "            item['ID'] = self.ids[idx]\n",
    "            \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoder_input['input_ids'])\n",
    "\n",
    "print(\"âœ… Configuration ë° Dataset í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a30b54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¯ ì¦ê°•ëœ ë°ì´í„°ë¡œ í•™ìŠµ ì‹œì‘\n",
      "================================================================================\n",
      ">>> Train: 16126ê°œ (ì›ë³¸ + ì¦ê°•)\n",
      ">>> Val: 499ê°œ\n",
      ">>> í† í¬ë‚˜ì´ì§•...\n",
      ">>> Train Dataset: 16126\n",
      ">>> Val Dataset: 499\n",
      "\n",
      ">>> ëª¨ë¸ ë¡œë“œ: gogamza/kobart-base-v2\n",
      ">>> Train Dataset: 16126\n",
      ">>> Val Dataset: 499\n",
      "\n",
      ">>> ëª¨ë¸ ë¡œë“œ: gogamza/kobart-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> ëª¨ë¸ íŒŒë¼ë¯¸í„°: 123,867,648\n",
      "\n",
      ">>> Training Start...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6048' max='7560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6048/7560 35:11 < 08:47, 2.86 it/s, Epoch 12/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "      <th>Combined Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.220800</td>\n",
       "      <td>3.155904</td>\n",
       "      <td>0.299015</td>\n",
       "      <td>0.114798</td>\n",
       "      <td>0.275451</td>\n",
       "      <td>0.229755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.975200</td>\n",
       "      <td>2.991809</td>\n",
       "      <td>0.318984</td>\n",
       "      <td>0.132542</td>\n",
       "      <td>0.295845</td>\n",
       "      <td>0.249124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.792700</td>\n",
       "      <td>2.948306</td>\n",
       "      <td>0.314710</td>\n",
       "      <td>0.129933</td>\n",
       "      <td>0.290915</td>\n",
       "      <td>0.245186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.661900</td>\n",
       "      <td>2.921874</td>\n",
       "      <td>0.331654</td>\n",
       "      <td>0.145361</td>\n",
       "      <td>0.306323</td>\n",
       "      <td>0.261113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.529700</td>\n",
       "      <td>2.925705</td>\n",
       "      <td>0.335952</td>\n",
       "      <td>0.145201</td>\n",
       "      <td>0.308357</td>\n",
       "      <td>0.263170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.428700</td>\n",
       "      <td>2.938806</td>\n",
       "      <td>0.331669</td>\n",
       "      <td>0.145588</td>\n",
       "      <td>0.308501</td>\n",
       "      <td>0.261919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.343000</td>\n",
       "      <td>2.971051</td>\n",
       "      <td>0.323906</td>\n",
       "      <td>0.133884</td>\n",
       "      <td>0.300111</td>\n",
       "      <td>0.252633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.253800</td>\n",
       "      <td>2.957273</td>\n",
       "      <td>0.336976</td>\n",
       "      <td>0.142700</td>\n",
       "      <td>0.310852</td>\n",
       "      <td>0.263509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.201100</td>\n",
       "      <td>2.980606</td>\n",
       "      <td>0.335071</td>\n",
       "      <td>0.148061</td>\n",
       "      <td>0.310230</td>\n",
       "      <td>0.264454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.161900</td>\n",
       "      <td>2.996882</td>\n",
       "      <td>0.334135</td>\n",
       "      <td>0.143152</td>\n",
       "      <td>0.307588</td>\n",
       "      <td>0.261625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.117600</td>\n",
       "      <td>3.009844</td>\n",
       "      <td>0.330547</td>\n",
       "      <td>0.139767</td>\n",
       "      <td>0.303855</td>\n",
       "      <td>0.258056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.092600</td>\n",
       "      <td>3.021217</td>\n",
       "      <td>0.333183</td>\n",
       "      <td>0.141172</td>\n",
       "      <td>0.306211</td>\n",
       "      <td>0.260189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Training Finished!\n",
      ">>> Best Model: ./results_kobart_v2_augmented/best_model\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5ï¸âƒ£ í•™ìŠµ ì‹¤í–‰\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ¯ ì¦ê°•ëœ ë°ì´í„°ë¡œ í•™ìŠµ ì‹œì‘\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def compute_metrics_aug(eval_pred):\n",
    "    \"\"\"ROUGE ë©”íŠ¸ë¦­ ê³„ì‚°\"\"\"\n",
    "    rouge = Rouge()\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    \n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer_aug.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer_aug.pad_token_id)\n",
    "    \n",
    "    decoded_preds = tokenizer_aug.batch_decode(predictions.tolist(), skip_special_tokens=False)\n",
    "    decoded_labels = tokenizer_aug.batch_decode(labels.tolist(), skip_special_tokens=False)\n",
    "    \n",
    "    remove_tokens = [tokenizer_aug.bos_token, tokenizer_aug.eos_token, tokenizer_aug.pad_token, '<usr>']\n",
    "    \n",
    "    def clean_for_rouge(text_list):\n",
    "        cleaned = []\n",
    "        for text in text_list:\n",
    "            for token in remove_tokens:\n",
    "                if token:\n",
    "                    text = text.replace(token, \"\").strip()\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            cleaned.append(text if text else \"empty\")\n",
    "        return cleaned\n",
    "\n",
    "    decoded_preds = clean_for_rouge(decoded_preds)\n",
    "    decoded_labels = clean_for_rouge(decoded_labels)\n",
    "    \n",
    "    try:\n",
    "        results = rouge.get_scores(decoded_preds, decoded_labels, avg=True)\n",
    "        r1 = results[\"rouge-1\"][\"f\"]\n",
    "        r2 = results[\"rouge-2\"][\"f\"]\n",
    "        rl = results[\"rouge-l\"][\"f\"]\n",
    "        combined_score = (r1 + r2 + rl) / 3\n",
    "        \n",
    "        return {\n",
    "            \"rouge-1\": r1,\n",
    "            \"rouge-2\": r2,\n",
    "            \"rouge-l\": rl,\n",
    "            \"combined_score\": combined_score\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Metrics Error: {e}\")\n",
    "        return {\"combined_score\": 0.0}\n",
    "\n",
    "\n",
    "def train_with_augmented_data():\n",
    "    \"\"\"ì¦ê°•ëœ ë°ì´í„°ë¡œ í•™ìŠµ\"\"\"\n",
    "    \n",
    "    # ë°ì´í„° ì¤€ë¹„\n",
    "    bos = CONF_AUG['tokenizer']['bos_token']\n",
    "    eos = CONF_AUG['tokenizer']['eos_token']\n",
    "    \n",
    "    # Train: ì¦ê°•ëœ ë°ì´í„° (Dev í¬í•¨ X - ê²€ì¦ìš©ìœ¼ë¡œ ë¶„ë¦¬)\n",
    "    train_aug = augmented_df[augmented_df['augment_type'] != 'dev_data'].copy()\n",
    "    train_aug = train_aug.sample(frac=1, random_state=42).reset_index(drop=True)  # ì…”í”Œ\n",
    "    \n",
    "    # Validation: ì›ë³¸ Dev ì¤‘ ì¼ë¶€ (ê²€ì¦ìš©ìœ¼ë¡œ ë³„ë„ ë¶„ë¦¬)\n",
    "    # ì‹¤ì œ ìµœì¢… í•™ìŠµ ì‹œì—ëŠ” Devë„ í•™ìŠµì— í¬í•¨í•˜ê³ , ë³„ë„ validation set ì—†ì´ í•™ìŠµ ê°€ëŠ¥\n",
    "    val_df = dev_df.copy()\n",
    "    val_df['dialogue'] = val_df['dialogue'].apply(advanced_clean_text)\n",
    "    val_df['summary'] = val_df['summary'].apply(advanced_clean_text)\n",
    "    \n",
    "    print(f\">>> Train: {len(train_aug)}ê°œ (ì›ë³¸ + ì¦ê°•)\")\n",
    "    print(f\">>> Val: {len(val_df)}ê°œ\")\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì¦ˆ\n",
    "    print(\">>> í† í¬ë‚˜ì´ì§•...\")\n",
    "    enc_train = tokenizer_aug(\n",
    "        train_aug['dialogue'].tolist(), return_tensors=\"pt\",\n",
    "        padding=True, truncation=True, max_length=CONF_AUG['tokenizer']['encoder_max_len']\n",
    "    )\n",
    "    dec_in_train = tokenizer_aug(\n",
    "        [bos + s for s in train_aug['summary'].tolist()], return_tensors=\"pt\",\n",
    "        padding=True, truncation=True, max_length=CONF_AUG['tokenizer']['decoder_max_len']\n",
    "    )\n",
    "    dec_out_train = tokenizer_aug(\n",
    "        [s + eos for s in train_aug['summary'].tolist()], return_tensors=\"pt\",\n",
    "        padding=True, truncation=True, max_length=CONF_AUG['tokenizer']['decoder_max_len']\n",
    "    )\n",
    "    \n",
    "    train_dataset = AugmentedDataset(enc_train, dec_in_train, dec_out_train)\n",
    "    \n",
    "    enc_val = tokenizer_aug(\n",
    "        val_df['dialogue'].tolist(), return_tensors=\"pt\",\n",
    "        padding=True, truncation=True, max_length=CONF_AUG['tokenizer']['encoder_max_len']\n",
    "    )\n",
    "    dec_in_val = tokenizer_aug(\n",
    "        [bos + s for s in val_df['summary'].tolist()], return_tensors=\"pt\",\n",
    "        padding=True, truncation=True, max_length=CONF_AUG['tokenizer']['decoder_max_len']\n",
    "    )\n",
    "    dec_out_val = tokenizer_aug(\n",
    "        [s + eos for s in val_df['summary'].tolist()], return_tensors=\"pt\",\n",
    "        padding=True, truncation=True, max_length=CONF_AUG['tokenizer']['decoder_max_len']\n",
    "    )\n",
    "    \n",
    "    val_dataset = AugmentedDataset(enc_val, dec_in_val, dec_out_val)\n",
    "    \n",
    "    print(f\">>> Train Dataset: {len(train_dataset)}\")\n",
    "    print(f\">>> Val Dataset: {len(val_dataset)}\")\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    print(f\"\\n>>> ëª¨ë¸ ë¡œë“œ: {CONF_AUG['general']['model_name']}\")\n",
    "    model_aug = BartForConditionalGeneration.from_pretrained(CONF_AUG['general']['model_name'])\n",
    "    model_aug.resize_token_embeddings(len(tokenizer_aug))\n",
    "    model_aug.to(device)\n",
    "    \n",
    "    print(f\">>> ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model_aug.parameters()):,}\")\n",
    "    \n",
    "    # Training Arguments\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=CONF_AUG['general']['output_dir'],\n",
    "        overwrite_output_dir=CONF_AUG['training']['overwrite_output_dir'],\n",
    "        num_train_epochs=CONF_AUG['training']['num_train_epochs'],\n",
    "        learning_rate=CONF_AUG['training']['learning_rate'],\n",
    "        per_device_train_batch_size=CONF_AUG['training']['per_device_train_batch_size'],\n",
    "        per_device_eval_batch_size=CONF_AUG['training']['per_device_eval_batch_size'],\n",
    "        warmup_ratio=CONF_AUG['training']['warmup_ratio'],\n",
    "        weight_decay=CONF_AUG['training']['weight_decay'],\n",
    "        lr_scheduler_type=CONF_AUG['training']['lr_scheduler_type'],\n",
    "        optim=CONF_AUG['training']['optim'],\n",
    "        gradient_accumulation_steps=CONF_AUG['training']['gradient_accumulation_steps'],\n",
    "        evaluation_strategy=CONF_AUG['training']['evaluation_strategy'],\n",
    "        save_strategy=CONF_AUG['training']['save_strategy'],\n",
    "        save_total_limit=CONF_AUG['training']['save_total_limit'],\n",
    "        fp16=CONF_AUG['training']['fp16'],\n",
    "        load_best_model_at_end=CONF_AUG['training']['load_best_model_at_end'],\n",
    "        metric_for_best_model=CONF_AUG['training']['metric_for_best_model'],\n",
    "        greater_is_better=CONF_AUG['training']['greater_is_better'],\n",
    "        logging_dir=CONF_AUG['training']['logging_dir'],\n",
    "        logging_steps=CONF_AUG['training']['logging_steps'],\n",
    "        predict_with_generate=CONF_AUG['training']['predict_with_generate'],\n",
    "        generation_max_length=CONF_AUG['training']['generation_max_length'],\n",
    "        report_to=CONF_AUG['training']['report_to'],\n",
    "        seed=CONF_AUG['general']['seed'],\n",
    "        label_smoothing_factor=CONF_AUG['training']['label_smoothing_factor'],\n",
    "    )\n",
    "    \n",
    "    # Early Stopping\n",
    "    early_stopping = EarlyStoppingCallback(\n",
    "        early_stopping_patience=CONF_AUG['training']['early_stopping_patience']\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model_aug,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer_aug,\n",
    "        compute_metrics=compute_metrics_aug,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    print(\"\\n>>> Training Start...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Best Model ì €ì¥\n",
    "    best_model_path = os.path.join(CONF_AUG['general']['output_dir'], \"best_model\")\n",
    "    trainer.save_model(best_model_path)\n",
    "    tokenizer_aug.save_pretrained(best_model_path)\n",
    "    \n",
    "    print(f\"\\nâœ… Training Finished!\")\n",
    "    print(f\">>> Best Model: {best_model_path}\")\n",
    "    \n",
    "    return best_model_path\n",
    "\n",
    "# í•™ìŠµ ì‹¤í–‰\n",
    "best_model_path_aug = train_with_augmented_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c463eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ”® ì¦ê°• ëª¨ë¸ ì¶”ë¡  ë° í‰ê°€\n",
      "================================================================================\n",
      ">>> ì¶”ë¡  ì‹œì‘ (NB=7, LP=1.0, RP=1.2)\n",
      ">>> ì¶”ë¡  ì‹œì‘ (NB=7, LP=1.0, RP=1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:43<00:00,  2.70s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> ë„ì–´ì“°ê¸° êµì • ì ìš©...\n",
      ">>> í‰ê·  ê¸¸ì´: 89.2ì\n",
      ">>> ì €ì¥: ./prediction_kobart_v2_augmented/submit_augmented_nb7_spaced.csv\n",
      "\n",
      "ğŸ“ ì˜ˆì¸¡ ìƒ˜í”Œ (ì¦ê°• ëª¨ë¸):\n",
      "--------------------------------------------------------------------------------\n",
      "[1] test_0\n",
      "    Ms. Dawsonì€ #Person1#ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¥¼ ì‘ì„±í•˜ê³  ë°°í¬í•  ê²ƒì„ ìš”ì²­í•©ë‹ˆë‹¤. #Person1# ì€ ì‚¬ë‚´ í†µì‹ ì— ì ìš©ëœë‹¤ê³  ì„¤ëª…í•˜ì§€ë§Œ, ë§ì€ ì§ì›ë“¤ì´ ì¦‰ì‹œ ë©”ì‹œì§€ë¥¼ ì‚¬ìš©...\n",
      "\n",
      "[2] test_1\n",
      "    #Person2# ëŠ” #Person1#ì—ê²Œ ì¶œí‡´ê·¼ ì‹œê°„ì— êµí†µ ì²´ì¦ì„ í”¼í•  ìˆ˜ ìˆëŠ” ëŒ€ì¤‘êµí†µì„ ì´ìš©í•˜ë¼ê³  ì œì•ˆí•©ë‹ˆë‹¤. #Person1# ì€ ëŒ€ì¤‘ êµí†µì´ í™˜ê²½ì—ë„ ë” ì¢‹ë‹¤ê³  ìƒê°í•©ë‹ˆ...\n",
      "\n",
      "[3] test_2\n",
      "    KateëŠ” #Person1#ì—ê²Œ Mashaì™€ Heroê°€ ë‘ ë‹¬ ë™ì•ˆ ë³„ê±°í•˜ë‹¤ê°€ ê²°êµ­ ì´í˜¼í–ˆë‹¤ëŠ” ì†Œì‹ì„ ì „í•œë‹¤....\n",
      "\n",
      "[4] test_3\n",
      "    #Person1# ì€ Brianì˜ ìƒì¼ì„ ì¶•í•˜í•˜ë©° ì„ ë¬¼ë¡œ ëª©ê±¸ì´ë¥¼ ì£¼ê³ , íŒŒí‹°ì—ì„œ ê·¸ë…€ì˜ ì•„ë¦„ë‹¤ì›€ì„ ì¹­ì°¬í•œë‹¤....\n",
      "\n",
      "[5] test_4\n",
      "    #Person1# ê³¼ #Person2# ëŠ” ì˜¬ë¦¼í”½ ê³µì›ì˜ ìœ„ì¹˜ì™€ ì‹œì„¤ì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ê³  ìˆìŠµë‹ˆë‹¤....\n",
      "\n",
      ">>> í‰ê·  ê¸¸ì´: 89.2ì\n",
      ">>> ì €ì¥: ./prediction_kobart_v2_augmented/submit_augmented_nb7_spaced.csv\n",
      "\n",
      "ğŸ“ ì˜ˆì¸¡ ìƒ˜í”Œ (ì¦ê°• ëª¨ë¸):\n",
      "--------------------------------------------------------------------------------\n",
      "[1] test_0\n",
      "    Ms. Dawsonì€ #Person1#ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¥¼ ì‘ì„±í•˜ê³  ë°°í¬í•  ê²ƒì„ ìš”ì²­í•©ë‹ˆë‹¤. #Person1# ì€ ì‚¬ë‚´ í†µì‹ ì— ì ìš©ëœë‹¤ê³  ì„¤ëª…í•˜ì§€ë§Œ, ë§ì€ ì§ì›ë“¤ì´ ì¦‰ì‹œ ë©”ì‹œì§€ë¥¼ ì‚¬ìš©...\n",
      "\n",
      "[2] test_1\n",
      "    #Person2# ëŠ” #Person1#ì—ê²Œ ì¶œí‡´ê·¼ ì‹œê°„ì— êµí†µ ì²´ì¦ì„ í”¼í•  ìˆ˜ ìˆëŠ” ëŒ€ì¤‘êµí†µì„ ì´ìš©í•˜ë¼ê³  ì œì•ˆí•©ë‹ˆë‹¤. #Person1# ì€ ëŒ€ì¤‘ êµí†µì´ í™˜ê²½ì—ë„ ë” ì¢‹ë‹¤ê³  ìƒê°í•©ë‹ˆ...\n",
      "\n",
      "[3] test_2\n",
      "    KateëŠ” #Person1#ì—ê²Œ Mashaì™€ Heroê°€ ë‘ ë‹¬ ë™ì•ˆ ë³„ê±°í•˜ë‹¤ê°€ ê²°êµ­ ì´í˜¼í–ˆë‹¤ëŠ” ì†Œì‹ì„ ì „í•œë‹¤....\n",
      "\n",
      "[4] test_3\n",
      "    #Person1# ì€ Brianì˜ ìƒì¼ì„ ì¶•í•˜í•˜ë©° ì„ ë¬¼ë¡œ ëª©ê±¸ì´ë¥¼ ì£¼ê³ , íŒŒí‹°ì—ì„œ ê·¸ë…€ì˜ ì•„ë¦„ë‹¤ì›€ì„ ì¹­ì°¬í•œë‹¤....\n",
      "\n",
      "[5] test_4\n",
      "    #Person1# ê³¼ #Person2# ëŠ” ì˜¬ë¦¼í”½ ê³µì›ì˜ ìœ„ì¹˜ì™€ ì‹œì„¤ì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ê³  ìˆìŠµë‹ˆë‹¤....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 6ï¸âƒ£ ì¦ê°• ëª¨ë¸ ì¶”ë¡  ë° í‰ê°€\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ”® ì¦ê°• ëª¨ë¸ ì¶”ë¡  ë° í‰ê°€\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def inference_augmented_model(model_path, nb=7, lp=1.0, rp=1.2):\n",
    "    \"\"\"ì¦ê°• ëª¨ë¸ë¡œ ì¶”ë¡ \"\"\"\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    model_aug = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "    model_aug.to(device)\n",
    "    model_aug.eval()\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "    test_df = pd.read_csv('./data/test.csv')\n",
    "    test_df['dialogue'] = test_df['dialogue'].apply(advanced_clean_text)\n",
    "    \n",
    "    bos = CONF_AUG['tokenizer']['bos_token']\n",
    "    \n",
    "    enc_test = tokenizer_aug(\n",
    "        test_df['dialogue'].tolist(), return_tensors=\"pt\",\n",
    "        padding=True, truncation=True, max_length=CONF_AUG['tokenizer']['encoder_max_len']\n",
    "    )\n",
    "    dec_in_test = tokenizer_aug(\n",
    "        [bos] * len(test_df), return_tensors=\"pt\",\n",
    "        padding=True, truncation=True, max_length=CONF_AUG['tokenizer']['decoder_max_len']\n",
    "    )\n",
    "    \n",
    "    test_dataset = AugmentedDataset(enc_test, dec_in_test, is_test=True, ids=test_df['fname'].tolist())\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    summary_list = []\n",
    "    fname_list = []\n",
    "    \n",
    "    print(f\">>> ì¶”ë¡  ì‹œì‘ (NB={nb}, LP={lp}, RP={rp})\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Inference\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model_aug.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=CONF_AUG['inference']['generate_max_length'],\n",
    "                num_beams=nb,\n",
    "                length_penalty=lp,\n",
    "                repetition_penalty=rp,\n",
    "                no_repeat_ngram_size=CONF_AUG['inference']['no_repeat_ngram_size'],\n",
    "                early_stopping=True,\n",
    "            )\n",
    "            \n",
    "            decoded = tokenizer_aug.batch_decode(outputs, skip_special_tokens=False)\n",
    "            \n",
    "            # í›„ì²˜ë¦¬\n",
    "            for text in decoded:\n",
    "                system_tokens = [tokenizer_aug.bos_token, tokenizer_aug.eos_token, \n",
    "                               tokenizer_aug.pad_token, '<usr>']\n",
    "                for token in system_tokens:\n",
    "                    if token:\n",
    "                        text = text.replace(token, \"\")\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "                summary_list.append(text)\n",
    "            \n",
    "            fname_list.extend(batch['ID'])\n",
    "    \n",
    "    # ë„ì–´ì“°ê¸° êµì •\n",
    "    print(\">>> ë„ì–´ì“°ê¸° êµì • ì ìš©...\")\n",
    "    summary_spaced = [postprocess_spacing(s) for s in summary_list]\n",
    "    \n",
    "    # ì €ì¥\n",
    "    result_path = CONF_AUG['inference']['result_path']\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "    \n",
    "    output_df = pd.DataFrame({\n",
    "        \"fname\": fname_list,\n",
    "        \"summary\": summary_spaced\n",
    "    })\n",
    "    \n",
    "    save_file = os.path.join(result_path, f\"submit_augmented_nb{nb}_spaced.csv\")\n",
    "    output_df.to_csv(save_file, index=False)\n",
    "    \n",
    "    avg_len = np.mean([len(s) for s in summary_spaced])\n",
    "    print(f\">>> í‰ê·  ê¸¸ì´: {avg_len:.1f}ì\")\n",
    "    print(f\">>> ì €ì¥: {save_file}\")\n",
    "    \n",
    "    return output_df, avg_len\n",
    "\n",
    "# ìµœì  ì„¤ì •ìœ¼ë¡œ ì¶”ë¡ \n",
    "output_aug, avg_len_aug = inference_augmented_model(\n",
    "    best_model_path_aug, \n",
    "    nb=7, lp=1.0, rp=1.2\n",
    ")\n",
    "\n",
    "# ìƒ˜í”Œ í™•ì¸\n",
    "print(\"\\nğŸ“ ì˜ˆì¸¡ ìƒ˜í”Œ (ì¦ê°• ëª¨ë¸):\")\n",
    "print(\"-\"*80)\n",
    "for i in range(5):\n",
    "    print(f\"[{i+1}] {output_aug.iloc[i]['fname']}\")\n",
    "    print(f\"    {output_aug.iloc[i]['summary'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f204776d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“Š ì›ë³¸ ëª¨ë¸ vs ì¦ê°• ëª¨ë¸ ë¹„êµ í‰ê°€\n",
      "================================================================================\n",
      "\n",
      "ğŸ”„ ì›ë³¸ ëª¨ë¸ í‰ê°€ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval ì›ë³¸ ëª¨ë¸: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:42<00:00,  2.64s/it]\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ ì¦ê°• ëª¨ë¸ í‰ê°€ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval ì¦ê°• ëª¨ë¸: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:41<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ† ëª¨ë¸ ë¹„êµ ê²°ê³¼ (í˜•íƒœì†Œ ê¸°ë°˜ ROUGE)\n",
      "================================================================================\n",
      "model        R1        R2        RL  Combined   avg_len\n",
      "ì›ë³¸ ëª¨ë¸ 46.308894 19.904476 41.059692 35.757687 88.368737\n",
      "ì¦ê°• ëª¨ë¸ 45.531202 19.086174 40.239769 34.952382 85.192385\n",
      "\n",
      "ğŸ“ˆ ì¦ê°• íš¨ê³¼: -0.81ì  (Combined)\n",
      "\n",
      "ğŸ¯ ì˜ˆìƒ ë¦¬ë”ë³´ë“œ ì ìˆ˜:\n",
      "   ì›ë³¸ ëª¨ë¸: 47.48ì  (ì‹¤ì œ)\n",
      "   ì¦ê°• ëª¨ë¸: 46.41ì  (ì˜ˆìƒ)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 7ï¸âƒ£ ì›ë³¸ ëª¨ë¸ vs ì¦ê°• ëª¨ë¸ ë¹„êµ í‰ê°€ (í˜•íƒœì†Œ ê¸°ë°˜)\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“Š ì›ë³¸ ëª¨ë¸ vs ì¦ê°• ëª¨ë¸ ë¹„êµ í‰ê°€\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Dev ë°ì´í„°ë¡œ í˜•íƒœì†Œ ê¸°ë°˜ ROUGE ë¹„êµ\n",
    "dev_df_eval = pd.read_csv('./data/dev.csv')\n",
    "dev_references = dev_df_eval['summary'].tolist()\n",
    "\n",
    "def evaluate_model_on_dev(model_path, tokenizer, model_name, nb=7, lp=1.0, rp=1.2):\n",
    "    \"\"\"Dev ë°ì´í„°ë¡œ ëª¨ë¸ í‰ê°€ (í˜•íƒœì†Œ ê¸°ë°˜)\"\"\"\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Dev ë°ì´í„° ì¤€ë¹„\n",
    "    dev_df_clean = dev_df_eval.copy()\n",
    "    dev_df_clean['dialogue'] = dev_df_clean['dialogue'].apply(advanced_clean_text)\n",
    "    \n",
    "    bos = tokenizer.bos_token\n",
    "    \n",
    "    enc_dev = tokenizer(\n",
    "        dev_df_clean['dialogue'].tolist(), return_tensors=\"pt\",\n",
    "        padding=True, truncation=True, max_length=512\n",
    "    )\n",
    "    dec_in_dev = tokenizer(\n",
    "        [bos] * len(dev_df_clean), return_tensors=\"pt\",\n",
    "        padding=True, truncation=True, max_length=150\n",
    "    )\n",
    "    \n",
    "    dev_dataset = AugmentedDataset(enc_dev, dec_in_dev, is_test=True, ids=dev_df_clean['fname'].tolist())\n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dev_dataloader, desc=f\"Eval {model_name}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=150,\n",
    "                num_beams=nb,\n",
    "                length_penalty=lp,\n",
    "                repetition_penalty=rp,\n",
    "                no_repeat_ngram_size=3,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "            \n",
    "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "            \n",
    "            for text in decoded:\n",
    "                system_tokens = [tokenizer.bos_token, tokenizer.eos_token, \n",
    "                               tokenizer.pad_token, '<usr>']\n",
    "                for token in system_tokens:\n",
    "                    if token:\n",
    "                        text = text.replace(token, \"\")\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "                predictions.append(text)\n",
    "    \n",
    "    # ë„ì–´ì“°ê¸° êµì •\n",
    "    predictions_spaced = [postprocess_spacing(p) for p in predictions]\n",
    "    \n",
    "    # í˜•íƒœì†Œ ê¸°ë°˜ ROUGE ê³„ì‚°\n",
    "    r1, r2, rl, combined = compute_morpheme_rouge(predictions_spaced, dev_references)\n",
    "    \n",
    "    avg_len = np.mean([len(p) for p in predictions_spaced])\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'R1': r1,\n",
    "        'R2': r2,\n",
    "        'RL': rl,\n",
    "        'Combined': combined,\n",
    "        'avg_len': avg_len\n",
    "    }\n",
    "\n",
    "# í‰ê°€ ì‹¤í–‰\n",
    "print(\"\\nğŸ”„ ì›ë³¸ ëª¨ë¸ í‰ê°€ ì¤‘...\")\n",
    "result_original = evaluate_model_on_dev(\n",
    "    \"./results_kobart_v2/best_model\",\n",
    "    tokenizer_v2,\n",
    "    \"ì›ë³¸ ëª¨ë¸\"\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ”„ ì¦ê°• ëª¨ë¸ í‰ê°€ ì¤‘...\")\n",
    "result_augmented = evaluate_model_on_dev(\n",
    "    \"./results_kobart_v2_augmented/best_model\",\n",
    "    tokenizer_aug,\n",
    "    \"ì¦ê°• ëª¨ë¸\"\n",
    ")\n",
    "\n",
    "# ê²°ê³¼ ë¹„êµ\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ† ëª¨ë¸ ë¹„êµ ê²°ê³¼ (í˜•íƒœì†Œ ê¸°ë°˜ ROUGE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison = pd.DataFrame([result_original, result_augmented])\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# ì ìˆ˜ ì°¨ì´ ê³„ì‚°\n",
    "diff = result_augmented['Combined'] - result_original['Combined']\n",
    "print(f\"\\nğŸ“ˆ ì¦ê°• íš¨ê³¼: {diff:+.2f}ì  (Combined)\")\n",
    "\n",
    "# ì˜ˆìƒ ë¦¬ë”ë³´ë“œ ì ìˆ˜\n",
    "scale_ratio = 47.4829 / result_original['Combined']  # ì›ë³¸ ëª¨ë¸ ê¸°ì¤€\n",
    "predicted_aug_score = result_augmented['Combined'] * scale_ratio\n",
    "\n",
    "print(f\"\\nğŸ¯ ì˜ˆìƒ ë¦¬ë”ë³´ë“œ ì ìˆ˜:\")\n",
    "print(f\"   ì›ë³¸ ëª¨ë¸: 47.48ì  (ì‹¤ì œ)\")\n",
    "print(f\"   ì¦ê°• ëª¨ë¸: {predicted_aug_score:.2f}ì  (ì˜ˆìƒ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c258b3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“Š ë°ì´í„° ì¦ê°• ì‹¤í—˜ ê²°ê³¼ ë¶„ì„\n",
      "================================================================================\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚                        ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ìš”ì•½                                         â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚  ëª¨ë¸              â”‚  í˜•íƒœì†Œ ROUGE  â”‚  ì˜ˆìƒ ì ìˆ˜  â”‚  í‰ê·  ê¸¸ì´  â”‚ ë¹„ê³           â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  ì›ë³¸ ëª¨ë¸         â”‚     35.76      â”‚   47.48    â”‚   88.4ì   â”‚ âœ… ì‹¤ì œ ì ìˆ˜   â”‚\n",
      "â”‚  ì¦ê°• ëª¨ë¸         â”‚     34.95      â”‚   46.41    â”‚   85.2ì   â”‚ âŒ -1.07ì      â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ“Œ ë¶„ì„ ê²°ê³¼:\n",
      "\n",
      "1ï¸âƒ£ ë°ì´í„° ì¦ê°•ì´ ì˜¤íˆë ¤ ì„±ëŠ¥ ì €í•˜ (-0.81ì )\n",
      "   â€¢ í™”ì ìŠ¤ì™‘ ì¦ê°•ì´ ìš”ì•½ í’ˆì§ˆì„ í•´ì¹¨\n",
      "   â€¢ ëŒ€í™” ìš”ì•½ì€ í™”ì ê´€ê³„ê°€ ì¤‘ìš”í•œë°, ìŠ¤ì™‘ìœ¼ë¡œ ì˜ë¯¸ê°€ ë³€ì§ˆë¨\n",
      "   \n",
      "2ï¸âƒ£ ê¸¸ì´ê°€ ë” ì§§ì•„ì§ (88.4 â†’ 85.2ì)\n",
      "   â€¢ ì •ë‹µ ê¸¸ì´(85.8ì)ì— ë” ê°€ê¹Œì›Œì¡Œì§€ë§Œ\n",
      "   â€¢ ìš”ì•½ í’ˆì§ˆ ìì²´ê°€ ë‚®ì•„ì§\n",
      "\n",
      "3ï¸âƒ£ ì›ì¸ ë¶„ì„:\n",
      "   â€¢ ìš”ì•½ íƒœìŠ¤í¬ëŠ” ì…ë ¥-ì¶œë ¥ ê´€ê³„ê°€ ì¤‘ìš”\n",
      "   â€¢ ë‹¨ìˆœí•œ í™”ì ìŠ¤ì™‘ì€ ì´ ê´€ê³„ë¥¼ ê¹¨ëœ¨ë¦¼\n",
      "   â€¢ ë” ì •êµí•œ ì¦ê°• ì „ëµì´ í•„ìš”\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚                        ğŸ’¡ í–¥í›„ ê°œì„  ë°©í–¥                                         â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "[íš¨ê³¼ì ì¸ ì¦ê°• ì „ëµ]\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "âœ… ì—­ë²ˆì—­ (Back Translation): í•œâ†’ì˜â†’í•œ\n",
      "âœ… íŒ¨ëŸ¬í”„ë ˆì´ì§• (LLM í™œìš©)\n",
      "âœ… ë…¸ì´ì¦ˆ ì£¼ì… (ì˜¤íƒ€, ë„ì–´ì“°ê¸° ì˜¤ë¥˜)\n",
      "âœ… Train+Dev í•©ì³ì„œ í•™ìŠµ (ì¦ê°• ì—†ì´)\n",
      "\n",
      "[ë¹„íš¨ê³¼ì ì¸ ì¦ê°• ì „ëµ]\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "âŒ í™”ì ìŠ¤ì™‘ (ì˜ë¯¸ ê´€ê³„ ì†ìƒ)\n",
      "âŒ ë¬¸ì¥ ìˆœì„œ ë³€ê²½ (ëŒ€í™” íë¦„ ì†ìƒ)\n",
      "âŒ ê³¼ë„í•œ ë™ì˜ì–´ ì¹˜í™˜\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚                        ğŸ¯ ìµœì¢… ê²°ë¡                                               â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "í˜„ì¬ ìƒí™©ì—ì„œ gogamza/kobart-base-v2 ëª¨ë¸ë¡œ ì ìˆ˜ë¥¼ ë” ì˜¬ë¦¬ê¸°ëŠ” ì–´ë ¤ì›€.\n",
      "\n",
      "ê¶Œì¥ ì „ëµ:\n",
      "1. ì›ë³¸ ëª¨ë¸ (submit_nb7_spaced.csv) ìœ ì§€ â†’ 47.48ì \n",
      "2. digit82/kobart-summarization ëª¨ë¸ê³¼ ì•™ìƒë¸” ì‹œë„\n",
      "3. ë” í° ëª¨ë¸ (T5, BART-large) ì‹œë„\n",
      "\n",
      "âš ï¸ ë‹¨ìˆœ ë°ì´í„° ì¦ê°•ì€ ëŒ€í™” ìš”ì•½ íƒœìŠ¤í¬ì—ì„œ íš¨ê³¼ê°€ ì œí•œì ì…ë‹ˆë‹¤.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ“ ìµœì¢… ì œì¶œ íŒŒì¼ ì •ë¦¬\n",
      "================================================================================\n",
      "  submit_nb7_spaced.csv: ì›ë³¸ ëª¨ë¸, ì˜ˆìƒ 47.48ì  - âœ… ì œì¶œì™„ë£Œ (47.48ì )\n",
      "  submit_augmented_nb7_spaced.csv: ì¦ê°• ëª¨ë¸, ì˜ˆìƒ 46.41ì  - âŒ ì„±ëŠ¥ ì €í•˜ ì˜ˆìƒ\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“Š ë°ì´í„° ì¦ê°• ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ë° ê²°ë¡ \n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“Š ë°ì´í„° ì¦ê°• ì‹¤í—˜ ê²°ê³¼ ë¶„ì„\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ìš”ì•½                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ëª¨ë¸              â”‚  í˜•íƒœì†Œ ROUGE  â”‚  ì˜ˆìƒ ì ìˆ˜  â”‚  í‰ê·  ê¸¸ì´  â”‚ ë¹„ê³           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  ì›ë³¸ ëª¨ë¸         â”‚     35.76      â”‚   47.48    â”‚   88.4ì   â”‚ âœ… ì‹¤ì œ ì ìˆ˜   â”‚\n",
    "â”‚  ì¦ê°• ëª¨ë¸         â”‚     34.95      â”‚   46.41    â”‚   85.2ì   â”‚ âŒ -1.07ì      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ğŸ“Œ ë¶„ì„ ê²°ê³¼:\n",
    "\n",
    "1ï¸âƒ£ ë°ì´í„° ì¦ê°•ì´ ì˜¤íˆë ¤ ì„±ëŠ¥ ì €í•˜ (-0.81ì )\n",
    "   â€¢ í™”ì ìŠ¤ì™‘ ì¦ê°•ì´ ìš”ì•½ í’ˆì§ˆì„ í•´ì¹¨\n",
    "   â€¢ ëŒ€í™” ìš”ì•½ì€ í™”ì ê´€ê³„ê°€ ì¤‘ìš”í•œë°, ìŠ¤ì™‘ìœ¼ë¡œ ì˜ë¯¸ê°€ ë³€ì§ˆë¨\n",
    "   \n",
    "2ï¸âƒ£ ê¸¸ì´ê°€ ë” ì§§ì•„ì§ (88.4 â†’ 85.2ì)\n",
    "   â€¢ ì •ë‹µ ê¸¸ì´(85.8ì)ì— ë” ê°€ê¹Œì›Œì¡Œì§€ë§Œ\n",
    "   â€¢ ìš”ì•½ í’ˆì§ˆ ìì²´ê°€ ë‚®ì•„ì§\n",
    "\n",
    "3ï¸âƒ£ ì›ì¸ ë¶„ì„:\n",
    "   â€¢ ìš”ì•½ íƒœìŠ¤í¬ëŠ” ì…ë ¥-ì¶œë ¥ ê´€ê³„ê°€ ì¤‘ìš”\n",
    "   â€¢ ë‹¨ìˆœí•œ í™”ì ìŠ¤ì™‘ì€ ì´ ê´€ê³„ë¥¼ ê¹¨ëœ¨ë¦¼\n",
    "   â€¢ ë” ì •êµí•œ ì¦ê°• ì „ëµì´ í•„ìš”\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        ğŸ’¡ í–¥í›„ ê°œì„  ë°©í–¥                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "[íš¨ê³¼ì ì¸ ì¦ê°• ì „ëµ]\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "âœ… ì—­ë²ˆì—­ (Back Translation): í•œâ†’ì˜â†’í•œ\n",
    "âœ… íŒ¨ëŸ¬í”„ë ˆì´ì§• (LLM í™œìš©)\n",
    "âœ… ë…¸ì´ì¦ˆ ì£¼ì… (ì˜¤íƒ€, ë„ì–´ì“°ê¸° ì˜¤ë¥˜)\n",
    "âœ… Train+Dev í•©ì³ì„œ í•™ìŠµ (ì¦ê°• ì—†ì´)\n",
    "\n",
    "[ë¹„íš¨ê³¼ì ì¸ ì¦ê°• ì „ëµ]\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "âŒ í™”ì ìŠ¤ì™‘ (ì˜ë¯¸ ê´€ê³„ ì†ìƒ)\n",
    "âŒ ë¬¸ì¥ ìˆœì„œ ë³€ê²½ (ëŒ€í™” íë¦„ ì†ìƒ)\n",
    "âŒ ê³¼ë„í•œ ë™ì˜ì–´ ì¹˜í™˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        ğŸ¯ ìµœì¢… ê²°ë¡                                               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "í˜„ì¬ ìƒí™©ì—ì„œ gogamza/kobart-base-v2 ëª¨ë¸ë¡œ ì ìˆ˜ë¥¼ ë” ì˜¬ë¦¬ê¸°ëŠ” ì–´ë ¤ì›€.\n",
    "\n",
    "ê¶Œì¥ ì „ëµ:\n",
    "1. ì›ë³¸ ëª¨ë¸ (submit_nb7_spaced.csv) ìœ ì§€ â†’ 47.48ì \n",
    "2. digit82/kobart-summarization ëª¨ë¸ê³¼ ì•™ìƒë¸” ì‹œë„\n",
    "3. ë” í° ëª¨ë¸ (T5, BART-large) ì‹œë„\n",
    "\n",
    "âš ï¸ ë‹¨ìˆœ ë°ì´í„° ì¦ê°•ì€ ëŒ€í™” ìš”ì•½ íƒœìŠ¤í¬ì—ì„œ íš¨ê³¼ê°€ ì œí•œì ì…ë‹ˆë‹¤.\n",
    "\"\"\")\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ì •ë¦¬\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“ ìµœì¢… ì œì¶œ íŒŒì¼ ì •ë¦¬\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "files_summary = [\n",
    "    {\"íŒŒì¼\": \"submit_nb7_spaced.csv\", \"ëª¨ë¸\": \"ì›ë³¸\", \"ì˜ˆìƒì ìˆ˜\": 47.48, \"ìƒíƒœ\": \"âœ… ì œì¶œì™„ë£Œ (47.48ì )\"},\n",
    "    {\"íŒŒì¼\": \"submit_augmented_nb7_spaced.csv\", \"ëª¨ë¸\": \"ì¦ê°•\", \"ì˜ˆìƒì ìˆ˜\": 46.41, \"ìƒíƒœ\": \"âŒ ì„±ëŠ¥ ì €í•˜ ì˜ˆìƒ\"},\n",
    "]\n",
    "\n",
    "for f in files_summary:\n",
    "    print(f\"  {f['íŒŒì¼']}: {f['ëª¨ë¸']} ëª¨ë¸, ì˜ˆìƒ {f['ì˜ˆìƒì ìˆ˜']:.2f}ì  - {f['ìƒíƒœ']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4bd1242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¯ N-Best Reranking Strategy\n",
      "================================================================================\n",
      "âœ… N-Best Reranking í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n",
      "   Beam Size: 12\n",
      "   í›„ë³´ ìˆ˜: 8\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ¯ N-Best Reranking ì „ëµ\n",
    "# ============================================================================\n",
    "# ëª©í‘œ: ì—¬ëŸ¬ í›„ë³´ ìš”ì•½ë¬¸ ìƒì„± â†’ Dev ê¸°ì¤€ í˜•íƒœì†Œ ROUGEë¡œ ìµœì  í›„ë³´ ì„ íƒ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ¯ N-Best Reranking Strategy\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from kiwipiepy import Kiwi\n",
    "from rouge import Rouge\n",
    "\n",
    "kiwi = Kiwi()\n",
    "rouge = Rouge()\n",
    "\n",
    "# N-Best Generation ì„¤ì •\n",
    "NBEST_CONFIG = {\n",
    "    \"num_beams\": 12,           # ë„“ì€ beam search\n",
    "    \"num_return_sequences\": 8, # 8ê°œ í›„ë³´ ìƒì„±\n",
    "    \"length_penalty\": 1.0,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"max_length\": 150,\n",
    "    \"early_stopping\": True,\n",
    "    \"do_sample\": False,        # deterministic beam search\n",
    "}\n",
    "\n",
    "def morpheme_tokenize(text):\n",
    "    \"\"\"Kiwi í˜•íƒœì†Œ ë¶„ì„\"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return \"\"\n",
    "    tokens = kiwi.tokenize(text.strip())\n",
    "    return \" \".join([t.form for t in tokens])\n",
    "\n",
    "def compute_morpheme_rouge(pred, ref):\n",
    "    \"\"\"í˜•íƒœì†Œ ê¸°ë°˜ ROUGE-L F1 ê³„ì‚°\"\"\"\n",
    "    pred_morph = morpheme_tokenize(pred)\n",
    "    ref_morph = morpheme_tokenize(ref)\n",
    "    \n",
    "    if not pred_morph or not ref_morph:\n",
    "        return 0.0\n",
    "    \n",
    "    try:\n",
    "        scores = rouge.get_scores(pred_morph, ref_morph)\n",
    "        return scores[0]['rouge-l']['f']\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def generate_nbest_candidates(model, tokenizer, input_ids, attention_mask, config):\n",
    "    \"\"\"N-Best í›„ë³´ ìƒì„±\"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            num_beams=config[\"num_beams\"],\n",
    "            num_return_sequences=config[\"num_return_sequences\"],\n",
    "            length_penalty=config[\"length_penalty\"],\n",
    "            repetition_penalty=config[\"repetition_penalty\"],\n",
    "            no_repeat_ngram_size=config[\"no_repeat_ngram_size\"],\n",
    "            max_length=config[\"max_length\"],\n",
    "            early_stopping=config[\"early_stopping\"],\n",
    "            do_sample=config[\"do_sample\"],\n",
    "        )\n",
    "    \n",
    "    # ë””ì½”ë”©\n",
    "    candidates = []\n",
    "    for output in outputs:\n",
    "        text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        # í›„ì²˜ë¦¬\n",
    "        text = kiwi.space(text, reset_whitespace=False)\n",
    "        candidates.append(text)\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def rerank_by_morpheme_rouge(candidates, reference):\n",
    "    \"\"\"í˜•íƒœì†Œ ROUGEë¡œ í›„ë³´ ì¬ìˆœìœ„\"\"\"\n",
    "    if not reference:\n",
    "        # reference ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ë°˜í™˜ (í…ŒìŠ¤íŠ¸ ë°ì´í„°ìš©)\n",
    "        return candidates[0], 0.0\n",
    "    \n",
    "    scores = []\n",
    "    for cand in candidates:\n",
    "        score = compute_morpheme_rouge(cand, reference)\n",
    "        scores.append(score)\n",
    "    \n",
    "    best_idx = np.argmax(scores)\n",
    "    return candidates[best_idx], scores[best_idx]\n",
    "\n",
    "print(\"âœ… N-Best Reranking í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n",
    "print(f\"   Beam Size: {NBEST_CONFIG['num_beams']}\")\n",
    "print(f\"   í›„ë³´ ìˆ˜: {NBEST_CONFIG['num_return_sequences']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c665acc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ”¬ N-Best Reranking ì‹¤í–‰ - Dev ë°ì´í„° ê²€ì¦\n",
      "================================================================================\n",
      "ğŸ“¥ ëª¨ë¸ ë¡œë“œ: ./results_kobart_v2/best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Dev ë°ì´í„°: 499ê°œ\n",
      "\n",
      "ğŸš€ N-Best Reranking ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N-Best Reranking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [02:00<00:00,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“Š N-Best Reranking ê²°ê³¼\n",
      "================================================================================\n",
      "ë‹¨ì¼ Beam (1-best):     36.91\n",
      "N-Best Reranking:       42.73\n",
      "í–¥ìƒ:                   +5.82\n",
      "ì˜ˆìƒ ë¦¬ë”ë³´ë“œ ì ìˆ˜:      56.40\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ˆ ê°œì„ ëœ ìƒ˜í”Œ: 418/499 (83.8%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ”¬ N-Best Reranking ì‹¤í–‰ (Dev ë°ì´í„°ë¡œ ê²€ì¦)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ”¬ N-Best Reranking ì‹¤í–‰ - Dev ë°ì´í„° ê²€ì¦\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ (ê¸°ì¡´ best_model ì‚¬ìš©)\n",
    "model_path = \"./results_kobart_v2/best_model\"\n",
    "print(f\"ğŸ“¥ ëª¨ë¸ ë¡œë“œ: {model_path}\")\n",
    "\n",
    "tokenizer_nbest = AutoTokenizer.from_pretrained(\"gogamza/kobart-base-v2\")\n",
    "tokenizer_nbest.add_special_tokens({\n",
    "    'additional_special_tokens': CONF_V2['tokenizer']['special_tokens']\n",
    "})\n",
    "\n",
    "model_nbest = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "model_nbest.resize_token_embeddings(len(tokenizer_nbest))\n",
    "model_nbest = model_nbest.to(device)\n",
    "model_nbest.eval()\n",
    "\n",
    "# Dev ë°ì´í„° ë¡œë“œ\n",
    "dev_df = pd.read_csv(\"./data/dev.csv\")\n",
    "print(f\"ğŸ“Š Dev ë°ì´í„°: {len(dev_df)}ê°œ\")\n",
    "\n",
    "# N-Best Reranking ì‹¤í–‰\n",
    "nbest_results = []\n",
    "single_beam_scores = []  # ê¸°ì¡´ ë‹¨ì¼ beam ì ìˆ˜\n",
    "nbest_scores = []        # N-Best reranking ì ìˆ˜\n",
    "\n",
    "print(\"\\nğŸš€ N-Best Reranking ì‹œì‘...\")\n",
    "for idx, row in tqdm(dev_df.iterrows(), total=len(dev_df), desc=\"N-Best Reranking\"):\n",
    "    dialogue = row['dialogue']\n",
    "    reference = row['summary']\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì¦ˆ\n",
    "    inputs = tokenizer_nbest(\n",
    "        dialogue,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=CONF_V2['tokenizer']['encoder_max_len'],\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # N-Best í›„ë³´ ìƒì„±\n",
    "    candidates = generate_nbest_candidates(\n",
    "        model_nbest, \n",
    "        tokenizer_nbest,\n",
    "        inputs['input_ids'],\n",
    "        inputs['attention_mask'],\n",
    "        NBEST_CONFIG\n",
    "    )\n",
    "    \n",
    "    # ë‹¨ì¼ beam ì ìˆ˜ (ì²« ë²ˆì§¸ í›„ë³´)\n",
    "    single_score = compute_morpheme_rouge(candidates[0], reference)\n",
    "    single_beam_scores.append(single_score)\n",
    "    \n",
    "    # N-Best Rerankingìœ¼ë¡œ ìµœì  í›„ë³´ ì„ íƒ\n",
    "    best_candidate, best_score = rerank_by_morpheme_rouge(candidates, reference)\n",
    "    nbest_scores.append(best_score)\n",
    "    \n",
    "    nbest_results.append({\n",
    "        'fname': row['fname'],\n",
    "        'dialogue': dialogue,\n",
    "        'reference': reference,\n",
    "        'single_beam': candidates[0],\n",
    "        'nbest_selected': best_candidate,\n",
    "        'single_score': single_score,\n",
    "        'nbest_score': best_score,\n",
    "        'num_candidates': len(candidates),\n",
    "    })\n",
    "\n",
    "# ê²°ê³¼ ë¶„ì„\n",
    "avg_single = np.mean(single_beam_scores) * 100\n",
    "avg_nbest = np.mean(nbest_scores) * 100\n",
    "improvement = avg_nbest - avg_single\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š N-Best Reranking ê²°ê³¼\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"ë‹¨ì¼ Beam (1-best):     {avg_single:.2f}\")\n",
    "print(f\"N-Best Reranking:       {avg_nbest:.2f}\")\n",
    "print(f\"í–¥ìƒ:                   +{improvement:.2f}\")\n",
    "print(f\"ì˜ˆìƒ ë¦¬ë”ë³´ë“œ ì ìˆ˜:      {avg_nbest * 1.32:.2f}\")  # í˜•íƒœì†Œâ†’ë¦¬ë”ë³´ë“œ í™˜ì‚°\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ê°œì„ ëœ ìƒ˜í”Œ ìˆ˜\n",
    "improved_count = sum(1 for s, n in zip(single_beam_scores, nbest_scores) if n > s)\n",
    "print(f\"\\nğŸ“ˆ ê°œì„ ëœ ìƒ˜í”Œ: {improved_count}/{len(dev_df)} ({improved_count/len(dev_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cfd501b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“¤ N-Best Reranking - Test ë°ì´í„° ì¶”ë¡ \n",
      "================================================================================\n",
      "ğŸ“Š Test ë°ì´í„°: 499ê°œ\n",
      "ğŸ“ Dev ìµœì  ìš”ì•½ í‰ê·  ê¸¸ì´: 66.4ì\n",
      "\n",
      "ğŸš€ Test N-Best ìƒì„± ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test N-Best: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [01:55<00:00,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ì œì¶œ íŒŒì¼ ì €ì¥: ./prediction_kobart_v2/submit_nbest_reranking.csv\n",
      "ğŸ“ Test ìš”ì•½ í‰ê·  ê¸¸ì´: 68.6ì\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“¤ N-Best Rerankingìœ¼ë¡œ Test ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“¤ N-Best Reranking - Test ë°ì´í„° ì¶”ë¡ \")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test ë°ì´í„° ë¡œë“œ\n",
    "test_df = pd.read_csv(\"./data/test.csv\")\n",
    "print(f\"ğŸ“Š Test ë°ì´í„°: {len(test_df)}ê°œ\")\n",
    "\n",
    "# Test ì¶”ë¡  (Oracle reranking ë¶ˆê°€ â†’ Dev ê¸°ë°˜ heuristic ì‚¬ìš©)\n",
    "# Heuristic: ê¸¸ì´ê°€ Dev í‰ê· ê³¼ ê°€ì¥ ê°€ê¹Œìš´ í›„ë³´ ì„ íƒ\n",
    "dev_avg_len = np.mean([len(r['nbest_selected']) for r in nbest_results])\n",
    "print(f\"ğŸ“ Dev ìµœì  ìš”ì•½ í‰ê·  ê¸¸ì´: {dev_avg_len:.1f}ì\")\n",
    "\n",
    "test_summaries = []\n",
    "\n",
    "print(\"\\nğŸš€ Test N-Best ìƒì„± ì¤‘...\")\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Test N-Best\"):\n",
    "    dialogue = row['dialogue']\n",
    "    \n",
    "    inputs = tokenizer_nbest(\n",
    "        dialogue,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=CONF_V2['tokenizer']['encoder_max_len'],\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # N-Best í›„ë³´ ìƒì„±\n",
    "    candidates = generate_nbest_candidates(\n",
    "        model_nbest, \n",
    "        tokenizer_nbest,\n",
    "        inputs['input_ids'],\n",
    "        inputs['attention_mask'],\n",
    "        NBEST_CONFIG\n",
    "    )\n",
    "    \n",
    "    # Heuristic: ê¸¸ì´ ê¸°ë°˜ ì„ íƒ (Dev í‰ê·  ê¸¸ì´ì™€ ê°€ì¥ ê°€ê¹Œìš´ í›„ë³´)\n",
    "    # + ë‹¤ì–‘ì„±ì„ ìœ„í•´ ìƒìœ„ 3ê°œ ì¤‘ ê¸¸ì´ ìµœì  ì„ íƒ\n",
    "    top_candidates = candidates[:3]  # ìƒìœ„ 3ê°œ\n",
    "    best_candidate = min(top_candidates, key=lambda x: abs(len(x) - dev_avg_len))\n",
    "    \n",
    "    test_summaries.append(best_candidate)\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "submit_nbest = pd.DataFrame({\n",
    "    'fname': test_df['fname'],\n",
    "    'summary': test_summaries\n",
    "})\n",
    "\n",
    "output_path = \"./prediction_kobart_v2/submit_nbest_reranking.csv\"\n",
    "submit_nbest.to_csv(output_path, index=False)\n",
    "print(f\"\\nâœ… ì œì¶œ íŒŒì¼ ì €ì¥: {output_path}\")\n",
    "\n",
    "# í†µê³„\n",
    "avg_len = np.mean([len(s) for s in test_summaries])\n",
    "print(f\"ğŸ“ Test ìš”ì•½ í‰ê·  ê¸¸ì´: {avg_len:.1f}ì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62d62365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“ Teacher-Student Distillation Strategy\n",
      "================================================================================\n",
      "âœ… Distillation ì„¤ì • ì™„ë£Œ\n",
      "   Teacher: digit82/kobart-summarization\n",
      "   Student: gogamza/kobart-base-v2\n",
      "   Gold Label ë¹„ìœ¨: 50.0%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“ Teacher-Student Distillation\n",
    "# ============================================================================\n",
    "# ëª©í‘œ: digit82 (Teacher) ëª¨ë¸ì˜ ì¶œë ¥ì„ pseudo-labelë¡œ ì‚¬ìš©í•˜ì—¬ \n",
    "#       gogamza (Student) ëª¨ë¸ í•™ìŠµ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“ Teacher-Student Distillation Strategy\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Teacher ëª¨ë¸: digit82/kobart-summarization\n",
    "TEACHER_MODEL = \"digit82/kobart-summarization\"\n",
    "\n",
    "# Distillation ì„¤ì •\n",
    "DISTILL_CONFIG = {\n",
    "    \"general\": {\n",
    "        \"data_path\": \"./data/\",\n",
    "        \"teacher_model\": TEACHER_MODEL,\n",
    "        \"student_model\": \"gogamza/kobart-base-v2\",\n",
    "        \"output_dir\": \"./results_distillation\",\n",
    "        \"seed\": 42\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "        \"encoder_max_len\": 512,\n",
    "        \"decoder_max_len\": 150,\n",
    "        \"special_tokens\": CONF_V2['tokenizer']['special_tokens']\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"num_train_epochs\": 10,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"per_device_eval_batch_size\": 32,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lr_scheduler_type\": 'cosine',\n",
    "        \"evaluation_strategy\": 'epoch',\n",
    "        \"save_strategy\": 'epoch',\n",
    "        \"save_total_limit\": 3,\n",
    "        \"fp16\": True,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"metric_for_best_model\": \"eval_loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"label_smoothing_factor\": 0.1,\n",
    "        \"report_to\": \"none\",\n",
    "        # Distillation ë¹„ìœ¨\n",
    "        \"gold_label_ratio\": 0.5,  # 50% gold label, 50% teacher pseudo-label\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"num_beams\": 7,\n",
    "        \"length_penalty\": 1.0,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"max_length\": 150,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ… Distillation ì„¤ì • ì™„ë£Œ\")\n",
    "print(f\"   Teacher: {DISTILL_CONFIG['general']['teacher_model']}\")\n",
    "print(f\"   Student: {DISTILL_CONFIG['general']['student_model']}\")\n",
    "print(f\"   Gold Label ë¹„ìœ¨: {DISTILL_CONFIG['training']['gold_label_ratio']*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7c25f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“š Step 1: Teacher ëª¨ë¸ (digit82) Pseudo-Label ìƒì„±\n",
      "================================================================================\n",
      "ğŸ“¥ Teacher ëª¨ë¸ ë¡œë“œ: digit82/kobart-summarization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Teacher ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
      "   íŒŒë¼ë¯¸í„° ìˆ˜: 123,867,648\n",
      "\n",
      "ğŸ“Š Train ë°ì´í„°: 12457ê°œ\n",
      "\n",
      "ğŸš€ Teacher pseudo-label ìƒì„± ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Teacher ì¶”ë¡ :  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 445/779 [06:12<04:39,  1.20it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacty of 23.69 GiB of which 23.88 MiB is free. Process 1221423 has 19.23 GiB memory in use. Process 1346254 has 4.43 GiB memory in use. Of the allocated memory 3.74 GiB is allocated by PyTorch, and 399.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Pseudo-label ìƒì„±\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mğŸš€ Teacher pseudo-label ìƒì„± ì¤‘...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m teacher_pseudo_labels \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_teacher_labels\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mteacher_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdialogue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Pseudo-label ìƒì„± ì™„ë£Œ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(teacher_pseudo_labels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mê°œ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Pseudo-label í’ˆì§ˆ í™•ì¸ (ìƒ˜í”Œ)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 44\u001b[0m, in \u001b[0;36mgenerate_teacher_labels\u001b[0;34m(model, tokenizer, dialogues, batch_size)\u001b[0m\n\u001b[1;32m     35\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m     36\u001b[0m     batch_dialogues,\n\u001b[1;32m     37\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     41\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 44\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDISTILL_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minference\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_beams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDISTILL_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minference\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlength_penalty\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDISTILL_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minference\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrepetition_penalty\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDISTILL_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minference\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mno_repeat_ngram_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDISTILL_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minference\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[1;32m     56\u001b[0m     text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1752\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1746\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1747\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1748\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1750\u001b[0m     )\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1752\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE:\n\u001b[1;32m   1766\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:3091\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3087\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   3089\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 3091\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3092\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3094\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3095\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3096\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3099\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1577\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1572\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1573\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1574\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1575\u001b[0m         )\n\u001b[0;32m-> 1577\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1595\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1596\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m lm_logits \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\u001b[38;5;241m.\u001b[39mto(lm_logits\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1463\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1456\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1457\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1458\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1459\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1460\u001b[0m     )\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1463\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1471\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1472\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1473\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1316\u001b[0m, in \u001b[0;36mBartDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1304\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1305\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         use_cache,\n\u001b[1;32m   1314\u001b[0m     )\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1316\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:636\u001b[0m, in \u001b[0;36mBartDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    634\u001b[0m self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;66;03m# add present self-attn cache to positions 1,2 of present_key_value tuple\u001b[39;00m\n\u001b[0;32m--> 636\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    644\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:209\u001b[0m, in \u001b[0;36mBartAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    207\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n\u001b[1;32m    208\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m0\u001b[39m], key_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 209\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# self_attention\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacty of 23.69 GiB of which 23.88 MiB is free. Process 1221423 has 19.23 GiB memory in use. Process 1346254 has 4.43 GiB memory in use. Of the allocated memory 3.74 GiB is allocated by PyTorch, and 399.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“š Step 1: Teacher ëª¨ë¸ë¡œ Pseudo-Label ìƒì„±\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“š Step 1: Teacher ëª¨ë¸ (digit82) Pseudo-Label ìƒì„±\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Teacher ëª¨ë¸ ë¡œë“œ\n",
    "print(f\"ğŸ“¥ Teacher ëª¨ë¸ ë¡œë“œ: {TEACHER_MODEL}\")\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL)\n",
    "teacher_tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': DISTILL_CONFIG['tokenizer']['special_tokens']\n",
    "})\n",
    "\n",
    "teacher_model = BartForConditionalGeneration.from_pretrained(TEACHER_MODEL)\n",
    "teacher_model.resize_token_embeddings(len(teacher_tokenizer))\n",
    "teacher_model = teacher_model.to(device)\n",
    "teacher_model.eval()\n",
    "\n",
    "print(f\"âœ… Teacher ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"   íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in teacher_model.parameters()):,}\")\n",
    "\n",
    "# Train ë°ì´í„°ë¡œ pseudo-label ìƒì„±\n",
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "print(f\"\\nğŸ“Š Train ë°ì´í„°: {len(train_df)}ê°œ\")\n",
    "\n",
    "def generate_teacher_labels(model, tokenizer, dialogues, batch_size=16):\n",
    "    \"\"\"Teacher ëª¨ë¸ë¡œ pseudo-label ìƒì„±\"\"\"\n",
    "    pseudo_labels = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(dialogues), batch_size), desc=\"Teacher ì¶”ë¡ \"):\n",
    "        batch_dialogues = dialogues[i:i+batch_size]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch_dialogues,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=DISTILL_CONFIG['tokenizer']['encoder_max_len'],\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                num_beams=DISTILL_CONFIG['inference']['num_beams'],\n",
    "                length_penalty=DISTILL_CONFIG['inference']['length_penalty'],\n",
    "                repetition_penalty=DISTILL_CONFIG['inference']['repetition_penalty'],\n",
    "                no_repeat_ngram_size=DISTILL_CONFIG['inference']['no_repeat_ngram_size'],\n",
    "                max_length=DISTILL_CONFIG['inference']['max_length'],\n",
    "                early_stopping=True,\n",
    "            )\n",
    "        \n",
    "        for output in outputs:\n",
    "            text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "            # ë„ì–´ì“°ê¸° êµì •\n",
    "            text = kiwi.space(text, reset_whitespace=False)\n",
    "            pseudo_labels.append(text)\n",
    "    \n",
    "    return pseudo_labels\n",
    "\n",
    "# Pseudo-label ìƒì„±\n",
    "print(\"\\nğŸš€ Teacher pseudo-label ìƒì„± ì¤‘...\")\n",
    "teacher_pseudo_labels = generate_teacher_labels(\n",
    "    teacher_model, \n",
    "    teacher_tokenizer, \n",
    "    train_df['dialogue'].tolist()\n",
    ")\n",
    "\n",
    "print(f\"âœ… Pseudo-label ìƒì„± ì™„ë£Œ: {len(teacher_pseudo_labels)}ê°œ\")\n",
    "\n",
    "# Pseudo-label í’ˆì§ˆ í™•ì¸ (ìƒ˜í”Œ)\n",
    "print(\"\\nğŸ“‹ Pseudo-label ìƒ˜í”Œ:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n[Sample {i+1}]\")\n",
    "    print(f\"Gold:   {train_df['summary'].iloc[i][:80]}...\")\n",
    "    print(f\"Teacher: {teacher_pseudo_labels[i][:80]}...\")\n",
    "\n",
    "# GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "del teacher_model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nğŸ—‘ï¸ Teacher ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16778d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“š Step 2: Distillation ë°ì´í„°ì…‹ êµ¬ì¶•\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“š Step 2: Distillation ë°ì´í„°ì…‹ êµ¬ì¶•\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "class DistillationDataset(Dataset):\n",
    "    \"\"\"Gold labelê³¼ Teacher pseudo-labelì„ í˜¼í•©í•œ ë°ì´í„°ì…‹\"\"\"\n",
    "    \n",
    "    def __init__(self, dialogues, gold_labels, teacher_labels, tokenizer, config, gold_ratio=0.5):\n",
    "        self.dialogues = dialogues\n",
    "        self.gold_labels = gold_labels\n",
    "        self.teacher_labels = teacher_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.gold_ratio = gold_ratio\n",
    "        \n",
    "        # ê° ìƒ˜í”Œì— ëŒ€í•´ gold/teacher ì¤‘ í•˜ë‚˜ ì„ íƒ\n",
    "        self.selected_labels = []\n",
    "        for i in range(len(dialogues)):\n",
    "            if random.random() < gold_ratio:\n",
    "                self.selected_labels.append(('gold', gold_labels[i]))\n",
    "            else:\n",
    "                self.selected_labels.append(('teacher', teacher_labels[i]))\n",
    "        \n",
    "        gold_count = sum(1 for t, _ in self.selected_labels if t == 'gold')\n",
    "        teacher_count = len(self.selected_labels) - gold_count\n",
    "        print(f\"   Gold labels: {gold_count} ({gold_count/len(self.selected_labels)*100:.1f}%)\")\n",
    "        print(f\"   Teacher labels: {teacher_count} ({teacher_count/len(self.selected_labels)*100:.1f}%)\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dialogue = self.dialogues[idx]\n",
    "        label_type, summary = self.selected_labels[idx]\n",
    "        \n",
    "        # ì¸ì½”ë” ì…ë ¥\n",
    "        encoder_inputs = self.tokenizer(\n",
    "            dialogue,\n",
    "            max_length=self.config['tokenizer']['encoder_max_len'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # ë””ì½”ë” ë ˆì´ë¸”\n",
    "        decoder_inputs = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=self.config['tokenizer']['decoder_max_len'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        labels = decoder_inputs['input_ids'].squeeze()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoder_inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': encoder_inputs['attention_mask'].squeeze(),\n",
    "            'labels': labels,\n",
    "        }\n",
    "\n",
    "# Student í† í¬ë‚˜ì´ì € ì¤€ë¹„\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(DISTILL_CONFIG['general']['student_model'])\n",
    "student_tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': DISTILL_CONFIG['tokenizer']['special_tokens']\n",
    "})\n",
    "\n",
    "# Distillation ë°ì´í„°ì…‹ ìƒì„±\n",
    "print(\"\\nğŸ“¦ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...\")\n",
    "train_distill_dataset = DistillationDataset(\n",
    "    dialogues=train_df['dialogue'].tolist(),\n",
    "    gold_labels=train_df['summary'].tolist(),\n",
    "    teacher_labels=teacher_pseudo_labels,\n",
    "    tokenizer=student_tokenizer,\n",
    "    config=DISTILL_CONFIG,\n",
    "    gold_ratio=DISTILL_CONFIG['training']['gold_label_ratio']\n",
    ")\n",
    "\n",
    "# Dev ë°ì´í„°ì…‹ (gold labelë§Œ ì‚¬ìš©)\n",
    "dev_df = pd.read_csv(\"./data/dev.csv\")\n",
    "dev_distill_dataset = DistillationDataset(\n",
    "    dialogues=dev_df['dialogue'].tolist(),\n",
    "    gold_labels=dev_df['summary'].tolist(),\n",
    "    teacher_labels=dev_df['summary'].tolist(),  # devëŠ” goldë§Œ\n",
    "    tokenizer=student_tokenizer,\n",
    "    config=DISTILL_CONFIG,\n",
    "    gold_ratio=1.0  # 100% gold\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… ë°ì´í„°ì…‹ êµ¬ì¶• ì™„ë£Œ\")\n",
    "print(f\"   Train: {len(train_distill_dataset)}ê°œ\")\n",
    "print(f\"   Dev: {len(dev_distill_dataset)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807aec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“ Step 3: Student ëª¨ë¸ Distillation í•™ìŠµ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“ Step 3: Student ëª¨ë¸ (gogamza) Distillation í•™ìŠµ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Student ëª¨ë¸ ë¡œë“œ (ê¸°ì¡´ fine-tuned ëª¨ë¸ì—ì„œ ì‹œì‘ ë˜ëŠ” fresh start)\n",
    "USE_PRETRAINED_STUDENT = True  # True: ê¸°ì¡´ best_model ì‚¬ìš©, False: ì²˜ìŒë¶€í„° í•™ìŠµ\n",
    "\n",
    "if USE_PRETRAINED_STUDENT and os.path.exists(\"./results_kobart_v2/best_model\"):\n",
    "    print(\"ğŸ“¥ ê¸°ì¡´ fine-tuned Student ëª¨ë¸ ë¡œë“œ...\")\n",
    "    student_model = BartForConditionalGeneration.from_pretrained(\"./results_kobart_v2/best_model\")\n",
    "else:\n",
    "    print(\"ğŸ“¥ Fresh Student ëª¨ë¸ ë¡œë“œ...\")\n",
    "    student_model = BartForConditionalGeneration.from_pretrained(DISTILL_CONFIG['general']['student_model'])\n",
    "\n",
    "student_model.resize_token_embeddings(len(student_tokenizer))\n",
    "print(f\"âœ… Student ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(f\"   íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in student_model.parameters()):,}\")\n",
    "\n",
    "# í•™ìŠµ ì¸ì ì„¤ì •\n",
    "distill_training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=DISTILL_CONFIG['general']['output_dir'],\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=DISTILL_CONFIG['training']['num_train_epochs'],\n",
    "    learning_rate=DISTILL_CONFIG['training']['learning_rate'],\n",
    "    per_device_train_batch_size=DISTILL_CONFIG['training']['per_device_train_batch_size'],\n",
    "    per_device_eval_batch_size=DISTILL_CONFIG['training']['per_device_eval_batch_size'],\n",
    "    warmup_ratio=DISTILL_CONFIG['training']['warmup_ratio'],\n",
    "    weight_decay=DISTILL_CONFIG['training']['weight_decay'],\n",
    "    lr_scheduler_type=DISTILL_CONFIG['training']['lr_scheduler_type'],\n",
    "    evaluation_strategy=DISTILL_CONFIG['training']['evaluation_strategy'],\n",
    "    save_strategy=DISTILL_CONFIG['training']['save_strategy'],\n",
    "    save_total_limit=DISTILL_CONFIG['training']['save_total_limit'],\n",
    "    fp16=DISTILL_CONFIG['training']['fp16'],\n",
    "    load_best_model_at_end=DISTILL_CONFIG['training']['load_best_model_at_end'],\n",
    "    metric_for_best_model=DISTILL_CONFIG['training']['metric_for_best_model'],\n",
    "    greater_is_better=DISTILL_CONFIG['training']['greater_is_better'],\n",
    "    label_smoothing_factor=DISTILL_CONFIG['training']['label_smoothing_factor'],\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=DISTILL_CONFIG['inference']['max_length'],\n",
    "    report_to=DISTILL_CONFIG['training']['report_to'],\n",
    "    logging_steps=50,\n",
    "    seed=DISTILL_CONFIG['general']['seed'],\n",
    ")\n",
    "\n",
    "# Trainer ì„¤ì •\n",
    "distill_trainer = Seq2SeqTrainer(\n",
    "    model=student_model,\n",
    "    args=distill_training_args,\n",
    "    train_dataset=train_distill_dataset,\n",
    "    eval_dataset=dev_distill_dataset,\n",
    "    tokenizer=student_tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "print(\"\\nğŸš€ Distillation í•™ìŠµ ì‹œì‘...\")\n",
    "print(f\"   Epochs: {DISTILL_CONFIG['training']['num_train_epochs']}\")\n",
    "print(f\"   Learning Rate: {DISTILL_CONFIG['training']['learning_rate']}\")\n",
    "print(f\"   Batch Size: {DISTILL_CONFIG['training']['per_device_train_batch_size']}\")\n",
    "\n",
    "# í•™ìŠµ ì‹¤í–‰\n",
    "distill_trainer.train()\n",
    "\n",
    "# ìµœì¢… ëª¨ë¸ ì €ì¥\n",
    "best_distill_path = os.path.join(DISTILL_CONFIG['general']['output_dir'], \"best_model\")\n",
    "distill_trainer.save_model(best_distill_path)\n",
    "student_tokenizer.save_pretrained(best_distill_path)\n",
    "\n",
    "print(f\"\\nâœ… Distillation í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(f\"   ëª¨ë¸ ì €ì¥: {best_distill_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693c77ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“Š Step 4: Distillation ëª¨ë¸ í‰ê°€ ë° ë¹„êµ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“Š Step 4: Distillation ëª¨ë¸ í‰ê°€\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Distillation ëª¨ë¸ ë¡œë“œ\n",
    "distill_model_path = \"./results_distillation/best_model\"\n",
    "print(f\"ğŸ“¥ Distillation ëª¨ë¸ ë¡œë“œ: {distill_model_path}\")\n",
    "\n",
    "distill_tokenizer = AutoTokenizer.from_pretrained(distill_model_path)\n",
    "distill_model = BartForConditionalGeneration.from_pretrained(distill_model_path)\n",
    "distill_model = distill_model.to(device)\n",
    "distill_model.eval()\n",
    "\n",
    "# Dev ë°ì´í„°ë¡œ í‰ê°€\n",
    "dev_df = pd.read_csv(\"./data/dev.csv\")\n",
    "\n",
    "def evaluate_model_morpheme_rouge(model, tokenizer, df, config):\n",
    "    \"\"\"ëª¨ë¸ í‰ê°€ - í˜•íƒœì†Œ ê¸°ë°˜ ROUGE\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"í‰ê°€\"):\n",
    "        inputs = tokenizer(\n",
    "            row['dialogue'],\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=config['tokenizer']['encoder_max_len'],\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                num_beams=config['inference']['num_beams'],\n",
    "                length_penalty=config['inference']['length_penalty'],\n",
    "                repetition_penalty=config['inference']['repetition_penalty'],\n",
    "                no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],\n",
    "                max_length=config['inference']['max_length'],\n",
    "                early_stopping=True,\n",
    "            )\n",
    "        \n",
    "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        pred = kiwi.space(pred, reset_whitespace=False)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # í˜•íƒœì†Œ ROUGE ê³„ì‚°\n",
    "    scores = []\n",
    "    for pred, ref in zip(predictions, df['summary'].tolist()):\n",
    "        score = compute_morpheme_rouge(pred, ref)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return predictions, np.mean(scores) * 100\n",
    "\n",
    "# Distillation ëª¨ë¸ í‰ê°€\n",
    "print(\"\\nğŸ”¬ Distillation ëª¨ë¸ í‰ê°€ ì¤‘...\")\n",
    "distill_preds, distill_score = evaluate_model_morpheme_rouge(\n",
    "    distill_model, distill_tokenizer, dev_df, DISTILL_CONFIG\n",
    ")\n",
    "\n",
    "# ê¸°ì¡´ ëª¨ë¸ ì ìˆ˜ (ì´ì „ ì‹¤í—˜ì—ì„œ)\n",
    "original_score = avg_single  # N-Best ë‹¨ì¼ beam ì ìˆ˜\n",
    "nbest_score_val = avg_nbest  # N-Best reranking ì ìˆ˜\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (Dev Set)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Original (gogamza fine-tuned):   {original_score:.2f}\")\n",
    "print(f\"N-Best Reranking:                {nbest_score_val:.2f}\")\n",
    "print(f\"Teacher-Student Distillation:    {distill_score:.2f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ë¦¬ë”ë³´ë“œ ì˜ˆìƒ ì ìˆ˜\n",
    "print(\"\\nğŸ¯ ë¦¬ë”ë³´ë“œ ì˜ˆìƒ ì ìˆ˜ (Ã—1.32 í™˜ì‚°)\")\n",
    "print(f\"Original:     {original_score * 1.32:.2f}\")\n",
    "print(f\"N-Best:       {nbest_score_val * 1.32:.2f}\")\n",
    "print(f\"Distillation: {distill_score * 1.32:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2b548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“¤ Step 5: ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± (Distillation + N-Best Reranking)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“¤ Step 5: ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Distillation ëª¨ë¸ë¡œ N-Best Reranking ì ìš©\n",
    "print(\"ğŸ”§ Distillation ëª¨ë¸ + N-Best Reranking ì¡°í•©\")\n",
    "\n",
    "# Test ë°ì´í„° ë¡œë“œ\n",
    "test_df = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "# Distillation ëª¨ë¸ë¡œ N-Best ìƒì„± í›„ Heuristic ì„ íƒ\n",
    "FINAL_NBEST_CONFIG = {\n",
    "    \"num_beams\": 12,\n",
    "    \"num_return_sequences\": 8,\n",
    "    \"length_penalty\": 1.0,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"max_length\": 150,\n",
    "    \"early_stopping\": True,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "final_summaries = []\n",
    "\n",
    "print(\"\\nğŸš€ ìµœì¢… ì¶”ë¡  ì¤‘ (Distillation + N-Best)...\")\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Final Inference\"):\n",
    "    dialogue = row['dialogue']\n",
    "    \n",
    "    inputs = distill_tokenizer(\n",
    "        dialogue,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=DISTILL_CONFIG['tokenizer']['encoder_max_len'],\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # N-Best í›„ë³´ ìƒì„±\n",
    "    with torch.no_grad():\n",
    "        outputs = distill_model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            **FINAL_NBEST_CONFIG\n",
    "        )\n",
    "    \n",
    "    candidates = []\n",
    "    for output in outputs:\n",
    "        text = distill_tokenizer.decode(output, skip_special_tokens=True)\n",
    "        text = kiwi.space(text, reset_whitespace=False)\n",
    "        candidates.append(text)\n",
    "    \n",
    "    # Heuristic: ìƒìœ„ 3ê°œ ì¤‘ ê¸¸ì´ ìµœì  ì„ íƒ\n",
    "    top_candidates = candidates[:3]\n",
    "    best_candidate = min(top_candidates, key=lambda x: abs(len(x) - dev_avg_len))\n",
    "    final_summaries.append(best_candidate)\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "submit_final = pd.DataFrame({\n",
    "    'fname': test_df['fname'],\n",
    "    'summary': final_summaries\n",
    "})\n",
    "\n",
    "# íŒŒì¼ ì €ì¥\n",
    "output_path_final = \"./prediction_kobart_v2/submit_distillation_nbest.csv\"\n",
    "submit_final.to_csv(output_path_final, index=False)\n",
    "\n",
    "print(f\"\\nâœ… ìµœì¢… ì œì¶œ íŒŒì¼ ì €ì¥: {output_path_final}\")\n",
    "\n",
    "# í†µê³„\n",
    "avg_len_final = np.mean([len(s) for s in final_summaries])\n",
    "print(f\"ğŸ“ í‰ê·  ìš”ì•½ ê¸¸ì´: {avg_len_final:.1f}ì\")\n",
    "\n",
    "# ìƒ˜í”Œ ì¶œë ¥\n",
    "print(\"\\nğŸ“‹ ìµœì¢… ì œì¶œ ìƒ˜í”Œ:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n[Test {i}]\")\n",
    "    print(f\"Summary: {final_summaries[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cada2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        ğŸ¯ ì ìˆ˜ í–¥ìƒ ì „ëµ ì‹¤í–‰ ê²°ê³¼                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ ì „ëµ                        â”‚ Dev í˜•íƒœì†Œ ROUGE â”‚ ì˜ˆìƒ ë¦¬ë”ë³´ë“œ  â”‚ í–¥ìƒ     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Original (submit_nb7)       â”‚     35.76        â”‚    47.48      â”‚ baseline â”‚\n",
    "â”‚ N-Best Reranking            â”‚     {:.2f}        â”‚    {:.2f}      â”‚ +{:.2f}    â”‚\n",
    "â”‚ Teacher-Student Distill     â”‚     {:.2f}        â”‚    {:.2f}      â”‚ +{:.2f}    â”‚\n",
    "â”‚ Distill + N-Best (ìµœì¢…)     â”‚     (ì¶”ë¡ ì¤‘)      â”‚    (ì¶”ë¡ ì¤‘)    â”‚ (ì¶”ë¡ ì¤‘) â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ ëª©í‘œ: digit82 ì ìˆ˜          â”‚     36.60        â”‚    48.32      â”‚ +0.84    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\".format(\n",
    "    nbest_score_val, nbest_score_val * 1.32, nbest_score_val - 35.76,\n",
    "    distill_score, distill_score * 1.32, distill_score - 35.76\n",
    "))\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“ ìƒì„±ëœ ì œì¶œ íŒŒì¼:\n",
    "   1. submit_nbest_reranking.csv      - N-Best Reranking ì ìš©\n",
    "   2. submit_distillation_nbest.csv   - Distillation + N-Best ì¡°í•©\n",
    "\n",
    "ğŸ”§ ì¶”ê°€ ìµœì í™” ì˜µì…˜:\n",
    "   - ë‘ íŒŒì¼ ì•™ìƒë¸” (Voting/Selection)\n",
    "   - Gold ratio ì¡°ì • (í˜„ì¬ 50%)\n",
    "   - Beam size ì¡°ì • (í˜„ì¬ 12)\n",
    "   - ë” ë§ì€ Teacher pseudo-label (Dev ë°ì´í„° ì¶”ê°€)\n",
    "\"\"\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "del distill_model, model_nbest\n",
    "torch.cuda.empty_cache()\n",
    "print(\"ğŸ—‘ï¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
