{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c25ea3",
   "metadata": {},
   "source": [
    "# üîÑ GEM-B1: ÏàúÏàò Î≤†Ïù¥Ïä§ÎùºÏù∏ ÌöåÍ∑Ä Ï†ÑÎûµ\n",
    "\n",
    "**Ï†êÏàò ÌöåÎ≥µÏùÑ ÏúÑÌïú Îã®ÏàúÌôî:**\n",
    "- ‚ùå TF-IDF ÌÇ§ÏõåÎìú ÌîÑÎ°¨ÌîÑÌä∏ Ï†úÍ±∞\n",
    "- ‚ùå N-Best Reranking Ï†úÍ±∞\n",
    "- ‚ùå ÎèôÏ†Å Ï∂îÎ°† ÏÑ§Ï†ï Ï†úÍ±∞\n",
    "- ‚úÖ ÌïôÏäµÍ≥º ÎèôÏùºÌïú ÏûÖÎ†• ÌòïÌÉú Ïú†ÏßÄ\n",
    "- ‚úÖ #Person# ÌÉúÍ∑∏ Î≥¥Ï°¥\n",
    "- ‚úÖ Îã®Ïàú beam searchÎßå ÏÇ¨Ïö©\n",
    "\n",
    "**Î™©Ìëú: 48Ï†êÎåÄ ÌöåÎ≥µ ‚Üí 49Ï†êÎåÄ ÎèÑÏ†Ñ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1929c82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "================================================================================\n",
      "üîÑ ÏàúÏàò Î≤†Ïù¥Ïä§ÎùºÏù∏ Î™®Îìú:\n",
      "   - TF-IDF ÌîÑÎ°¨ÌîÑÌä∏: ÎπÑÌôúÏÑ±Ìôî\n",
      "   - ÎèôÏ†Å Ï∂îÎ°†: ÎπÑÌôúÏÑ±Ìôî\n",
      "   - N-Best Reranking: ÎπÑÌôúÏÑ±Ìôî\n",
      "   - ÌïôÏäµ-Ï∂îÎ°† ÏùºÍ¥ÄÏÑ±: ÏµúÏö∞ÏÑ†\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üîÑ GEM-B1: ÏàúÏàò Î≤†Ïù¥Ïä§ÎùºÏù∏ ÌöåÍ∑Ä Ï†ÑÎûµ\n",
      "================================================================================\n",
      "\n",
      "Î™©Ìëú:\n",
      "  1. Î™®Îì† Î≥µÏû°Ìïú Ï†ÑÏ≤òÎ¶¨ Ï†úÍ±∞\n",
      "  2. ÌïôÏäµ-Ï∂îÎ°† ÏùºÍ¥ÄÏÑ± ÏµúÏö∞ÏÑ†\n",
      "  3. #Person# ÌÉúÍ∑∏ Î≥¥Ï°¥\n",
      "  4. Îã®Ïàú beam searchÎßå ÏÇ¨Ïö©\n",
      "  5. Ï†êÏàò 48Ï†êÎåÄ ÌöåÎ≥µ\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üìö Î™®Îç∏ ÌïôÏäµ ÏãúÏûë (ÏàúÏàò Î≤†Ïù¥Ïä§ÎùºÏù∏)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üîÑ GEM-B1: ÏàúÏàò Î≤†Ïù¥Ïä§ÎùºÏù∏ ÌöåÍ∑Ä Ï†ÑÎûµ\n",
      "================================================================================\n",
      "\n",
      "Î™©Ìëú:\n",
      "  1. Î™®Îì† Î≥µÏû°Ìïú Ï†ÑÏ≤òÎ¶¨ Ï†úÍ±∞\n",
      "  2. ÌïôÏäµ-Ï∂îÎ°† ÏùºÍ¥ÄÏÑ± ÏµúÏö∞ÏÑ†\n",
      "  3. #Person# ÌÉúÍ∑∏ Î≥¥Ï°¥\n",
      "  4. Îã®Ïàú beam searchÎßå ÏÇ¨Ïö©\n",
      "  5. Ï†êÏàò 48Ï†êÎåÄ ÌöåÎ≥µ\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üìö Î™®Îç∏ ÌïôÏäµ ÏãúÏûë (ÏàúÏàò Î≤†Ïù¥Ïä§ÎùºÏù∏)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Î™®Îç∏ Î°úÎìú ÏôÑÎ£å: digit82/kobart-summarization\n",
      "   Vocab size: 30010\n",
      "\n",
      "üöÄ Training Start...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 23.69 GiB of which 268.94 MiB is free. Process 2610232 has 3.15 GiB memory in use. Process 2795486 has 14.77 GiB memory in use. Process 3253811 has 5.50 GiB memory in use. Of the allocated memory 5.12 GiB is allocated by PyTorch, and 74.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 430\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# 1. ÌïôÏäµ Ïã§Ìñâ\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m best_ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# 2. ÏàúÏàò Î≤†Ïù¥Ïä§ÎùºÏù∏ Ï∂îÎ°†\u001b[39;00m\n\u001b[1;32m    433\u001b[0m output \u001b[38;5;241m=\u001b[39m inference_pure_baseline(best_ckpt)\n",
      "Cell \u001b[0;32mIn[1], line 311\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m    301\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    302\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping]\n\u001b[1;32m    308\u001b[0m )\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müöÄ Training Start...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 311\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# Best Model Ï†ÄÏû•\u001b[39;00m\n\u001b[1;32m    314\u001b[0m best_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CONF[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneral\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2725\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2722\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2724\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2725\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2728\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2748\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2747\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2748\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2749\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2750\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:680\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:668\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1577\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1572\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1573\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1574\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1575\u001b[0m         )\n\u001b[0;32m-> 1577\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1595\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1596\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m lm_logits \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\u001b[38;5;241m.\u001b[39mto(lm_logits\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1445\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1442\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1445\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1446\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1450\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1453\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1074\u001b[0m, in \u001b[0;36mBartEncoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1067\u001b[0m             encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1068\u001b[0m             hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1071\u001b[0m             output_attentions,\n\u001b[1;32m   1072\u001b[0m         )\n\u001b[1;32m   1073\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1074\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:537\u001b[0m, in \u001b[0;36mBartEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;124;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;124;03m        returned tensors for more detail.\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    536\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 537\u001b[0m hidden_states, attn_weights, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    544\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:247\u001b[0m, in \u001b[0;36mBartAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    244\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39mview(bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len) \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[1;32m    245\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len)\n\u001b[0;32m--> 247\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer_head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layer_head_mask\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1856\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1854\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1856\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 23.69 GiB of which 268.94 MiB is free. Process 2610232 has 3.15 GiB memory in use. Process 2795486 has 14.77 GiB memory in use. Process 3253811 has 5.50 GiB memory in use. Of the allocated memory 5.12 GiB is allocated by PyTorch, and 74.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from rouge import Rouge\n",
    "\n",
    "# Transformers & Torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    BartForConditionalGeneration, \n",
    "    BartConfig,\n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Configuration & Seed Setting (ÏÑ§Ï†ï Î∞è ÏãúÎìú Í≥†Ï†ï)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ÏàúÏàò Î≤†Ïù¥Ïä§ÎùºÏù∏ ÏÑ§Ï†ï (Î≥µÏû°Ìïú Ï†ÑÏ≤òÎ¶¨ Î™®Îëê Ï†úÍ±∞)\n",
    "CONF = {\n",
    "    \"general\": {\n",
    "        \"data_path\": \"./data/\",\n",
    "        \"model_name\": \"digit82/kobart-summarization\",\n",
    "        \"output_dir\": \"./results_gem_b1_pure_baseline\",\n",
    "        \"seed\": 42\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "        \"encoder_max_len\": 512,\n",
    "        \"decoder_max_len\": 128,\n",
    "        \"special_tokens\": ['#Person1#', '#Person2#', '#Person3#', '#Person4#', \n",
    "                          '#Person5#', '#Person6#', '#Person7#', \n",
    "                          '#PhoneNumber#', '#Address#', '#PassportNumber#']\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"num_train_epochs\": 15,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"per_device_train_batch_size\": 32,\n",
    "        \"per_device_eval_batch_size\": 32,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lr_scheduler_type\": 'cosine',\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"evaluation_strategy\": 'epoch',\n",
    "        \"save_strategy\": 'epoch',\n",
    "        \"save_total_limit\": 3,\n",
    "        \"fp16\": True,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"metric_for_best_model\": \"combined_score\",\n",
    "        \"greater_is_better\": True,\n",
    "        \"logging_dir\": \"./logs\",\n",
    "        \"logging_strategy\": \"steps\",\n",
    "        \"logging_steps\": 100,\n",
    "        \"predict_with_generate\": True,\n",
    "        \"generation_max_length\": 128,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"report_to\": \"none\"\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"result_path\": \"./prediction/\",\n",
    "        # ÏàúÏàò Î≤†Ïù¥Ïä§ÎùºÏù∏ Ï∂îÎ°† ÏÑ§Ï†ï\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "        \"early_stopping\": True,\n",
    "        \"max_length\": 100,\n",
    "        \"num_beams\": 4,\n",
    "        \"length_penalty\": 1.0,\n",
    "        \"batch_size\": 32,\n",
    "    }\n",
    "}\n",
    "\n",
    "seed_everything(CONF['general']['seed'])\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "print(\"=\" * 80)\n",
    "print(\"üîÑ ÏàúÏàò Î≤†Ïù¥Ïä§ÎùºÏù∏ Î™®Îìú:\")\n",
    "print(\"   - TF-IDF ÌîÑÎ°¨ÌîÑÌä∏: ÎπÑÌôúÏÑ±Ìôî\")\n",
    "print(\"   - ÎèôÏ†Å Ï∂îÎ°†: ÎπÑÌôúÏÑ±Ìôî\")\n",
    "print(\"   - N-Best Reranking: ÎπÑÌôúÏÑ±Ìôî\")\n",
    "print(\"   - ÌïôÏäµ-Ï∂îÎ°† ÏùºÍ¥ÄÏÑ±: ÏµúÏö∞ÏÑ†\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. Data Preprocessing (Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Tokenizer Î°úÎìú Î∞è Special Token Ï∂îÍ∞Ä\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONF['general']['model_name'])\n",
    "special_tokens_dict = {'additional_special_tokens': CONF['tokenizer']['special_tokens']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "CONF['tokenizer']['bos_token'] = tokenizer.bos_token\n",
    "CONF['tokenizer']['eos_token'] = tokenizer.eos_token\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, bos_token, eos_token):\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "\n",
    "    def make_set_as_df(self, file_path, is_train=True):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if is_train:\n",
    "            return df[['fname', 'dialogue', 'summary']]\n",
    "        else:\n",
    "            return df[['fname', 'dialogue']]\n",
    "\n",
    "    def make_input(self, dataset, is_test=False):\n",
    "        if is_test:\n",
    "            encoder_input = dataset['dialogue']\n",
    "            decoder_input = [self.bos_token] * len(dataset['dialogue'])\n",
    "            return encoder_input.tolist(), list(decoder_input)\n",
    "        else:\n",
    "            encoder_input = dataset['dialogue']\n",
    "            decoder_input = dataset['summary'].apply(lambda x: self.bos_token + str(x))\n",
    "            decoder_output = dataset['summary'].apply(lambda x: str(x) + self.eos_token)\n",
    "            return encoder_input.tolist(), decoder_input.tolist(), decoder_output.tolist()\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, labels=None, is_test=False, ids=None):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.labels = labels\n",
    "        self.is_test = is_test\n",
    "        self.ids = ids\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
    "        \n",
    "        if 'input_ids' in self.decoder_input:\n",
    "            item['decoder_input_ids'] = self.decoder_input['input_ids'][idx].clone().detach()\n",
    "            item['decoder_attention_mask'] = self.decoder_input['attention_mask'][idx].clone().detach()\n",
    "\n",
    "        if not self.is_test:\n",
    "            item['labels'] = self.labels['input_ids'][idx].clone().detach()\n",
    "            \n",
    "        if self.ids is not None:\n",
    "            item['ID'] = self.ids[idx]\n",
    "            \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoder_input['input_ids'])\n",
    "\n",
    "def prepare_data(conf, tokenizer, is_train=True):\n",
    "    preprocessor = Preprocess(conf['tokenizer']['bos_token'], conf['tokenizer']['eos_token'])\n",
    "    data_path = conf['general']['data_path']\n",
    "    \n",
    "    if is_train:\n",
    "        train_df = preprocessor.make_set_as_df(os.path.join(data_path, 'train.csv'))\n",
    "        val_df = preprocessor.make_set_as_df(os.path.join(data_path, 'dev.csv'))\n",
    "        \n",
    "        # Train Data\n",
    "        enc_train, dec_in_train, dec_out_train = preprocessor.make_input(train_df)\n",
    "        tokenized_enc_train = tokenizer(enc_train, return_tensors=\"pt\", padding=True, truncation=True, max_length=conf['tokenizer']['encoder_max_len'])\n",
    "        tokenized_dec_in_train = tokenizer(dec_in_train, return_tensors=\"pt\", padding=True, truncation=True, max_length=conf['tokenizer']['decoder_max_len'])\n",
    "        tokenized_dec_out_train = tokenizer(dec_out_train, return_tensors=\"pt\", padding=True, truncation=True, max_length=conf['tokenizer']['decoder_max_len'])\n",
    "        \n",
    "        train_dataset = CustomDataset(tokenized_enc_train, tokenized_dec_in_train, tokenized_dec_out_train)\n",
    "        \n",
    "        # Val Data\n",
    "        enc_val, dec_in_val, dec_out_val = preprocessor.make_input(val_df)\n",
    "        tokenized_enc_val = tokenizer(enc_val, return_tensors=\"pt\", padding=True, truncation=True, max_length=conf['tokenizer']['encoder_max_len'])\n",
    "        tokenized_dec_in_val = tokenizer(dec_in_val, return_tensors=\"pt\", padding=True, truncation=True, max_length=conf['tokenizer']['decoder_max_len'])\n",
    "        tokenized_dec_out_val = tokenizer(dec_out_val, return_tensors=\"pt\", padding=True, truncation=True, max_length=conf['tokenizer']['decoder_max_len'])\n",
    "        \n",
    "        val_dataset = CustomDataset(tokenized_enc_val, tokenized_dec_in_val, tokenized_dec_out_val)\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "    \n",
    "    else: # Test\n",
    "        test_df = preprocessor.make_set_as_df(os.path.join(data_path, 'test.csv'), is_train=False)\n",
    "        enc_test, dec_in_test = preprocessor.make_input(test_df, is_test=True)\n",
    "        \n",
    "        tokenized_enc_test = tokenizer(enc_test, return_tensors=\"pt\", padding=True, truncation=True, max_length=conf['tokenizer']['encoder_max_len'])\n",
    "        tokenized_dec_in_test = tokenizer(dec_in_test, return_tensors=\"pt\", padding=True, truncation=True, max_length=conf['tokenizer']['decoder_max_len'])\n",
    "        \n",
    "        test_dataset = CustomDataset(tokenized_enc_test, tokenized_dec_in_test, is_test=True, \n",
    "                                     ids=test_df['fname'].tolist())\n",
    "        return test_df, test_dataset\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. Model Training (Î™®Îç∏ ÌïôÏäµ)\n",
    "# -----------------------------------------------------------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    rouge = Rouge()\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    \n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # skip_special_tokens=FalseÎ°ú #Person# ÌÉúÍ∑∏ Î≥¥Ï°¥\n",
    "    decoded_preds = tokenizer.batch_decode(predictions.tolist(), skip_special_tokens=False)\n",
    "    decoded_labels = tokenizer.batch_decode(labels.tolist(), skip_special_tokens=False)\n",
    "    \n",
    "    # ÏãúÏä§ÌÖú ÌÜ†ÌÅ∞Îßå Ï†úÍ±∞ (#Person# Î≥¥Ï°¥)\n",
    "    remove_tokens = [tokenizer.bos_token, tokenizer.eos_token, tokenizer.pad_token, '<usr>']\n",
    "    \n",
    "    def clean_text(text_list):\n",
    "        cleaned = []\n",
    "        for text in text_list:\n",
    "            for token in remove_tokens:\n",
    "                if token is not None:\n",
    "                    text = text.replace(token, \"\").strip() \n",
    "            cleaned.append(text)\n",
    "        return cleaned\n",
    "\n",
    "    decoded_preds = clean_text(decoded_preds)\n",
    "    decoded_labels = clean_text(decoded_labels)\n",
    "    \n",
    "    try:\n",
    "        results = rouge.get_scores(decoded_preds, decoded_labels, avg=True)\n",
    "        r1 = results[\"rouge-1\"][\"f\"]\n",
    "        r2 = results[\"rouge-2\"][\"f\"]\n",
    "        rl = results[\"rouge-l\"][\"f\"]\n",
    "        \n",
    "        combined_score = (r1 + r2 + rl) / 3\n",
    "        \n",
    "        return {\n",
    "            \"rouge-1\": r1,\n",
    "            \"rouge-2\": r2,\n",
    "            \"rouge-l\": rl,\n",
    "            \"combined_score\": combined_score\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"combined_score\": 0.0}\n",
    "\n",
    "\n",
    "def train():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìö Î™®Îç∏ ÌïôÏäµ ÏãúÏûë (ÏàúÏàò Î≤†Ïù¥Ïä§ÎùºÏù∏)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "    train_dataset, val_dataset = prepare_data(CONF, tokenizer, is_train=True)\n",
    "    \n",
    "    # Î™®Îç∏ Î°úÎìú\n",
    "    model = BartForConditionalGeneration.from_pretrained(CONF['general']['model_name'])\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"‚úÖ Î™®Îç∏ Î°úÎìú ÏôÑÎ£å: {CONF['general']['model_name']}\")\n",
    "    print(f\"   Vocab size: {len(tokenizer)}\")\n",
    "    \n",
    "    # Training Arguments\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=CONF['general']['output_dir'],\n",
    "        overwrite_output_dir=CONF['training']['overwrite_output_dir'],\n",
    "        num_train_epochs=CONF['training']['num_train_epochs'],\n",
    "        learning_rate=CONF['training']['learning_rate'],\n",
    "        per_device_train_batch_size=CONF['training']['per_device_train_batch_size'],\n",
    "        per_device_eval_batch_size=CONF['training']['per_device_eval_batch_size'],\n",
    "        warmup_ratio=CONF['training']['warmup_ratio'],\n",
    "        weight_decay=CONF['training']['weight_decay'],\n",
    "        lr_scheduler_type=CONF['training']['lr_scheduler_type'],\n",
    "        optim=CONF['training']['optim'],\n",
    "        evaluation_strategy=CONF['training']['evaluation_strategy'],\n",
    "        save_strategy=CONF['training']['save_strategy'],\n",
    "        save_total_limit=CONF['training']['save_total_limit'],\n",
    "        fp16=CONF['training']['fp16'],\n",
    "        load_best_model_at_end=CONF['training']['load_best_model_at_end'],\n",
    "        metric_for_best_model=CONF['training']['metric_for_best_model'],\n",
    "        greater_is_better=CONF['training']['greater_is_better'],\n",
    "        logging_dir=CONF['training']['logging_dir'],\n",
    "        logging_steps=CONF['training']['logging_steps'],\n",
    "        predict_with_generate=CONF['training']['predict_with_generate'],\n",
    "        generation_max_length=CONF['training']['generation_max_length'],\n",
    "        report_to=CONF['training']['report_to'],\n",
    "        seed=CONF['general']['seed']\n",
    "    )\n",
    "    \n",
    "    # Early Stopping\n",
    "    early_stopping = EarlyStoppingCallback(\n",
    "        early_stopping_patience=CONF['training']['early_stopping_patience']\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüöÄ Training Start...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Best Model Ï†ÄÏû•\n",
    "    best_model_path = os.path.join(CONF['general']['output_dir'], \"best_model\")\n",
    "    trainer.save_model(best_model_path)\n",
    "    print(f\"\\n‚úÖ Training Finished. Best Model Saved at {best_model_path}\")\n",
    "    \n",
    "    return best_model_path\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. Pure Baseline Inference (ÏàúÏàò Î≤†Ïù¥Ïä§ÎùºÏù∏ Ï∂îÎ°†)\n",
    "# -----------------------------------------------------------------------------\n",
    "def inference_pure_baseline(model_path=None):\n",
    "    \"\"\"\n",
    "    ÏàúÏàò Î≤†Ïù¥Ïä§ÎùºÏù∏ Ï∂îÎ°†\n",
    "    - ÌîÑÎ°¨ÌîÑÌä∏ ÏóÜÏùå\n",
    "    - Ï†ÑÏ≤òÎ¶¨ ÏóÜÏùå\n",
    "    - Îã®Ïàú beam search\n",
    "    - #Person# ÌÉúÍ∑∏ Î≥¥Ï°¥\n",
    "    \"\"\"\n",
    "    if model_path is None:\n",
    "        model_path = os.path.join(CONF['general']['output_dir'], \"best_model\")\n",
    "        \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"üîÑ ÏàúÏàò Î≤†Ïù¥Ïä§ÎùºÏù∏ Ï∂îÎ°†\")\n",
    "    print(f\"   Model: {model_path}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "    test_df, test_dataset = prepare_data(CONF, tokenizer, is_train=False)\n",
    "    dataloader = DataLoader(test_dataset, batch_size=CONF['inference']['batch_size'], shuffle=False)\n",
    "    \n",
    "    summary_list = []\n",
    "    fname_list = []\n",
    "    \n",
    "    # ÏãúÏä§ÌÖú ÌÜ†ÌÅ∞Îßå Ï†úÍ±∞ (#Person# Î≥¥Ï°¥)\n",
    "    system_tokens = [tokenizer.bos_token, tokenizer.eos_token, tokenizer.pad_token, '<usr>']\n",
    "\n",
    "    print(\"\\nüöÄ Inference Start...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Îã®Ïàú beam search\n",
    "            summary_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                num_beams=CONF['inference']['num_beams'],\n",
    "                max_length=CONF['inference']['max_length'],\n",
    "                no_repeat_ngram_size=CONF['inference']['no_repeat_ngram_size'],\n",
    "                early_stopping=CONF['inference']['early_stopping'],\n",
    "                length_penalty=CONF['inference']['length_penalty']\n",
    "            )\n",
    "            \n",
    "            # skip_special_tokens=FalseÎ°ú #Person# Î≥¥Ï°¥\n",
    "            decoded = tokenizer.batch_decode(summary_ids, skip_special_tokens=False)\n",
    "            \n",
    "            # ÏãúÏä§ÌÖú ÌÜ†ÌÅ∞Îßå Ï†úÍ±∞\n",
    "            cleaned_batch = []\n",
    "            for text in decoded:\n",
    "                for token in system_tokens:\n",
    "                    if token is not None:\n",
    "                        text = text.replace(token, \"\").strip()\n",
    "                # Ïó∞ÏÜç Í≥µÎ∞± Ï†úÍ±∞\n",
    "                text = ' '.join(text.split())\n",
    "                cleaned_batch.append(text)\n",
    "            \n",
    "            summary_list.extend(cleaned_batch)\n",
    "            fname_list.extend(batch['ID'])\n",
    "    \n",
    "    # Í≤∞Í≥º Ï†ÄÏû•\n",
    "    result_path = CONF['inference']['result_path']\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "    \n",
    "    output_df = pd.DataFrame({\n",
    "        \"fname\": fname_list,\n",
    "        \"summary\": summary_list\n",
    "    })\n",
    "    \n",
    "    save_file = os.path.join(result_path, \"submit_gem_b1_pure_baseline.csv\")\n",
    "    output_df.to_csv(save_file, index=False)\n",
    "    print(f\"\\n‚úÖ Inference Finished. Saved at {save_file}\")\n",
    "    \n",
    "    # ÌÜµÍ≥Ñ Ï∂úÎ†•\n",
    "    lengths = [len(s) for s in summary_list]\n",
    "    print(f\"\\nüìä ÏöîÏïΩÎ¨∏ ÌÜµÍ≥Ñ:\")\n",
    "    print(f\"   ÌèâÍ∑† Í∏∏Ïù¥: {np.mean(lengths):.1f}Ïûê\")\n",
    "    print(f\"   ÏµúÏÜå/ÏµúÎåÄ: {min(lengths)}/{max(lengths)}Ïûê\")\n",
    "    \n",
    "    # ÏÉòÌîå Ï∂úÎ†•\n",
    "    print(f\"\\nüìù ÏÉòÌîå ÏöîÏïΩ (Ï≤òÏùå 3Í∞ú):\")\n",
    "    for i in range(min(3, len(summary_list))):\n",
    "        print(f\"   [{i+1}] {summary_list[i][:100]}...\")\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. Main Execution\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üîÑ GEM-B1: ÏàúÏàò Î≤†Ïù¥Ïä§ÎùºÏù∏ ÌöåÍ∑Ä Ï†ÑÎûµ\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nÎ™©Ìëú:\")\n",
    "    print(\"  1. Î™®Îì† Î≥µÏû°Ìïú Ï†ÑÏ≤òÎ¶¨ Ï†úÍ±∞\")\n",
    "    print(\"  2. ÌïôÏäµ-Ï∂îÎ°† ÏùºÍ¥ÄÏÑ± ÏµúÏö∞ÏÑ†\")\n",
    "    print(\"  3. #Person# ÌÉúÍ∑∏ Î≥¥Ï°¥\")\n",
    "    print(\"  4. Îã®Ïàú beam searchÎßå ÏÇ¨Ïö©\")\n",
    "    print(\"  5. Ï†êÏàò 48Ï†êÎåÄ ÌöåÎ≥µ\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. ÌïôÏäµ Ïã§Ìñâ\n",
    "    best_ckpt = train()\n",
    "    \n",
    "    # 2. ÏàúÏàò Î≤†Ïù¥Ïä§ÎùºÏù∏ Ï∂îÎ°†\n",
    "    output = inference_pure_baseline(best_ckpt)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ Î™®Îì† ÏûëÏóÖ ÏôÑÎ£å!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nÏÉùÏÑ±Îêú Ï†úÏ∂ú ÌååÏùº:\")\n",
    "    print(f\"  - submit_gem_b1_pure_baseline.csv\")\n",
    "    print(f\"\\nüí° Ïù¥ ÌååÏùºÏùÑ Î¶¨ÎçîÎ≥¥ÎìúÏóê Ï†úÏ∂úÌïòÏó¨ Ï†êÏàò ÌöåÎ≥µÏùÑ ÌôïÏù∏ÌïòÏÑ∏Ïöî!\")\n",
    "    print(f\"   ÏòàÏÉÅ Ï†êÏàò: 48Ï†êÎåÄ\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b701cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Í∏∞Ï°¥ ÌïôÏäµÎêú Î™®Îç∏Î°ú ÏàúÏàò Î≤†Ïù¥Ïä§ÎùºÏù∏ Ï∂îÎ°† (Î©îÎ™®Î¶¨ Ìö®Ïú®Ï†Å)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîÑ Í∏∞Ï°¥ ÌïôÏäµ Î™®Îç∏Î°ú ÏàúÏàò Î≤†Ïù¥Ïä§ÎùºÏù∏ Ï∂îÎ°†\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Í∏∞Ï°¥ Î™®Îç∏ Í≤ΩÎ°ú\n",
    "MODEL_PATH = \"./prediction_kobart_v1/checkpoint-4674\"\n",
    "\n",
    "print(f\"\\n‚úÖ Í∏∞Ï°¥ Î™®Îç∏ ÏÇ¨Ïö©: {MODEL_PATH}\")\n",
    "print(\"   Î©îÎ™®Î¶¨ Î∂ÄÏ°±ÏúºÎ°ú Ïû¨ÌïôÏäµ ÎåÄÏã† Í∏∞Ï°¥ Î™®Îç∏ ÌôúÏö©\")\n",
    "print(\"   Î™®Îì† Ï†ÑÏ≤òÎ¶¨ Ï†úÍ±∞ÌïòÍ≥† ÏàúÏàò Ï∂îÎ°†Îßå ÏàòÌñâ\")\n",
    "\n",
    "# ÏàúÏàò Î≤†Ïù¥Ïä§ÎùºÏù∏ Ï∂îÎ°† Ïã§Ìñâ\n",
    "output = inference_pure_baseline(MODEL_PATH)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ ÏàúÏàò Î≤†Ïù¥Ïä§ÎùºÏù∏ Ï∂îÎ°† ÏôÑÎ£å!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nÏÉùÏÑ±Îêú Ï†úÏ∂ú ÌååÏùº:\")\n",
    "print(f\"  - submit_gem_b1_pure_baseline.csv\")\n",
    "print(f\"\\nüí° Î¶¨ÎçîÎ≥¥ÎìúÏóê Ï†úÏ∂úÌïòÏó¨ Ï†êÏàò ÌöåÎ≥µÏùÑ ÌôïÏù∏ÌïòÏÑ∏Ïöî!\")\n",
    "print(f\"   ÏòàÏÉÅ: 48Ï†êÎåÄÎ°ú ÌöåÎ≥µ (Ïù¥Ï†Ñ 39.67Ï†ê ‚Üí 48Ï†êÎåÄ)\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
