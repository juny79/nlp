{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee3469cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 1. 환경 설정 및 라이브러리 로드\n",
    "!pip install -q -U torch transformers peft bitsandbytes accelerate pandas datasets evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b22106ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'Could not load this library: /opt/conda/lib/python3.10/site-packages/torchvision/image.so'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "r = load(\"rouge\")\n",
    "print(r.compute(predictions=[\"이것은 테스트 요약입니다.\"], references=[\"이것은 테스트 요약입니다.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e772f8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# 설정\n",
    "CONF = {\n",
    "    \"base_model\": \"upstage/SOLAR-10.7B-Instruct-v1.0\",\n",
    "    \"adapter_path\": \"./results_solar\",  # 학습된 어댑터 경로\n",
    "    \"data_path\": \"./data/\",\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede979dd",
   "metadata": {},
   "source": [
    "## 2. 모델 및 토크나이저 로드 (Base + Adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13f421ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722c232f6a9448169c292a3724491126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4-bit Quantization Config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load Base Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONF['base_model'],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load Adapter\n",
    "model = PeftModel.from_pretrained(base_model, CONF['adapter_path'])\n",
    "model.eval()\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONF['base_model'], trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b75113",
   "metadata": {},
   "source": [
    "## 3. 데이터 로드 및 평가 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a498ad1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set size: 499\n"
     ]
    }
   ],
   "source": [
    "# Load Dev Data for Validation\n",
    "dev_df = pd.read_csv(os.path.join(CONF['data_path'], 'dev.csv'))\n",
    "print(f\"Dev set size: {len(dev_df)}\")\n",
    "\n",
    "# ROUGE Metric (use `evaluate` instead of deprecated `datasets.load_metric`)\n",
    "from evaluate import load as load_metric\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "\n",
    "def compute_rouge(predictions, references):\n",
    "    # evaluate returns Rouge object with attributes, not nested dicts\n",
    "    results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "    # Extract fmeasure from the Rouge score objects\n",
    "    return {\n",
    "        \"rouge1\": results[\"rouge1\"].mid.fmeasure if hasattr(results[\"rouge1\"], \"mid\") else results[\"rouge1\"],\n",
    "        \"rouge2\": results[\"rouge2\"].mid.fmeasure if hasattr(results[\"rouge2\"], \"mid\") else results[\"rouge2\"],\n",
    "        \"rougeL\": results[\"rougeL\"].mid.fmeasure if hasattr(results[\"rougeL\"], \"mid\") else results[\"rougeL\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f52a59",
   "metadata": {},
   "source": [
    "## 4. 실험 1: Prompt Engineering\n",
    "다양한 프롬프트 스타일을 테스트하여 모델이 요약을 더 잘 생성하도록 유도합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4d61210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prompt Engineering Test ===\n",
      "Prompt [basic]: R1=0.8033, R2=0.4756, RL=0.7467\n",
      "Prompt [korean_explicit]: R1=0.7400, R2=0.4100, RL=0.6900\n",
      "Prompt [concise]: R1=0.8200, R2=0.4756, RL=0.7633\n",
      "Prompt [detailed]: R1=0.8400, R2=0.4590, RL=0.7500\n"
     ]
    }
   ],
   "source": [
    "prompts = {\n",
    "    \"basic\": \"### User:\\nSummarize the following dialogue:\\n\\n{dialogue}\\n\\n### Assistant:\\n\",\n",
    "    \"korean_explicit\": \"### User:\\n다음 대화를 한국어로 요약해줘:\\n\\n{dialogue}\\n\\n### Assistant:\\n\",\n",
    "    \"concise\": \"### User:\\nSummarize the dialogue briefly in 3 sentences:\\n\\n{dialogue}\\n\\n### Assistant:\\n\",\n",
    "    \"detailed\": \"### User:\\nProvide a detailed summary of the conversation:\\n\\n{dialogue}\\n\\n### Assistant:\\n\"\n",
    "}\n",
    "\n",
    "def run_inference(model, tokenizer, dialogue, prompt_template, params):\n",
    "    prompt = prompt_template.format(dialogue=dialogue)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            **params,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    try:\n",
    "        summary = generated_text.split(\"### Assistant:\\n\")[1].strip()\n",
    "    except:\n",
    "        summary = generated_text\n",
    "    return summary\n",
    "\n",
    "# 테스트용 샘플 (속도를 위해 10개만)\n",
    "sample_dev = dev_df.head(10)\n",
    "\n",
    "print(\"=== Prompt Engineering Test ===\")\n",
    "default_params = {\"max_new_tokens\": 128, \"do_sample\": False, \"repetition_penalty\": 1.2}\n",
    "\n",
    "for name, template in prompts.items():\n",
    "    preds = []\n",
    "    for dialogue in sample_dev['dialogue']:\n",
    "        preds.append(run_inference(model, tokenizer, dialogue, template, default_params))\n",
    "    \n",
    "    scores = compute_rouge(preds, sample_dev['summary'].tolist())\n",
    "    print(f\"Prompt [{name}]: R1={scores['rouge1']:.4f}, R2={scores['rouge2']:.4f}, RL={scores['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef875d63",
   "metadata": {},
   "source": [
    "## 5. 실험 2: Decoding Strategies (Grid Search)\n",
    "Beam Search, Repetition Penalty, No Repeat N-gram Size 등을 조합하여 최적의 파라미터를 찾습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e9e16ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Decoding Strategy Search ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Greedy (Baseline): 100%|██████████| 20/20 [06:13<00:00, 18.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config [Greedy (Baseline)]: R1=0.7751, R2=0.4634, RL=0.7221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beam-3: 100%|██████████| 20/20 [20:12<00:00, 60.63s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config [Beam-3]: R1=0.7764, R2=0.4370, RL=0.6833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beam-5 + NoRepeat: 100%|██████████| 20/20 [35:57<00:00, 107.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config [Beam-5 + NoRepeat]: R1=0.3439, R2=0.1525, RL=0.3099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Length Penalty: 100%|██████████| 20/20 [20:59<00:00, 62.98s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config [Length Penalty]: R1=0.7964, R2=0.4791, RL=0.7063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 실험할 파라미터 조합\n",
    "search_space = [\n",
    "    {\"name\": \"Greedy (Baseline)\", \"params\": {\"max_new_tokens\": 150, \"do_sample\": False, \"repetition_penalty\": 1.2}},\n",
    "    {\"name\": \"Beam-3\", \"params\": {\"max_new_tokens\": 150, \"num_beams\": 3, \"early_stopping\": True, \"repetition_penalty\": 1.2}},\n",
    "    {\"name\": \"Beam-5 + NoRepeat\", \"params\": {\"max_new_tokens\": 150, \"num_beams\": 5, \"no_repeat_ngram_size\": 3, \"repetition_penalty\": 1.2}},\n",
    "    {\"name\": \"Length Penalty\", \"params\": {\"max_new_tokens\": 150, \"num_beams\": 3, \"length_penalty\": 1.2, \"repetition_penalty\": 1.2}},\n",
    "]\n",
    "\n",
    "# 최적의 프롬프트 선택 (위 실험 결과에 따라 수정 가능, 여기선 기본값 사용)\n",
    "best_prompt = prompts[\"basic\"] \n",
    "\n",
    "print(\"=== Decoding Strategy Search ===\")\n",
    "for config in search_space:\n",
    "    preds = []\n",
    "    # 전체 Dev 셋으로 검증하면 좋지만 시간상 20개로 테스트\n",
    "    eval_subset = dev_df.head(20)\n",
    "    \n",
    "    for dialogue in tqdm(eval_subset['dialogue'], desc=config['name']):\n",
    "        preds.append(run_inference(model, tokenizer, dialogue, best_prompt, config['params']))\n",
    "    \n",
    "    scores = compute_rouge(preds, eval_subset['summary'].tolist())\n",
    "    print(f\"Config [{config['name']}]: R1={scores['rouge1']:.4f}, R2={scores['rouge2']:.4f}, RL={scores['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55865f5",
   "metadata": {},
   "source": [
    "## 6. 최종 추론 및 제출 파일 생성\n",
    "위 실험에서 얻은 최적의 파라미터와 프롬프트를 적용하여 Test 셋에 대한 추론을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10cf5dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Final Inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 499/499 [9:08:05<00:00, 65.90s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./prediction/submit_solar_tuned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: 위 실험 결과를 바탕으로 최적의 파라미터로 수정하세요.\n",
    "FINAL_PROMPT = prompts[\"basic\"] \n",
    "FINAL_PARAMS = {\n",
    "    \"max_new_tokens\": 150,\n",
    "    \"num_beams\": 3,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"length_penalty\": 1.2\n",
    "}\n",
    "\n",
    "def post_process(text):\n",
    "    # 간단한 후처리: 문장이 중간에 끊긴 경우 마지막 마침표까지만 자르기\n",
    "    if '.' in text:\n",
    "        last_period_index = text.rfind('.')\n",
    "        return text[:last_period_index+1]\n",
    "    return text\n",
    "\n",
    "# Load Test Data\n",
    "test_df = pd.read_csv(os.path.join(CONF['data_path'], 'test.csv'))\n",
    "\n",
    "print(\"Starting Final Inference...\")\n",
    "final_summaries = []\n",
    "\n",
    "for dialogue in tqdm(test_df['dialogue']):\n",
    "    raw_summary = run_inference(model, tokenizer, dialogue, FINAL_PROMPT, FINAL_PARAMS)\n",
    "    clean_summary = post_process(raw_summary)\n",
    "    final_summaries.append(clean_summary)\n",
    "\n",
    "# Save Results\n",
    "submission = pd.DataFrame({\n",
    "    'fname': test_df['fname'],\n",
    "    'summary': final_summaries\n",
    "})\n",
    "\n",
    "submission.to_csv(\"./prediction/submit_solar_tuned.csv\", index=False)\n",
    "print(\"Saved to ./prediction/submit_solar_tuned.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
