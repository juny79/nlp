{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3469cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ\n",
    "!pip install -q -U torch transformers peft bitsandbytes accelerate pandas datasets evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22106ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'Could not load this library: /opt/conda/lib/python3.10/site-packages/torchvision/image.so'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "r = load(\"rouge\")\n",
    "print(r.compute(predictions=[\"ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ìš”ì•½ì…ë‹ˆë‹¤.\"], references=[\"ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ìš”ì•½ì…ë‹ˆë‹¤.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e772f8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# ì„¤ì •\n",
    "CONF = {\n",
    "    \"base_model\": \"upstage/SOLAR-10.7B-Instruct-v1.0\",\n",
    "    \"adapter_path\": \"./results_solar\",  # í•™ìŠµëœ ì–´ëŒ‘í„° ê²½ë¡œ\n",
    "    \"data_path\": \"./data/\",\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede979dd",
   "metadata": {},
   "source": [
    "## 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (Base + Adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13f421ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6523b9d7e9942718563d9e21ee25862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4-bit Quantization Config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load Base Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONF['base_model'],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load Adapter\n",
    "model = PeftModel.from_pretrained(base_model, CONF['adapter_path'])\n",
    "model.eval()\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONF['base_model'], trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b75113",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„° ë¡œë“œ ë° í‰ê°€ í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a498ad1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set size: 499\n",
      "\n",
      "ğŸ“Š Reference Summary Length Statistics:\n",
      "  - Mean: 15.3 words\n",
      "  - Min: 5 words\n",
      "  - Max: 54 words\n",
      "  - Median: 14 words\n",
      "\n",
      "ğŸ“ˆ Length Distribution (words):\n",
      "    0-  9: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (83)\n",
      "   10- 19: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (314)\n",
      "   20- 29: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (81)\n",
      "   30- 39: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (18)\n",
      "   40- 49: â–ˆ (2)\n",
      "   50- 59:  (1)\n"
     ]
    }
   ],
   "source": [
    "# Load Dev Data for Validation\n",
    "dev_df = pd.read_csv(os.path.join(CONF['data_path'], 'dev.csv'))\n",
    "print(f\"Dev set size: {len(dev_df)}\")\n",
    "\n",
    "# ROUGE Metric (use `evaluate` instead of deprecated `datasets.load_metric`)\n",
    "from evaluate import load as load_metric\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "\n",
    "def compute_rouge(predictions, references):\n",
    "    \"\"\"ROUGE ì ìˆ˜ ê³„ì‚°\"\"\"\n",
    "    results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "    return {\n",
    "        \"rouge1\": results[\"rouge1\"].mid.fmeasure if hasattr(results[\"rouge1\"], \"mid\") else results[\"rouge1\"],\n",
    "        \"rouge2\": results[\"rouge2\"].mid.fmeasure if hasattr(results[\"rouge2\"], \"mid\") else results[\"rouge2\"],\n",
    "        \"rougeL\": results[\"rougeL\"].mid.fmeasure if hasattr(results[\"rouge1\"], \"mid\") else results[\"rougeL\"],\n",
    "    }\n",
    "\n",
    "# ============================================\n",
    "# ì •ë‹µ ìš”ì•½ ê¸¸ì´ ë¶„ì„\n",
    "# ============================================\n",
    "import re\n",
    "\n",
    "ref_lengths = [len(s.split()) for s in dev_df['summary']]\n",
    "print(f\"\\nğŸ“Š Reference Summary Length Statistics:\")\n",
    "print(f\"  - Mean: {sum(ref_lengths)/len(ref_lengths):.1f} words\")\n",
    "print(f\"  - Min: {min(ref_lengths)} words\")\n",
    "print(f\"  - Max: {max(ref_lengths)} words\")\n",
    "print(f\"  - Median: {sorted(ref_lengths)[len(ref_lengths)//2]} words\")\n",
    "\n",
    "# ê¸¸ì´ ë¶„í¬ ì‹œê°í™”ë¥¼ ìœ„í•œ íˆìŠ¤í† ê·¸ë¨ ë°ì´í„°\n",
    "length_bins = {}\n",
    "for l in ref_lengths:\n",
    "    bin_key = (l // 10) * 10  # 10ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”\n",
    "    length_bins[bin_key] = length_bins.get(bin_key, 0) + 1\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Length Distribution (words):\")\n",
    "for bin_start in sorted(length_bins.keys()):\n",
    "    count = length_bins[bin_start]\n",
    "    bar = \"â–ˆ\" * (count // 2)\n",
    "    print(f\"  {bin_start:3d}-{bin_start+9:3d}: {bar} ({count})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f52a59",
   "metadata": {},
   "source": [
    "## 4. ì‹¤í—˜ 1: Prompt Engineering\n",
    "ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ì„ í…ŒìŠ¤íŠ¸í•˜ì—¬ ëª¨ë¸ì´ ìš”ì•½ì„ ë” ì˜ ìƒì„±í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4d61210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation sample size: 100\n",
      "=== Prompt Engineering Test (100 samples) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt [basic]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [29:28<00:00, 17.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt [basic]: R1=0.7660, R2=0.4936, RL=0.7226, Final=66.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt [korean]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [32:02<00:00, 19.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt [korean]: R1=0.7777, R2=0.5279, RL=0.7405, Final=68.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt [ko_token]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [40:17<00:00, 24.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt [ko_token]: R1=0.7915, R2=0.5178, RL=0.7514, Final=68.69\n",
      "\n",
      "âœ… Best Prompt: ko_token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ (í•œêµ­ì–´ + íŠ¹ìˆ˜í† í° ëª…ì‹œ)\n",
    "# ============================================\n",
    "prompts = {\n",
    "    \"basic\": \"### User:\\nSummarize the following dialogue:\\n\\n{dialogue}\\n\\n### Assistant:\\n\",\n",
    "    \n",
    "    \"korean\": \"### User:\\në‹¤ìŒ ëŒ€í™”ë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì¤˜:\\n\\n{dialogue}\\n\\n### Assistant:\\n\",\n",
    "    \n",
    "    \"ko_token\": (\n",
    "        \"### User:\\n\"\n",
    "        \"ë‹¤ìŒ ì¼ìƒ ëŒ€í™”ë¥¼ í•œêµ­ì–´ë¡œ 2~3ë¬¸ì¥ ì •ë„ì˜ ìì—°ìŠ¤ëŸ¬ìš´ í‰ì„œë¬¸ìœ¼ë¡œ ìš”ì•½í•´ì¤˜.\\n\"\n",
    "        \"- ëŒ€í™”ì˜ í•µì‹¬ ë‚´ìš©ê³¼ ì¤‘ìš”í•œ ì‚¬ê±´, ì¸ë¬¼ì˜ í–‰ë™ë§Œ í¬í•¨í•´.\\n\"\n",
    "        \"- #Person1#, #Person2#, #PhoneNumber# ê°™ì€ íŠ¹ìˆ˜ í† í°ì€ ìš”ì•½ì—ì„œë„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´.\\n\\n\"\n",
    "        \"{dialogue}\\n\\n### Assistant:\\n\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def run_inference(model, tokenizer, dialogue, prompt_template, params):\n",
    "    prompt = prompt_template.format(dialogue=dialogue)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            **params,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    try:\n",
    "        summary = generated_text.split(\"### Assistant:\\n\")[1].strip()\n",
    "    except:\n",
    "        summary = generated_text\n",
    "    return summary\n",
    "\n",
    "# ============================================\n",
    "# ê°œì„ ëœ í‰ê°€ ì„¤ì •\n",
    "# ============================================\n",
    "# (1) Dev ìƒ˜í”Œ ìˆ˜: 10ê°œ â†’ 100ê°œ (ëœë¤ ìƒ˜í”Œë§)\n",
    "EVAL_SAMPLE_SIZE = 100\n",
    "eval_df = dev_df.sample(n=min(EVAL_SAMPLE_SIZE, len(dev_df)), random_state=CONF[\"seed\"]).reset_index(drop=True)\n",
    "print(f\"Evaluation sample size: {len(eval_df)}\")\n",
    "\n",
    "# (2) ëŒ€íšŒ ë°©ì‹ ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜\n",
    "def compute_final_score(rouge_scores):\n",
    "    \"\"\"ëŒ€íšŒ ê¸°ì¤€: (R1 + R2 + RL) / 3 * 100\"\"\"\n",
    "    return (rouge_scores[\"rouge1\"] + rouge_scores[\"rouge2\"] + rouge_scores[\"rougeL\"]) / 3 * 100\n",
    "\n",
    "# ============================================\n",
    "# í”„ë¡¬í”„íŠ¸ ë¹„êµ ì‹¤í—˜ (100ê°œ ìƒ˜í”Œ)\n",
    "# ============================================\n",
    "print(\"=== Prompt Engineering Test (100 samples) ===\")\n",
    "default_params = {\"max_new_tokens\": 96, \"do_sample\": False, \"repetition_penalty\": 1.1}\n",
    "\n",
    "prompt_results = []\n",
    "for name, template in prompts.items():\n",
    "    preds = []\n",
    "    for dialogue in tqdm(eval_df['dialogue'], desc=f\"Prompt [{name}]\"):\n",
    "        preds.append(run_inference(model, tokenizer, dialogue, template, default_params))\n",
    "    \n",
    "    scores = compute_rouge(preds, eval_df['summary'].tolist())\n",
    "    final = compute_final_score(scores)\n",
    "    prompt_results.append({\"prompt\": name, **scores, \"final_score\": final})\n",
    "    print(f\"Prompt [{name}]: R1={scores['rouge1']:.4f}, R2={scores['rouge2']:.4f}, RL={scores['rougeL']:.4f}, Final={final:.2f}\")\n",
    "\n",
    "# ìµœì  í”„ë¡¬í”„íŠ¸ ì„ íƒ\n",
    "best_prompt_name = max(prompt_results, key=lambda x: x[\"final_score\"])[\"prompt\"]\n",
    "best_prompt = prompts[best_prompt_name]\n",
    "print(f\"\\nâœ… Best Prompt: {best_prompt_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef875d63",
   "metadata": {},
   "source": [
    "## 5. ì‹¤í—˜ 2: Decoding Strategies (Grid Search)\n",
    "Beam Search, Repetition Penalty, No Repeat N-gram Size ë“±ì„ ì¡°í•©í•˜ì—¬ ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e9e16ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Decoding Strategy Search (using prompt: ko_token) ===\n",
      "Evaluating on 100 samples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A: beam=1, rep=1.1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [40:15<00:00, 24.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config [A: beam=1, rep=1.1]: R1=0.7915, R2=0.5178, RL=0.7514, Final=68.69, AvgLen=14.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B: beam=3, rep=1.1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [1:56:49<00:00, 70.09s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config [B: beam=3, rep=1.1]: R1=0.7416, R2=0.4823, RL=0.6978, Final=64.06, AvgLen=15.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C: beam=1, rep=1.0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [43:25<00:00, 26.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config [C: beam=1, rep=1.0]: R1=0.7715, R2=0.5185, RL=0.7374, Final=67.58, AvgLen=16.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D: beam=3, rep=1.0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [1:57:03<00:00, 70.24s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config [D: beam=3, rep=1.0]: R1=0.7445, R2=0.4891, RL=0.6993, Final=64.43, AvgLen=16.2\n",
      "\n",
      "âœ… Best Decoding Config: A: beam=1, rep=1.1\n",
      "   Final Score: 68.69\n",
      "   Avg Length: 14.8 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ë””ì½”ë”© íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„œì¹˜ (í•µì‹¬ 4ê°œ ì¡°í•©ë§Œ)\n",
    "# ============================================\n",
    "# ì œì•ˆëœ ìµœì†Œ ì‹¤í—˜ ì„¸íŠ¸:\n",
    "# - repetition_penalty: 1.0 vs 1.1\n",
    "# - num_beams: 1 vs 3\n",
    "\n",
    "search_space = [\n",
    "    {\"name\": \"A: beam=1, rep=1.1\", \"params\": {\"max_new_tokens\": 96, \"num_beams\": 1, \"do_sample\": False, \"repetition_penalty\": 1.1}},\n",
    "    {\"name\": \"B: beam=3, rep=1.1\", \"params\": {\"max_new_tokens\": 96, \"num_beams\": 3, \"early_stopping\": True, \"repetition_penalty\": 1.1}},\n",
    "    {\"name\": \"C: beam=1, rep=1.0\", \"params\": {\"max_new_tokens\": 96, \"num_beams\": 1, \"do_sample\": False, \"repetition_penalty\": 1.0}},\n",
    "    {\"name\": \"D: beam=3, rep=1.0\", \"params\": {\"max_new_tokens\": 96, \"num_beams\": 3, \"early_stopping\": True, \"repetition_penalty\": 1.0}},\n",
    "]\n",
    "\n",
    "print(f\"=== Decoding Strategy Search (using prompt: {best_prompt_name}) ===\")\n",
    "print(f\"Evaluating on {len(eval_df)} samples\\n\")\n",
    "\n",
    "decoding_results = []\n",
    "for config in search_space:\n",
    "    preds = []\n",
    "    lengths = []\n",
    "    \n",
    "    for dialogue in tqdm(eval_df['dialogue'], desc=config['name']):\n",
    "        summary = run_inference(model, tokenizer, dialogue, best_prompt, config['params'])\n",
    "        preds.append(summary)\n",
    "        lengths.append(len(summary.split()))\n",
    "    \n",
    "    scores = compute_rouge(preds, eval_df['summary'].tolist())\n",
    "    final = compute_final_score(scores)\n",
    "    avg_len = sum(lengths) / len(lengths)\n",
    "    \n",
    "    decoding_results.append({\n",
    "        \"config\": config['name'], \n",
    "        **scores, \n",
    "        \"final_score\": final,\n",
    "        \"avg_length\": avg_len\n",
    "    })\n",
    "    print(f\"Config [{config['name']}]: R1={scores['rouge1']:.4f}, R2={scores['rouge2']:.4f}, RL={scores['rougeL']:.4f}, Final={final:.2f}, AvgLen={avg_len:.1f}\")\n",
    "\n",
    "# ìµœì  ë””ì½”ë”© ì„¤ì • ì„ íƒ\n",
    "best_decoding = max(decoding_results, key=lambda x: x[\"final_score\"])\n",
    "print(f\"\\nâœ… Best Decoding Config: {best_decoding['config']}\")\n",
    "print(f\"   Final Score: {best_decoding['final_score']:.2f}\")\n",
    "print(f\"   Avg Length: {best_decoding['avg_length']:.1f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab77fce3",
   "metadata": {},
   "source": [
    "## 5-1. ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°: max_new_tokens ê·¸ë¦¬ë“œ ì„œì¹˜\n",
    "í˜„ì¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ max_new_tokensì„ 128, 150, 160ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ê³  ì¶”ê°€ íŒŒë¼ë¯¸í„°ë„ ì¡°ì •í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5597098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸° (Dev 20ê°œ ìƒ˜í”Œ) ===\n",
      "í˜„ì¬ reference í‰ê·  ê¸¸ì´: 15.3 ë‹¨ì–´\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ (max=96, beam=1, rep=1.1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [08:40<00:00, 26.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸° (Dev 20ê°œ ìƒ˜í”Œ) ===\n",
      "í˜„ì¬ reference í‰ê·  ê¸¸ì´: 15.3 ë‹¨ì–´\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ (max=96, beam=1, rep=1.1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [08:40<00:00, 26.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[í˜„ì¬ (max=96, beam=1, rep=1.1)]\n",
      "  R1: 0.8186 | R2: 0.4533 | RL: 0.7711\n",
      "  Final: 68.10 | Avg Length: 16.1 ë‹¨ì–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê°œì„ 1 (max=128, beam=1, rep=1.1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [09:31<00:00, 28.57s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸° (Dev 20ê°œ ìƒ˜í”Œ) ===\n",
      "í˜„ì¬ reference í‰ê·  ê¸¸ì´: 15.3 ë‹¨ì–´\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ (max=96, beam=1, rep=1.1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [08:40<00:00, 26.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[í˜„ì¬ (max=96, beam=1, rep=1.1)]\n",
      "  R1: 0.8186 | R2: 0.4533 | RL: 0.7711\n",
      "  Final: 68.10 | Avg Length: 16.1 ë‹¨ì–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê°œì„ 1 (max=128, beam=1, rep=1.1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [09:31<00:00, 28.57s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ê°œì„ 1 (max=128, beam=1, rep=1.1)]\n",
      "  R1: 0.8186 | R2: 0.4533 | RL: 0.7711\n",
      "  Final: 68.10 | Avg Length: 17.1 ë‹¨ì–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê°œì„ 2 (max=150, beam=1, rep=1.1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [09:54<00:00, 29.72s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸° (Dev 20ê°œ ìƒ˜í”Œ) ===\n",
      "í˜„ì¬ reference í‰ê·  ê¸¸ì´: 15.3 ë‹¨ì–´\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ (max=96, beam=1, rep=1.1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [08:40<00:00, 26.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[í˜„ì¬ (max=96, beam=1, rep=1.1)]\n",
      "  R1: 0.8186 | R2: 0.4533 | RL: 0.7711\n",
      "  Final: 68.10 | Avg Length: 16.1 ë‹¨ì–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê°œì„ 1 (max=128, beam=1, rep=1.1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [09:31<00:00, 28.57s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ê°œì„ 1 (max=128, beam=1, rep=1.1)]\n",
      "  R1: 0.8186 | R2: 0.4533 | RL: 0.7711\n",
      "  Final: 68.10 | Avg Length: 17.1 ë‹¨ì–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê°œì„ 2 (max=150, beam=1, rep=1.1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [09:54<00:00, 29.72s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ê°œì„ 2 (max=150, beam=1, rep=1.1)]\n",
      "  R1: 0.8186 | R2: 0.4533 | RL: 0.7711\n",
      "  Final: 68.10 | Avg Length: 17.4 ë‹¨ì–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê°œì„ 3 (max=160, beam=1, rep=1.0, len=1.0): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [10:45<00:00, 32.26s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸° (Dev 20ê°œ ìƒ˜í”Œ) ===\n",
      "í˜„ì¬ reference í‰ê·  ê¸¸ì´: 15.3 ë‹¨ì–´\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ (max=96, beam=1, rep=1.1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [08:40<00:00, 26.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[í˜„ì¬ (max=96, beam=1, rep=1.1)]\n",
      "  R1: 0.8186 | R2: 0.4533 | RL: 0.7711\n",
      "  Final: 68.10 | Avg Length: 16.1 ë‹¨ì–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê°œì„ 1 (max=128, beam=1, rep=1.1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [09:31<00:00, 28.57s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ê°œì„ 1 (max=128, beam=1, rep=1.1)]\n",
      "  R1: 0.8186 | R2: 0.4533 | RL: 0.7711\n",
      "  Final: 68.10 | Avg Length: 17.1 ë‹¨ì–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê°œì„ 2 (max=150, beam=1, rep=1.1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [09:54<00:00, 29.72s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ê°œì„ 2 (max=150, beam=1, rep=1.1)]\n",
      "  R1: 0.8186 | R2: 0.4533 | RL: 0.7711\n",
      "  Final: 68.10 | Avg Length: 17.4 ë‹¨ì–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê°œì„ 3 (max=160, beam=1, rep=1.0, len=1.0): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [10:45<00:00, 32.26s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ê°œì„ 3 (max=160, beam=1, rep=1.0, len=1.0)]\n",
      "  R1: 0.7935 | R2: 0.4847 | RL: 0.7614\n",
      "  Final: 67.99 | Avg Length: 19.6 ë‹¨ì–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê°œì„ 4 (max=150, beam=3, rep=1.1, len=1.2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [31:50<00:00, 95.53s/it] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸° (Dev 20ê°œ ìƒ˜í”Œ) ===\n",
      "í˜„ì¬ reference í‰ê·  ê¸¸ì´: 15.3 ë‹¨ì–´\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ (max=96, beam=1, rep=1.1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [08:40<00:00, 26.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[í˜„ì¬ (max=96, beam=1, rep=1.1)]\n",
      "  R1: 0.8186 | R2: 0.4533 | RL: 0.7711\n",
      "  Final: 68.10 | Avg Length: 16.1 ë‹¨ì–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê°œì„ 1 (max=128, beam=1, rep=1.1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [09:31<00:00, 28.57s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ê°œì„ 1 (max=128, beam=1, rep=1.1)]\n",
      "  R1: 0.8186 | R2: 0.4533 | RL: 0.7711\n",
      "  Final: 68.10 | Avg Length: 17.1 ë‹¨ì–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê°œì„ 2 (max=150, beam=1, rep=1.1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [09:54<00:00, 29.72s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ê°œì„ 2 (max=150, beam=1, rep=1.1)]\n",
      "  R1: 0.8186 | R2: 0.4533 | RL: 0.7711\n",
      "  Final: 68.10 | Avg Length: 17.4 ë‹¨ì–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê°œì„ 3 (max=160, beam=1, rep=1.0, len=1.0): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [10:45<00:00, 32.26s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ê°œì„ 3 (max=160, beam=1, rep=1.0, len=1.0)]\n",
      "  R1: 0.7935 | R2: 0.4847 | RL: 0.7614\n",
      "  Final: 67.99 | Avg Length: 19.6 ë‹¨ì–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê°œì„ 4 (max=150, beam=3, rep=1.1, len=1.2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [31:50<00:00, 95.53s/it] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ê°œì„ 4 (max=150, beam=3, rep=1.1, len=1.2)]\n",
      "  R1: 0.7049 | R2: 0.4387 | RL: 0.6929\n",
      "  Final: 61.22 | Avg Length: 21.6 ë‹¨ì–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê°œì„ 5 (max=160, beam=3, rep=1.0, len=1.2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [33:24<00:00, 100.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸° (Dev 20ê°œ ìƒ˜í”Œ) ===\n",
      "í˜„ì¬ reference í‰ê·  ê¸¸ì´: 15.3 ë‹¨ì–´\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ (max=96, beam=1, rep=1.1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [08:40<00:00, 26.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[í˜„ì¬ (max=96, beam=1, rep=1.1)]\n",
      "  R1: 0.8186 | R2: 0.4533 | RL: 0.7711\n",
      "  Final: 68.10 | Avg Length: 16.1 ë‹¨ì–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê°œì„ 1 (max=128, beam=1, rep=1.1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [09:31<00:00, 28.57s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ê°œì„ 1 (max=128, beam=1, rep=1.1)]\n",
      "  R1: 0.8186 | R2: 0.4533 | RL: 0.7711\n",
      "  Final: 68.10 | Avg Length: 17.1 ë‹¨ì–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê°œì„ 2 (max=150, beam=1, rep=1.1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [09:54<00:00, 29.72s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ê°œì„ 2 (max=150, beam=1, rep=1.1)]\n",
      "  R1: 0.8186 | R2: 0.4533 | RL: 0.7711\n",
      "  Final: 68.10 | Avg Length: 17.4 ë‹¨ì–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê°œì„ 3 (max=160, beam=1, rep=1.0, len=1.0): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [10:45<00:00, 32.26s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ê°œì„ 3 (max=160, beam=1, rep=1.0, len=1.0)]\n",
      "  R1: 0.7935 | R2: 0.4847 | RL: 0.7614\n",
      "  Final: 67.99 | Avg Length: 19.6 ë‹¨ì–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê°œì„ 4 (max=150, beam=3, rep=1.1, len=1.2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [31:50<00:00, 95.53s/it] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ê°œì„ 4 (max=150, beam=3, rep=1.1, len=1.2)]\n",
      "  R1: 0.7049 | R2: 0.4387 | RL: 0.6929\n",
      "  Final: 61.22 | Avg Length: 21.6 ë‹¨ì–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê°œì„ 5 (max=160, beam=3, rep=1.0, len=1.2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [33:24<00:00, 100.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ê°œì„ 5 (max=160, beam=3, rep=1.0, len=1.2)]\n",
      "  R1: 0.7119 | R2: 0.3805 | RL: 0.6997\n",
      "  Final: 59.73 | Avg Length: 21.6 ë‹¨ì–´\n",
      "\n",
      "================================================================================\n",
      "âœ… ìµœì  ì„¤ì • ì„ íƒ: í˜„ì¬ (max=96, beam=1, rep=1.1)\n",
      "   ì˜ˆìƒ Final Score: 68.10\n",
      "   í‰ê·  ê¸¸ì´: 16.1 ë‹¨ì–´\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°: max_new_tokens ê·¸ë¦¬ë“œ ì„œì¹˜\n",
    "# ============================================\n",
    "# ë¬¸ì œ: í˜„ì¬ max_new_tokens=96ì€ ë„ˆë¬´ ì§§ìŒ (í‰ê·  14.9 ë‹¨ì–´ ìƒì„±)\n",
    "# ì •ë‹µ ìš”ì•½ í‰ê· : ~30-50 ë‹¨ì–´ í•„ìš”\n",
    "# \n",
    "# í…ŒìŠ¤íŠ¸í•  ì„¤ì •:\n",
    "# 1. max_new_tokens: 128, 150, 160\n",
    "# 2. num_beams: 1, 3\n",
    "# 3. length_penalty: 1.0, 1.2 (ë” ê¸´ ìš”ì•½ ì„ í˜¸)\n",
    "\n",
    "print(f\"=== ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸° (Dev 20ê°œ ìƒ˜í”Œ) ===\")\n",
    "print(f\"í˜„ì¬ reference í‰ê·  ê¸¸ì´: {sum(ref_lengths)/len(ref_lengths):.1f} ë‹¨ì–´\\n\")\n",
    "\n",
    "# ì‘ì€ ìƒ˜í”Œë¡œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸ (20ê°œë§Œ)\n",
    "test_size = 20\n",
    "test_df_small = eval_df.sample(n=test_size, random_state=CONF['seed']).reset_index(drop=True)\n",
    "\n",
    "search_configs = [\n",
    "    # ê¸°ì¡´ ì„¤ì • (ë¹„êµìš©)\n",
    "    {\n",
    "        \"name\": \"í˜„ì¬ (max=96, beam=1, rep=1.1)\",\n",
    "        \"params\": {\"max_new_tokens\": 96, \"num_beams\": 1, \"do_sample\": False, \"repetition_penalty\": 1.1}\n",
    "    },\n",
    "    # ìƒˆë¡œìš´ ì„¤ì •ë“¤\n",
    "    {\n",
    "        \"name\": \"ê°œì„ 1 (max=128, beam=1, rep=1.1)\",\n",
    "        \"params\": {\"max_new_tokens\": 128, \"num_beams\": 1, \"do_sample\": False, \"repetition_penalty\": 1.1}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ê°œì„ 2 (max=150, beam=1, rep=1.1)\",\n",
    "        \"params\": {\"max_new_tokens\": 150, \"num_beams\": 1, \"do_sample\": False, \"repetition_penalty\": 1.1}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ê°œì„ 3 (max=160, beam=1, rep=1.0, len=1.0)\",\n",
    "        \"params\": {\"max_new_tokens\": 160, \"num_beams\": 1, \"do_sample\": False, \"repetition_penalty\": 1.0, \"length_penalty\": 1.0}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ê°œì„ 4 (max=150, beam=3, rep=1.1, len=1.2)\",\n",
    "        \"params\": {\"max_new_tokens\": 150, \"num_beams\": 3, \"early_stopping\": True, \"repetition_penalty\": 1.1, \"length_penalty\": 1.2}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ê°œì„ 5 (max=160, beam=3, rep=1.0, len=1.2)\",\n",
    "        \"params\": {\"max_new_tokens\": 160, \"num_beams\": 3, \"early_stopping\": True, \"repetition_penalty\": 1.0, \"length_penalty\": 1.2}\n",
    "    },\n",
    "]\n",
    "\n",
    "param_search_results = []\n",
    "\n",
    "for config in search_configs:\n",
    "    preds = []\n",
    "    lengths = []\n",
    "    \n",
    "    for dialogue in tqdm(test_df_small['dialogue'], desc=config['name']):\n",
    "        summary = run_inference(model, tokenizer, dialogue, best_prompt, config['params'])\n",
    "        summary = post_process(summary)\n",
    "        preds.append(summary)\n",
    "        lengths.append(len(summary.split()))\n",
    "    \n",
    "    scores = compute_rouge(preds, test_df_small['summary'].tolist())\n",
    "    final = compute_final_score(scores)\n",
    "    avg_len = sum(lengths) / len(lengths)\n",
    "    \n",
    "    param_search_results.append({\n",
    "        \"config\": config['name'],\n",
    "        \"rouge1\": scores['rouge1'],\n",
    "        \"rouge2\": scores['rouge2'],\n",
    "        \"rougeL\": scores['rougeL'],\n",
    "        \"final_score\": final,\n",
    "        \"avg_length\": avg_len\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n[{config['name']}]\")\n",
    "    print(f\"  R1: {scores['rouge1']:.4f} | R2: {scores['rouge2']:.4f} | RL: {scores['rougeL']:.4f}\")\n",
    "    print(f\"  Final: {final:.2f} | Avg Length: {avg_len:.1f} ë‹¨ì–´\")\n",
    "\n",
    "# ìµœì  ì„¤ì • ì„ íƒ (final_score ê¸°ì¤€)\n",
    "best_config_idx = max(range(len(param_search_results)), key=lambda i: param_search_results[i]['final_score'])\n",
    "best_config = param_search_results[best_config_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ… ìµœì  ì„¤ì • ì„ íƒ: {search_configs[best_config_idx]['name']}\")\n",
    "print(f\"   ì˜ˆìƒ Final Score: {best_config['final_score']:.2f}\")\n",
    "print(f\"   í‰ê·  ê¸¸ì´: {best_config['avg_length']:.1f} ë‹¨ì–´\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2f1746",
   "metadata": {},
   "source": [
    "## 6. 51.6863 ì´ˆê³¼ë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° íƒìƒ‰\n",
    "ê¸°ì¤€ì (51.6863)ì„ ë„˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„° ì¡°í•©ì„ Dev ìƒ˜í”Œì—ì„œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "281e626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”¬ 51.6863 ì´ˆê³¼ë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° íƒìƒ‰\n",
      "============================================================\n",
      "\n",
      "â–¶ ê¸°ì¤€ì  (len=1.2, beam=3, rep=1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [49:33<00:00, 59.48s/it] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Final: 75.27 | R1: 83.41 | R2: 62.34 | RL: 80.06 | Len: 15.9\n",
      "\n",
      "â–¶ ì‹¤í—˜1: len=1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [49:35<00:00, 59.52s/it] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Final: 75.27 | R1: 83.41 | R2: 62.34 | RL: 80.06 | Len: 15.8\n",
      "\n",
      "â–¶ ì‹¤í—˜2: len=1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [49:35<00:00, 59.50s/it] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Final: 76.48 | R1: 83.84 | R2: 64.21 | RL: 81.39 | Len: 15.9\n",
      "\n",
      "â–¶ ì‹¤í—˜3: beam=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [1:04:20<00:00, 77.22s/it] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Final: 73.81 | R1: 82.58 | R2: 59.94 | RL: 78.91 | Len: 15.8\n",
      "\n",
      "â–¶ ì‹¤í—˜4: rep=1.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [50:46<00:00, 60.93s/it] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Final: 73.25 | R1: 81.77 | R2: 59.64 | RL: 78.35 | Len: 16.6\n",
      "\n",
      "â–¶ ì‹¤í—˜5: len=1.1 + beam=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   4%|â–         | 2/50 [02:16<54:25, 68.03s/it]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, params \u001b[38;5;129;01min\u001b[39;00m experiments:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâ–¶ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m     final, scores, preds \u001b[38;5;241m=\u001b[39m \u001b[43mquick_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     avg_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(p\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m preds) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(preds)\n\u001b[1;32m     68\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: name,\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_len\u001b[39m\u001b[38;5;124m\"\u001b[39m: avg_len\n\u001b[1;32m     76\u001b[0m     })\n",
      "Cell \u001b[0;32mIn[12], line 34\u001b[0m, in \u001b[0;36mquick_evaluate\u001b[0;34m(params, prompt_name)\u001b[0m\n\u001b[1;32m     32\u001b[0m preds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dialogue \u001b[38;5;129;01min\u001b[39;00m tqdm(quick_eval_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdialogue\u001b[39m\u001b[38;5;124m'\u001b[39m], desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 34\u001b[0m     raw \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdialogue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     clean \u001b[38;5;241m=\u001b[39m post_process(raw)\n\u001b[1;32m     36\u001b[0m     preds\u001b[38;5;241m.\u001b[39mappend(clean)\n",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m, in \u001b[0;36mrun_inference\u001b[0;34m(model, tokenizer, dialogue, prompt_template, params)\u001b[0m\n\u001b[1;32m     20\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 23\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:2048\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2047\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 2048\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2050\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[0;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2566\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[1;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2579\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:3265\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3262\u001b[0m flat_running_sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flatten_beam_dim(running_sequences[:, :, :cur_len])\n\u001b[1;32m   3263\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(flat_running_sequences, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 3265\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3267\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3268\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3269\u001b[0m     model_outputs,\n\u001b[1;32m   3270\u001b[0m     model_kwargs,\n\u001b[1;32m   3271\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3272\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:459\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    441\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[1;32m    442\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:1072\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1072\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m     kwargs_without_recordable \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:395\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(hidden_states, position_ids)\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers]:\n\u001b[0;32m--> 395\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[1;32m    407\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    408\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    409\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:294\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m hidden_states, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:241\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    240\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m--> 241\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:139\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m    137\u001b[0m sin \u001b[38;5;241m=\u001b[39m sin\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m    138\u001b[0m q_embed \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(q) \u001b[38;5;241m*\u001b[39m sin)\n\u001b[0;32m--> 139\u001b[0m k_embed \u001b[38;5;241m=\u001b[39m (k \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (\u001b[43mrotate_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:109\u001b[0m, in \u001b[0;36mrotate_half\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    104\u001b[0m             sin \u001b[38;5;241m=\u001b[39m emb\u001b[38;5;241m.\u001b[39msin() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_scaling\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cos\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype), sin\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrotate_half\u001b[39m(x):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Rotates half the hidden dims of the input.\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ğŸ¯ 51.6863 ì´ˆê³¼ë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° íƒìƒ‰\n",
    "# ============================================\n",
    "# ê¸°ì¤€ì : 51.6863 (basic, 150, beam=3, rep=1.2, len=1.2)\n",
    "# \n",
    "# ì‹¤í—˜ ì „ëµ:\n",
    "#   - length_penalty: 1.1, 1.3\n",
    "#   - num_beams: 4\n",
    "#   - repetition_penalty: 1.15\n",
    "\n",
    "import re\n",
    "\n",
    "def post_process(summary: str) -> str:\n",
    "    \"\"\"ì¶”ë¡  í›„ ìš”ì•½ ê²°ê³¼ ì •ë¦¬\"\"\"\n",
    "    summary = re.sub(r\"\\s+\", \" \", summary).strip()\n",
    "    summary = summary.replace(\"### User:\", \"\").replace(\"### Assistant:\", \"\").strip()\n",
    "    if not summary:\n",
    "        return \"ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "    return summary\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ”¬ 51.6863 ì´ˆê³¼ë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° íƒìƒ‰\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dev ìƒ˜í”Œë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (50ê°œ)\n",
    "QUICK_EVAL_SIZE = 50\n",
    "quick_eval_df = dev_df.sample(n=QUICK_EVAL_SIZE, random_state=42)\n",
    "\n",
    "def quick_evaluate(params, prompt_name=\"basic\"):\n",
    "    \"\"\"ë¹ ë¥¸ Dev ìƒ˜í”Œ í‰ê°€\"\"\"\n",
    "    prompt = prompts[prompt_name]\n",
    "    preds = []\n",
    "    for dialogue in tqdm(quick_eval_df['dialogue'], desc=f\"Testing\"):\n",
    "        raw = run_inference(model, tokenizer, dialogue, prompt, params)\n",
    "        clean = post_process(raw)\n",
    "        preds.append(clean)\n",
    "    \n",
    "    refs = quick_eval_df['summary'].tolist()\n",
    "    scores = compute_rouge(preds, refs)\n",
    "    final = (scores[\"rouge1\"] + scores[\"rouge2\"] + scores[\"rougeL\"]) / 3 * 100\n",
    "    return final, scores, preds\n",
    "\n",
    "# ê¸°ì¤€ ì„¤ì • (51.6863 ì„¤ì •)\n",
    "baseline_params = {\n",
    "    \"max_new_tokens\": 150,\n",
    "    \"num_beams\": 3,\n",
    "    \"early_stopping\": True,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"length_penalty\": 1.2,\n",
    "}\n",
    "\n",
    "# ì‹¤í—˜ ì„¤ì •ë“¤\n",
    "experiments = [\n",
    "    (\"ê¸°ì¤€ì  (len=1.2, beam=3, rep=1.2)\", {**baseline_params}),\n",
    "    (\"ì‹¤í—˜1: len=1.1\", {**baseline_params, \"length_penalty\": 1.1}),\n",
    "    (\"ì‹¤í—˜2: len=1.3\", {**baseline_params, \"length_penalty\": 1.3}),\n",
    "    (\"ì‹¤í—˜3: beam=4\", {**baseline_params, \"num_beams\": 4}),\n",
    "    (\"ì‹¤í—˜4: rep=1.15\", {**baseline_params, \"repetition_penalty\": 1.15}),\n",
    "    (\"ì‹¤í—˜5: len=1.1 + beam=4\", {**baseline_params, \"length_penalty\": 1.1, \"num_beams\": 4}),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, params in experiments:\n",
    "    print(f\"\\nâ–¶ {name}\")\n",
    "    final, scores, preds = quick_evaluate(params)\n",
    "    avg_len = sum(len(p.split()) for p in preds) / len(preds)\n",
    "    results.append({\n",
    "        \"name\": name,\n",
    "        \"params\": params,\n",
    "        \"final_score\": final,\n",
    "        \"rouge1\": scores[\"rouge1\"] * 100,\n",
    "        \"rouge2\": scores[\"rouge2\"] * 100,\n",
    "        \"rougeL\": scores[\"rougeL\"] * 100,\n",
    "        \"avg_len\": avg_len\n",
    "    })\n",
    "    print(f\"   Final: {final:.2f} | R1: {scores['rouge1']*100:.2f} | R2: {scores['rouge2']*100:.2f} | RL: {scores['rougeL']*100:.2f} | Len: {avg_len:.1f}\")\n",
    "\n",
    "# ê²°ê³¼ ì •ë ¬ (ì ìˆ˜ ë†’ì€ ìˆœ)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š ê²°ê³¼ ìš”ì•½ (ì ìˆ˜ ë†’ì€ ìˆœ)\")\n",
    "print(\"=\" * 60)\n",
    "baseline_score = results[0][\"final_score\"]\n",
    "for r in sorted(results, key=lambda x: x[\"final_score\"], reverse=True):\n",
    "    diff = r[\"final_score\"] - baseline_score\n",
    "    marker = \"ğŸ†\" if r[\"final_score\"] == max(x[\"final_score\"] for x in results) else \"  \"\n",
    "    print(f\"{marker} {r['name']}: {r['final_score']:.2f} ({diff:+.2f}) | Len: {r['avg_len']:.1f}\")\n",
    "\n",
    "# ìµœê³  ì„¤ì • ì¶œë ¥\n",
    "best_result = max(results, key=lambda x: x[\"final_score\"])\n",
    "print(f\"\\nâœ… ìµœê³  ì„¤ì •: {best_result['name']}\")\n",
    "print(f\"   Dev Score: {best_result['final_score']:.2f}\")\n",
    "print(f\"   Params: {best_result['params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bc0430",
   "metadata": {},
   "source": [
    "## 7. ìµœì¢… ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "ìœ„ ì‹¤í—˜ì—ì„œ ì–»ì€ ìµœì ì˜ íŒŒë¼ë¯¸í„°ì™€ í”„ë¡¬í”„íŠ¸ë¥¼ ì ìš©í•˜ì—¬ Test ì…‹ì— ëŒ€í•œ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf5dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Final Inference with:\n",
      "  - Prompt: basic\n",
      "  - Params: {'max_new_tokens': 150, 'num_beams': 3, 'early_stopping': True, 'no_repeat_ngram_size': 0, 'repetition_penalty': 1.2, 'length_penalty': 1.3}\n",
      "  - Test set size: 499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [8:50:47<00:00, 63.82s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Saved to ./prediction/submit_solar_v3.csv\n",
      "\n",
      "ğŸ“Š Generated Summary Statistics:\n",
      "  - Total: 499\n",
      "  - Avg Length: 16.1 words\n",
      "  - Min Length: 6 words\n",
      "  - Max Length: 35 words\n",
      "\n",
      "ğŸ“ Length Comparison:\n",
      "  - Reference Avg: 15.3 words\n",
      "  - Generated Avg: 16.1 words\n",
      "  - Difference: +0.8 words (+5.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ìµœì¢… ì¶”ë¡  ì„¤ì • (51.6863ì  ìµœê³  ì„¤ì • ê¸°ë°˜)\n",
    "# ============================================\n",
    "best_prompt_name = \"basic\"\n",
    "FINAL_PROMPT = prompts[\"basic\"]\n",
    "\n",
    "# ìµœì í™”ëœ ë””ì½”ë”© íŒŒë¼ë¯¸í„°\n",
    "FINAL_PARAMS = {\n",
    "    \"max_new_tokens\": 150,           # í•œêµ­ì–´ ìš”ì•½ì— ì ì ˆí•œ ê¸¸ì´\n",
    "    \"num_beams\": 3,                  # beam search í™œì„±í™”\n",
    "    \"early_stopping\": True,          # beam search ìµœì í™”\n",
    "    \"no_repeat_ngram_size\": 0,       # ì¤‘ë³µ ì œê±° ë¹„í™œì„±í™”\n",
    "    \"repetition_penalty\": 1.2,       # ë°˜ë³µ ì–µì œ\n",
    "    \"length_penalty\": 1.3,           # ìš”ì•½ ê¸¸ì´ ì¡°ì ˆ\n",
    "}\n",
    "\n",
    "# ============================================\n",
    "# í›„ì²˜ë¦¬ í•¨ìˆ˜ (ì¶”ë¡  ê²°ê³¼ ì •ë¦¬ìš©)\n",
    "# ============================================\n",
    "def post_process(summary: str) -> str:\n",
    "    \"\"\"\n",
    "    ì¶”ë¡  í›„ ìš”ì•½ ê²°ê³¼ ì •ë¦¬:\n",
    "    1. ê³µë°± ì •ë¦¬ âœ…\n",
    "    2. í”„ë¡¬í”„íŠ¸ ì—ì½” ì œê±° âœ…\n",
    "    3. ë¬¸ì¥ ë¶„ë¦¬ & 3ë¬¸ì¥ ì œí•œ âŒ ì œê±° (ROUGE ì†ì‹¤ ë°©ì§€)\n",
    "    \"\"\"\n",
    "    # 1) ê³µë°± ì •ë¦¬\n",
    "    summary = re.sub(r\"\\s+\", \" \", summary).strip()\n",
    "    \n",
    "    # 2) í”„ë¡¬í”„íŠ¸ ì—ì½” ì œê±°\n",
    "    summary = summary.replace(\"### User:\", \"\").replace(\"### Assistant:\", \"\").strip()\n",
    "    \n",
    "    # 3) ë¹ˆ ê²°ê³¼ ë°©ì§€\n",
    "    if not summary:\n",
    "        return \"ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# ============================================\n",
    "# Test ì…‹ ì¶”ë¡ \n",
    "# ============================================\n",
    "test_df = pd.read_csv(os.path.join(CONF['data_path'], 'test.csv'))\n",
    "\n",
    "print(f\"Starting Final Inference with:\")\n",
    "print(f\"  - Prompt: {best_prompt_name}\")\n",
    "print(f\"  - Params: {FINAL_PARAMS}\")\n",
    "print(f\"  - Test set size: {len(test_df)}\")\n",
    "\n",
    "final_summaries = []\n",
    "final_lengths = []\n",
    "\n",
    "for dialogue in tqdm(test_df['dialogue'], desc=\"Final Inference\"):\n",
    "    raw_summary = run_inference(model, tokenizer, dialogue, FINAL_PROMPT, FINAL_PARAMS)\n",
    "    clean_summary = post_process(raw_summary)  # ì¶”ë¡  í›„ í›„ì²˜ë¦¬\n",
    "    final_summaries.append(clean_summary)\n",
    "    final_lengths.append(len(clean_summary.split()))\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "submission = pd.DataFrame({\n",
    "    'fname': test_df['fname'],\n",
    "    'summary': final_summaries\n",
    "})\n",
    "\n",
    "output_path = \"./prediction/submit_solar_v3.csv\"\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "# ============================================\n",
    "# í†µê³„ ì¶œë ¥\n",
    "# ============================================\n",
    "print(f\"\\nâœ… Saved to {output_path}\")\n",
    "print(f\"\\nğŸ“Š Generated Summary Statistics:\")\n",
    "print(f\"  - Total: {len(final_summaries)}\")\n",
    "print(f\"  - Avg Length: {sum(final_lengths)/len(final_lengths):.1f} words\")\n",
    "print(f\"  - Min Length: {min(final_lengths)} words\")\n",
    "print(f\"  - Max Length: {max(final_lengths)} words\")\n",
    "\n",
    "# ì •ë‹µ ê¸¸ì´ì™€ ë¹„êµ\n",
    "ref_avg = sum(ref_lengths) / len(ref_lengths)\n",
    "gen_avg = sum(final_lengths) / len(final_lengths)\n",
    "print(f\"\\nğŸ“ Length Comparison:\")\n",
    "print(f\"  - Reference Avg: {ref_avg:.1f} words\")\n",
    "print(f\"  - Generated Avg: {gen_avg:.1f} words\")\n",
    "print(f\"  - Difference: {gen_avg - ref_avg:+.1f} words ({(gen_avg/ref_avg - 1)*100:+.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
